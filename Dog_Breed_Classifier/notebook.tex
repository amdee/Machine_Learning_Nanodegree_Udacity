
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{dog\_app}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Artificial Intelligence
Nanodegree}\label{artificial-intelligence-nanodegree}

\subsection{Convolutional Neural
Networks}\label{convolutional-neural-networks}

\subsection{Project: Write an Algorithm for a Dog Identification
App}\label{project-write-an-algorithm-for-a-dog-identification-app}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this notebook, some template code has already been provided for you,
and you will need to implement additional functionality to successfully
complete this project. You will not need to modify the included code
beyond what is requested. Sections that begin with
\textbf{'(IMPLEMENTATION)'} in the header indicate that the following
block of code will require additional functionality which you must
provide. Instructions will be provided for each section, and the
specifics of the implementation are marked in the code block with a
'TODO' statement. Please be sure to read the instructions carefully!

\begin{quote}
\textbf{Note}: Once you have completed all of the code implementations,
you need to finalize your work by exporting the iPython Notebook as an
HTML document. Before exporting the notebook to html, all of the code
cells need to have been run so that reviewers can see the final
implementation and output. You can then export the notebook by using the
menu above and navigating to \n", "\textbf{File -\textgreater{} Download
as -\textgreater{} HTML (.html)}. Include the finished document along
with this notebook as your submission.
\end{quote}

In addition to implementing code, there will be questions that you must
answer which relate to the project and your implementation. Each section
where you will answer a question is preceded by a \textbf{'Question X'}
header. Carefully read each question and provide thorough answers in the
following text boxes that begin with \textbf{'Answer:'}. Your project
submission will be evaluated based on your answers to each of the
questions and the implementation you provide.

\begin{quote}
\textbf{Note:} Code and Markdown cells can be executed using the
\textbf{Shift + Enter} keyboard shortcut. Markdown cells can be edited
by double-clicking the cell to enter edit mode.
\end{quote}

The rubric contains \emph{optional} "Stand Out Suggestions" for
enhancing the project beyond the minimum requirements. If you decide to
pursue the "Stand Out Suggestions", you should include the code in this
IPython notebook.

 \#\# Step 0: Import Datasets

\subsubsection{Import Dog Dataset}\label{import-dog-dataset}

In the code cell below, we import a dataset of dog images. We populate a
few variables through the use of the \texttt{load\_files} function from
the scikit-learn library: - \texttt{train\_files},
\texttt{valid\_files}, \texttt{test\_files} - numpy arrays containing
file paths to images - \texttt{train\_targets}, \texttt{valid\_targets},
\texttt{test\_targets} - numpy arrays containing onehot-encoded
classification labels - \texttt{dog\_names} - list of string-valued dog
breed names for translating labels

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{import} \PY{n}{load\PYZus{}files}       
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{np\PYZus{}utils}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{glob} \PY{k}{import} \PY{n}{glob}
        
        \PY{c+c1}{\PYZsh{} define function to load train, test, and validation datasets}
        \PY{k}{def} \PY{n+nf}{load\PYZus{}dataset}\PY{p}{(}\PY{n}{path}\PY{p}{)}\PY{p}{:}
            \PY{n}{data} \PY{o}{=} \PY{n}{load\PYZus{}files}\PY{p}{(}\PY{n}{path}\PY{p}{)}
            \PY{n}{dog\PYZus{}files} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{filenames}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            \PY{n}{dog\PYZus{}targets} \PY{o}{=} \PY{n}{np\PYZus{}utils}\PY{o}{.}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{133}\PY{p}{)}
            \PY{k}{return} \PY{n}{dog\PYZus{}files}\PY{p}{,} \PY{n}{dog\PYZus{}targets}
        
        \PY{c+c1}{\PYZsh{} load train, test, and validation datasets}
        \PY{n}{train\PYZus{}files}\PY{p}{,} \PY{n}{train\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{valid\PYZus{}files}\PY{p}{,} \PY{n}{valid\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{test\PYZus{}files}\PY{p}{,} \PY{n}{test\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} load list of dog names}
        \PY{n}{dog\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{n}{item}\PY{p}{[}\PY{l+m+mi}{20}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{item} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dogImages/train/*/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} print statistics about the dataset}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ total dog categories.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{dog\PYZus{}names}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ total dog images.}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{train\PYZus{}files}\PY{p}{,} \PY{n}{valid\PYZus{}files}\PY{p}{,} \PY{n}{test\PYZus{}files}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ training dog images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ validation dog images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ test dog images.}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
There are 133 total dog categories.
There are 8351 total dog images.

There are 6680 training dog images.
There are 835 validation dog images.
There are 836 test dog images.

    \end{Verbatim}

    \subsubsection{Import Human Dataset}\label{import-human-dataset}

In the code cell below, we import a dataset of human images, where the
file paths are stored in the numpy array \texttt{human\_files}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k+kn}{import} \PY{n+nn}{random}
        \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{8675309}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} load filenames in shuffled human dataset}
        \PY{n}{human\PYZus{}files} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lfw/*/*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} print statistics about the dataset}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ total human images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
There are 13233 total human images.

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 1: Detect Humans

We use OpenCV's implementation of
\href{http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html}{Haar
feature-based cascade classifiers} to detect human faces in images.
OpenCV provides many pre-trained face detectors, stored as XML files on
\href{https://github.com/opencv/opencv/tree/master/data/haarcascades}{github}.
We have downloaded one of these detectors and stored it in the
\texttt{haarcascades} directory.

In the next code cell, we demonstrate how to use this detector to find
human faces in a sample image.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{import} \PY{n+nn}{cv2}                
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}                        
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline                               
        
        \PY{c+c1}{\PYZsh{} extract pre\PYZhy{}trained face detector}
        \PY{n}{face\PYZus{}cascade} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{CascadeClassifier}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{haarcascades/haarcascade\PYZus{}frontalface\PYZus{}alt.xml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} load color (BGR) image}
        \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} convert BGR image to grayscale}
        \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} find faces in image}
        \PY{n}{faces} \PY{o}{=} \PY{n}{face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} print number of faces detected in the image}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of faces detected:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} get bounding box for each detected face}
        \PY{k}{for} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{w}\PY{p}{,}\PY{n}{h}\PY{p}{)} \PY{o+ow}{in} \PY{n}{faces}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} add bounding box to color image}
            \PY{n}{cv2}\PY{o}{.}\PY{n}{rectangle}\PY{p}{(}\PY{n}{img}\PY{p}{,}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{x}\PY{o}{+}\PY{n}{w}\PY{p}{,}\PY{n}{y}\PY{o}{+}\PY{n}{h}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{255}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
            
        \PY{c+c1}{\PYZsh{} convert BGR image to RGB for plotting}
        \PY{n}{cv\PYZus{}rgb} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2RGB}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} display the image, along with bounding box}
        \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cv\PYZus{}rgb}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of faces detected: 1

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_5_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Before using any of the face detectors, it is standard procedure to
convert the images to grayscale. The \texttt{detectMultiScale} function
executes the classifier stored in \texttt{face\_cascade} and takes the
grayscale image as a parameter.

In the above code, \texttt{faces} is a numpy array of detected faces,
where each row corresponds to a detected face. Each detected face is a
1D array with four entries that specifies the bounding box of the
detected face. The first two entries in the array (extracted in the
above code as \texttt{x} and \texttt{y}) specify the horizontal and
vertical positions of the top left corner of the bounding box. The last
two entries in the array (extracted here as \texttt{w} and \texttt{h})
specify the width and height of the box.

\subsubsection{Write a Human Face
Detector}\label{write-a-human-face-detector}

We can use this procedure to write a function that returns \texttt{True}
if a human face is detected in an image and \texttt{False} otherwise.
This function, aptly named \texttt{face\_detector}, takes a
string-valued file path to an image as input and appears in the code
block below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} returns \PYZdq{}True\PYZdq{} if face is detected in image stored at img\PYZus{}path}
        \PY{k}{def} \PY{n+nf}{face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
            \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
            \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
            \PY{n}{faces} \PY{o}{=} \PY{n}{face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
            \PY{k}{return} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Assess the Human Face
Detector}\label{implementation-assess-the-human-face-detector}

\textbf{Question 1:} Use the code cell below to test the performance of
the \texttt{face\_detector} function.\\
- What percentage of the first 100 images in \texttt{human\_files} have
a detected human face?\\
- What percentage of the first 100 images in \texttt{dog\_files} have a
detected human face?

Ideally, we would like 100\% of human images with a detected face and
0\% of dog images with a detected face. You will see that our algorithm
falls short of this goal, but still gives acceptable performance. We
extract the file paths for the first 100 images from each of the
datasets and store them in the numpy arrays \texttt{human\_files\_short}
and \texttt{dog\_files\_short}.

\textbf{Answer:}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{human\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{human\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
         \PY{n}{dog\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{train\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
         \PY{c+c1}{\PYZsh{} Do NOT modify the code above this line.}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} TODO: Test the performance of the face\PYZus{}detector algorithm }
         \PY{c+c1}{\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
         
         \PY{k}{def} \PY{n+nf}{performance}\PY{p}{(}\PY{n}{filename}\PY{p}{,} \PY{n}{detector\PYZus{}func}\PY{p}{)}\PY{p}{:}
             \PY{n}{face\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{k}{for} \PY{n}{face} \PY{o+ow}{in} \PY{n}{filename}\PY{p}{:}
                 \PY{k}{if} \PY{n}{detector\PYZus{}func}\PY{p}{(}\PY{n}{face}\PY{p}{)}\PY{p}{:}
                     \PY{n}{face\PYZus{}count} \PY{o}{=} \PY{n}{face\PYZus{}count} \PY{o}{+} \PY{l+m+mi}{1}
                 \PY{n}{image\PYZus{}percent} \PY{o}{=} \PY{p}{(}\PY{n}{face\PYZus{}count} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{filename}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{100}
             \PY{k}{return} \PY{n}{image\PYZus{}percent}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The percentage of the first 100 images in human\PYZus{}files have a detected human face: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{performance}\PY{p}{(}\PY{n}{human\PYZus{}files\PYZus{}short}\PY{p}{,} \PY{n}{face\PYZus{}detector}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The percentage of the first 100 images in dog\PYZus{}files have detected human face: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{performance}\PY{p}{(}\PY{n}{dog\PYZus{}files\PYZus{}short}\PY{p}{,} \PY{n}{face\PYZus{}detector}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The percentage of the first 100 images in human\_files have a detected human face: 99.0\%
The percentage of the first 100 images in dog\_files have detected human face: 12.0\%

    \end{Verbatim}

    \textbf{Question 2:} This algorithmic choice necessitates that we
communicate to the user that we accept human images only when they
provide a clear view of a face (otherwise, we risk having unneccessarily
frustrated users!). In your opinion, is this a reasonable expectation to
pose on the user? If not, can you think of a way to detect humans in
images that does not necessitate an image with a clearly presented face?

\textbf{Answer:}

I think it is a valid assumption to pose to the user that we accept
human images only when they provide a clear view of a face because of
simplicity. A clear face will make face recognition easy and possible to
the algorithm compared to a blurry or a face which is not very clear.

We suggest the face detector from OpenCV as a potential way to detect
human images in your algorithm, but you are free to explore other
approaches, especially approaches that make use of deep learning :).
Please use the code cell below to design and test your own face
detection algorithm. If you decide to pursue this \emph{optional} task,
report performance on each of the datasets.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} (Optional) TODO: Report the performance of another  }
         \PY{c+c1}{\PYZsh{}\PYZsh{} face detection algorithm on the LFW dataset}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 2: Detect Dogs

In this section, we use a pre-trained
\href{http://ethereon.github.io/netscope/\#/gist/db945b393d40bfa26006}{ResNet-50}
model to detect dogs in images. Our first line of code downloads the
ResNet-50 model, along with weights that have been trained on
\href{http://www.image-net.org/}{ImageNet}, a very large, very popular
dataset used for image classification and other vision tasks. ImageNet
contains over 10 million URLs, each linking to an image containing an
object from one of
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{1000
categories}. Given an image, this pre-trained ResNet-50 model returns a
prediction (derived from the available categories in ImageNet) for the
object that is contained in the image.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{resnet50} \PY{k}{import} \PY{n}{ResNet50}
         
         \PY{c+c1}{\PYZsh{} define ResNet50 model}
         \PY{n}{ResNet50\PYZus{}model} \PY{o}{=} \PY{n}{ResNet50}\PY{p}{(}\PY{n}{weights}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imagenet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Pre-process the Data}\label{pre-process-the-data}

When using TensorFlow as backend, Keras CNNs require a 4D array (which
we'll also refer to as a 4D tensor) as input, with shape

\[
(\text{nb_samples}, \text{rows}, \text{columns}, \text{channels}),
\]

where \texttt{nb\_samples} corresponds to the total number of images (or
samples), and \texttt{rows}, \texttt{columns}, and \texttt{channels}
correspond to the number of rows, columns, and channels for each image,
respectively.

The \texttt{path\_to\_tensor} function below takes a string-valued file
path to a color image as input and returns a 4D tensor suitable for
supplying to a Keras CNN. The function first loads the image and resizes
it to a square image that is \(224 \times 224\) pixels. Next, the image
is converted to an array, which is then resized to a 4D tensor. In this
case, since we are working with color images, each image has three
channels. Likewise, since we are processing a single image (or sample),
the returned tensor will always have shape

\[
(1, 224, 224, 3).
\]

The \texttt{paths\_to\_tensor} function takes a numpy array of
string-valued image paths as input and returns a 4D tensor with shape

\[
(\text{nb_samples}, 224, 224, 3).
\]

Here, \texttt{nb\_samples} is the number of samples, or number of
images, in the supplied array of image paths. It is best to think of
\texttt{nb\_samples} as the number of 3D tensors (where each 3D tensor
corresponds to a different image) in your dataset!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{image}                  
         \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}
         
         \PY{k}{def} \PY{n+nf}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} loads RGB image as PIL.Image.Image type}
             \PY{n}{img} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{load\PYZus{}img}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{target\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{224}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)}
             \PY{n}{x} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{img\PYZus{}to\PYZus{}array}\PY{p}{(}\PY{n}{img}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{:}
             \PY{n}{list\PYZus{}of\PYZus{}tensors} \PY{o}{=} \PY{p}{[}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)} \PY{k}{for} \PY{n}{img\PYZus{}path} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{]}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}tensors}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Making Predictions with
ResNet-50}\label{making-predictions-with-resnet-50}

Getting the 4D tensor ready for ResNet-50, and for any other pre-trained
model in Keras, requires some additional processing. First, the RGB
image is converted to BGR by reordering the channels. All pre-trained
models have the additional normalization step that the mean pixel
(expressed in RGB as \([103.939, 116.779, 123.68]\) and calculated from
all pixels in all images in ImageNet) must be subtracted from every
pixel in each image. This is implemented in the imported function
\texttt{preprocess\_input}. If you're curious, you can check the code
for \texttt{preprocess\_input}
\href{https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py}{here}.

Now that we have a way to format our image for supplying to ResNet-50,
we are now ready to use the model to extract the predictions. This is
accomplished with the \texttt{predict} method, which returns an array
whose \(i\)-th entry is the model's predicted probability that the image
belongs to the \(i\)-th ImageNet category. This is implemented in the
\texttt{ResNet50\_predict\_labels} function below.

By taking the argmax of the predicted probability vector, we obtain an
integer corresponding to the model's predicted object class, which we
can identify with an object category through the use of this
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{dictionary}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{resnet50} \PY{k}{import} \PY{n}{preprocess\PYZus{}input}\PY{p}{,} \PY{n}{decode\PYZus{}predictions}
         
         \PY{k}{def} \PY{n+nf}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} returns prediction vector for image located at img\PYZus{}path}
             \PY{n}{img} \PY{o}{=} \PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{ResNet50\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Write a Dog Detector}\label{write-a-dog-detector}

While looking at the
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{dictionary},
you will notice that the categories corresponding to dogs appear in an
uninterrupted sequence and correspond to dictionary keys 151-268,
inclusive, to include all categories from
\texttt{\textquotesingle{}Chihuahua\textquotesingle{}} to
\texttt{\textquotesingle{}Mexican\ hairless\textquotesingle{}}. Thus, in
order to check to see if an image is predicted to contain a dog by the
pre-trained ResNet-50 model, we need only check if the
\texttt{ResNet50\_predict\_labels} function above returns a value
between 151 and 268 (inclusive).

We use these ideas to complete the \texttt{dog\_detector} function
below, which returns \texttt{True} if a dog is detected in an image (and
\texttt{False} if not).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} returns \PYZdq{}True\PYZdq{} if a dog is detected in the image stored at img\PYZus{}path}
         \PY{k}{def} \PY{n+nf}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{n}{prediction} \PY{o}{=} \PY{n}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
             \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{268}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{151}\PY{p}{)}\PY{p}{)} 
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Assess the Dog
Detector}\label{implementation-assess-the-dog-detector}

\textbf{Question 3:} Use the code cell below to test the performance of
your \texttt{dog\_detector} function.\\
- What percentage of the images in \texttt{human\_files\_short} have a
detected dog?\\
- What percentage of the images in \texttt{dog\_files\_short} have a
detected dog?

\textbf{Answer:}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Test the performance of the dog\PYZus{}detector function}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The percentage of the images in human\PYZus{}files\PYZus{}short have a detected dog: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{performance}\PY{p}{(}\PY{n}{human\PYZus{}files\PYZus{}short}\PY{p}{,} \PY{n}{dog\PYZus{}detector}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The percentage of the images in dog\PYZus{}files\PYZus{}short have a detected dog: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{performance}\PY{p}{(}\PY{n}{dog\PYZus{}files\PYZus{}short}\PY{p}{,} \PY{n}{dog\PYZus{}detector}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The percentage of the images in human\_files\_short have a detected dog: 1.0\%
The percentage of the images in dog\_files\_short have a detected dog: 100.0\%

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 3: Create a CNN to Classify Dog Breeds (from Scratch)

Now that we have functions for detecting humans and dogs in images, we
need a way to predict breed from images. In this step, you will create a
CNN that classifies dog breeds. You must create your CNN \emph{from
scratch} (so, you can't use transfer learning \emph{yet}!), and you must
attain a test accuracy of at least 1\%. In Step 5 of this notebook, you
will have the opportunity to use transfer learning to create a CNN that
attains greatly improved accuracy.

Be careful with adding too many trainable layers! More parameters means
longer training, which means you are more likely to need a GPU to
accelerate the training process. Thankfully, Keras provides a handy
estimate of the time that each epoch is likely to take; you can
extrapolate this estimate to figure out how long it will take for your
algorithm to train.

We mention that the task of assigning breed to dogs from images is
considered exceptionally challenging. To see why, consider that
\emph{even a human} would have great difficulty in distinguishing
between a Brittany and a Welsh Springer Spaniel.

\begin{longtable}[]{@{}ll@{}}
\toprule
Brittany & Welsh Springer Spaniel\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

It is not difficult to find other dog breed pairs with minimal
inter-class variation (for instance, Curly-Coated Retrievers and
American Water Spaniels).

\begin{longtable}[]{@{}ll@{}}
\toprule
Curly-Coated Retriever & American Water Spaniel\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

Likewise, recall that labradors come in yellow, chocolate, and black.
Your vision-based algorithm will have to conquer this high intra-class
variation to determine how to classify all of these different shades as
the same breed.

\begin{longtable}[]{@{}ll@{}}
\toprule
Yellow Labrador & Chocolate Labrador\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

We also mention that random chance presents an exceptionally low bar:
setting aside the fact that the classes are slightly imabalanced, a
random guess will provide a correct answer roughly 1 in 133 times, which
corresponds to an accuracy of less than 1\%.

Remember that the practice is far ahead of the theory in deep learning.
Experiment with many different architectures, and trust your intuition.
And, of course, have fun!

\subsubsection{Pre-process the Data}\label{pre-process-the-data}

We rescale the images by dividing every pixel in every image by 255.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{ImageFile}                            
         \PY{n}{ImageFile}\PY{o}{.}\PY{n}{LOAD\PYZus{}TRUNCATED\PYZus{}IMAGES} \PY{o}{=} \PY{k+kc}{True}                 
         
         \PY{c+c1}{\PYZsh{} pre\PYZhy{}process the data for Keras}
         \PY{n}{train\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{valid\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
         \PY{n}{test\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████████████████████████████████████████████████████████████████████████| 6680/6680 [02:49<00:00, 39.43it/s]
100\%|████████████████████████████████████████████████████████████████████████████████| 835/835 [00:21<00:00, 39.69it/s]
100\%|████████████████████████████████████████████████████████████████████████████████| 836/836 [00:21<00:00, 38.31it/s]

    \end{Verbatim}

    \subsubsection{(IMPLEMENTATION) Model
Architecture}\label{implementation-model-architecture}

Create a CNN to classify dog breed. At the end of your code cell block,
summarize the layers of your model by executing the line:

\begin{verbatim}
    model.summary()
\end{verbatim}

We have imported some Python modules to get you started, but feel free
to import as many modules as you need. If you end up getting stuck,
here's a hint that specifies a model that trains relatively fast on CPU
and attains \textgreater{}1\% test accuracy in 5 epochs:

\begin{figure}
\centering
\includegraphics{images/sample_cnn.png}
\caption{Sample CNN}
\end{figure}

\textbf{Question 4:} Outline the steps you took to get to your final CNN
architecture and your reasoning at each step. If you chose to use the
hinted architecture above, describe why you think that CNN architecture
should work well for the image classification task.

\textbf{Answer:}

I used the hinted architecture above through the following steps : I
added three layers of CNN to increase the depth of the array and I used
input layer (224, 224, 3). For the best result as suggested, I set
padding to same and I used a multiple of two for kernel\_size and
filters. I also reduce the dimensionality of the array by introducing
Maxpooling after each CNN layers. I added dropout layers prior to
flatten the array and pass it to the full connected layer (dense layer).
Finally, I added a softmax activation to the last layer to obtain the
probability.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Conv2D}\PY{p}{,} \PY{n}{MaxPooling2D}\PY{p}{,} \PY{n}{GlobalAveragePooling2D}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Flatten}\PY{p}{,} \PY{n}{Dense}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
         
         \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Define your architecture.}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                 \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{32}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{n}{filters}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.4}\PY{p}{)}\PY{p}{)}
         \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv2d\_4 (Conv2D)            (None, 224, 224, 16)      208       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_5 (MaxPooling2 (None, 112, 112, 16)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_5 (Conv2D)            (None, 112, 112, 32)      2080      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_6 (MaxPooling2 (None, 56, 56, 32)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_6 (Conv2D)            (None, 56, 56, 64)        8256      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_7 (MaxPooling2 (None, 28, 28, 64)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_3 (Dropout)          (None, 28, 28, 64)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_3 (Flatten)          (None, 50176)             0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_3 (Dense)              (None, 500)               25088500  
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_4 (Dropout)          (None, 500)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_4 (Dense)              (None, 133)               66633     
=================================================================
Total params: 25,165,677
Trainable params: 25,165,677
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{Compile the Model}\label{compile-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmsprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Train the
Model}\label{implementation-train-the-model}

Train your model in the code cell below. Use model checkpointing to save
the model that attains the best validation loss.

You are welcome to
\href{https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html}{augment
the training data}, but this is not a requirement.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{ModelCheckpoint}  
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: specify the number of epochs that you would like to use to train the model.}
         
         \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{10}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Do NOT modify the code below this line.}
         
         \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.from\PYZus{}scratch.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}tensors}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}tensors}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/10
4040/6680 [=================>{\ldots}] - ETA: 21:40 - loss: 4.8969 - acc: 0.0000e+ - ETA: 15:10 - loss: 8.5700 - acc: 0.0000e+ - ETA: 13:02 - loss: 8.1001 - acc: 0.0000e+ - ETA: 11:56 - loss: 7.4718 - acc: 0.0000e+ - ETA: 11:13 - loss: 6.9988 - acc: 0.0100   - ETA: 10:49 - loss: 6.6647 - acc: 0.00 - ETA: 10:28 - loss: 6.4582 - acc: 0.00 - ETA: 10:14 - loss: 6.2604 - acc: 0.00 - ETA: 10:01 - loss: 6.1286 - acc: 0.01 - ETA: 9:49 - loss: 5.9972 - acc: 0.0150 - ETA: 9:42 - loss: 5.9072 - acc: 0.018 - ETA: 9:35 - loss: 5.8291 - acc: 0.016 - ETA: 9:28 - loss: 5.7672 - acc: 0.015 - ETA: 9:23 - loss: 5.7073 - acc: 0.014 - ETA: 9:17 - loss: 5.6543 - acc: 0.013 - ETA: 9:12 - loss: 5.6053 - acc: 0.012 - ETA: 9:08 - loss: 5.5638 - acc: 0.011 - ETA: 9:03 - loss: 5.5246 - acc: 0.011 - ETA: 8:59 - loss: 5.4943 - acc: 0.010 - ETA: 8:57 - loss: 5.4656 - acc: 0.012 - ETA: 8:53 - loss: 5.4383 - acc: 0.011 - ETA: 8:51 - loss: 5.4137 - acc: 0.011 - ETA: 8:48 - loss: 5.3915 - acc: 0.010 - ETA: 8:45 - loss: 5.3718 - acc: 0.010 - ETA: 8:42 - loss: 5.3524 - acc: 0.010 - ETA: 8:39 - loss: 5.3349 - acc: 0.011 - ETA: 8:37 - loss: 5.3182 - acc: 0.011 - ETA: 8:35 - loss: 5.3027 - acc: 0.010 - ETA: 8:32 - loss: 5.2894 - acc: 0.010 - ETA: 8:30 - loss: 5.2755 - acc: 0.010 - ETA: 8:27 - loss: 5.2632 - acc: 0.011 - ETA: 8:24 - loss: 5.2512 - acc: 0.010 - ETA: 8:21 - loss: 5.2397 - acc: 0.010 - ETA: 8:19 - loss: 5.2299 - acc: 0.010 - ETA: 8:16 - loss: 5.2210 - acc: 0.010 - ETA: 8:14 - loss: 5.2121 - acc: 0.009 - ETA: 8:12 - loss: 5.2029 - acc: 0.009 - ETA: 8:10 - loss: 5.1958 - acc: 0.009 - ETA: 8:09 - loss: 5.1878 - acc: 0.009 - ETA: 8:07 - loss: 5.1797 - acc: 0.008 - ETA: 8:04 - loss: 5.1721 - acc: 0.008 - ETA: 8:02 - loss: 5.1652 - acc: 0.010 - ETA: 8:01 - loss: 5.1591 - acc: 0.010 - ETA: 7:59 - loss: 5.1527 - acc: 0.010 - ETA: 7:57 - loss: 5.1460 - acc: 0.010 - ETA: 7:55 - loss: 5.1402 - acc: 0.009 - ETA: 7:54 - loss: 5.1352 - acc: 0.010 - ETA: 7:52 - loss: 5.1301 - acc: 0.011 - ETA: 7:50 - loss: 5.1250 - acc: 0.011 - ETA: 7:48 - loss: 5.1191 - acc: 0.012 - ETA: 7:46 - loss: 5.1169 - acc: 0.011 - ETA: 7:45 - loss: 5.1125 - acc: 0.011 - ETA: 7:43 - loss: 5.1090 - acc: 0.011 - ETA: 7:43 - loss: 5.1045 - acc: 0.011 - ETA: 7:43 - loss: 5.1008 - acc: 0.010 - ETA: 7:43 - loss: 5.0968 - acc: 0.010 - ETA: 7:42 - loss: 5.0926 - acc: 0.010 - ETA: 7:41 - loss: 5.0888 - acc: 0.011 - ETA: 7:40 - loss: 5.0867 - acc: 0.011 - ETA: 7:39 - loss: 5.0833 - acc: 0.010 - ETA: 7:38 - loss: 5.0798 - acc: 0.011 - ETA: 7:38 - loss: 5.0772 - acc: 0.011 - ETA: 7:37 - loss: 5.0742 - acc: 0.011 - ETA: 7:35 - loss: 5.0712 - acc: 0.010 - ETA: 7:34 - loss: 5.0687 - acc: 0.010 - ETA: 7:32 - loss: 5.0657 - acc: 0.010 - ETA: 7:30 - loss: 5.0637 - acc: 0.010 - ETA: 7:28 - loss: 5.0612 - acc: 0.010 - ETA: 7:27 - loss: 5.0587 - acc: 0.010 - ETA: 7:26 - loss: 5.0567 - acc: 0.010 - ETA: 7:24 - loss: 5.0542 - acc: 0.009 - ETA: 7:22 - loss: 5.0519 - acc: 0.009 - ETA: 7:21 - loss: 5.0495 - acc: 0.009 - ETA: 7:19 - loss: 5.0474 - acc: 0.009 - ETA: 7:17 - loss: 5.0455 - acc: 0.009 - ETA: 7:16 - loss: 5.0435 - acc: 0.009 - ETA: 7:14 - loss: 5.0416 - acc: 0.009 - ETA: 7:13 - loss: 5.0398 - acc: 0.009 - ETA: 7:12 - loss: 5.0378 - acc: 0.008 - ETA: 7:10 - loss: 5.0361 - acc: 0.009 - ETA: 7:08 - loss: 5.0344 - acc: 0.009 - ETA: 7:06 - loss: 5.0327 - acc: 0.009 - ETA: 7:04 - loss: 5.0311 - acc: 0.009 - ETA: 7:03 - loss: 5.0296 - acc: 0.008 - ETA: 7:01 - loss: 5.0278 - acc: 0.008 - ETA: 6:59 - loss: 5.0262 - acc: 0.008 - ETA: 6:58 - loss: 5.0246 - acc: 0.009 - ETA: 6:56 - loss: 5.0232 - acc: 0.009 - ETA: 6:54 - loss: 5.0219 - acc: 0.009 - ETA: 6:53 - loss: 5.0205 - acc: 0.009 - ETA: 6:51 - loss: 5.0189 - acc: 0.009 - ETA: 6:49 - loss: 5.0174 - acc: 0.009 - ETA: 6:47 - loss: 5.0160 - acc: 0.009 - ETA: 6:45 - loss: 5.0142 - acc: 0.009 - ETA: 6:43 - loss: 5.0131 - acc: 0.009 - ETA: 6:41 - loss: 5.0118 - acc: 0.009 - ETA: 6:39 - loss: 5.0104 - acc: 0.009 - ETA: 6:37 - loss: 5.0094 - acc: 0.009 - ETA: 6:35 - loss: 5.0080 - acc: 0.009 - ETA: 6:33 - loss: 5.0067 - acc: 0.009 - ETA: 6:32 - loss: 5.0052 - acc: 0.009 - ETA: 6:30 - loss: 5.0048 - acc: 0.009 - ETA: 6:28 - loss: 5.0042 - acc: 0.009 - ETA: 6:26 - loss: 5.0031 - acc: 0.009 - ETA: 6:24 - loss: 5.0021 - acc: 0.009 - ETA: 6:22 - loss: 5.0009 - acc: 0.009 - ETA: 6:20 - loss: 4.9999 - acc: 0.009 - ETA: 6:18 - loss: 4.9988 - acc: 0.009 - ETA: 6:17 - loss: 4.9980 - acc: 0.009 - ETA: 6:15 - loss: 4.9971 - acc: 0.009 - ETA: 6:13 - loss: 4.9959 - acc: 0.009 - ETA: 6:11 - loss: 4.9950 - acc: 0.008 - ETA: 6:09 - loss: 4.9940 - acc: 0.008 - ETA: 6:07 - loss: 4.9927 - acc: 0.008 - ETA: 6:06 - loss: 4.9919 - acc: 0.008 - ETA: 6:04 - loss: 4.9908 - acc: 0.009 - ETA: 6:02 - loss: 4.9895 - acc: 0.009 - ETA: 6:00 - loss: 4.9896 - acc: 0.009 - ETA: 5:58 - loss: 4.9883 - acc: 0.009 - ETA: 5:57 - loss: 4.9870 - acc: 0.009 - ETA: 5:55 - loss: 4.9854 - acc: 0.009 - ETA: 5:53 - loss: 4.9841 - acc: 0.009 - ETA: 5:51 - loss: 4.9831 - acc: 0.009 - ETA: 5:49 - loss: 4.9826 - acc: 0.009 - ETA: 5:47 - loss: 4.9817 - acc: 0.009 - ETA: 5:46 - loss: 4.9813 - acc: 0.009 - ETA: 5:44 - loss: 4.9806 - acc: 0.009 - ETA: 5:42 - loss: 4.9797 - acc: 0.009 - ETA: 5:40 - loss: 4.9786 - acc: 0.008 - ETA: 5:39 - loss: 4.9772 - acc: 0.008 - ETA: 5:37 - loss: 4.9779 - acc: 0.008 - ETA: 5:35 - loss: 4.9767 - acc: 0.009 - ETA: 5:33 - loss: 4.9766 - acc: 0.009 - ETA: 5:31 - loss: 4.9758 - acc: 0.009 - ETA: 5:30 - loss: 4.9755 - acc: 0.008 - ETA: 5:28 - loss: 4.9749 - acc: 0.008 - ETA: 5:26 - loss: 4.9742 - acc: 0.008 - ETA: 5:24 - loss: 4.9733 - acc: 0.008 - ETA: 5:23 - loss: 4.9727 - acc: 0.008 - ETA: 5:21 - loss: 4.9721 - acc: 0.008 - ETA: 5:19 - loss: 4.9712 - acc: 0.008 - ETA: 5:17 - loss: 4.9710 - acc: 0.008 - ETA: 5:16 - loss: 4.9700 - acc: 0.008 - ETA: 5:14 - loss: 4.9689 - acc: 0.008 - ETA: 5:12 - loss: 4.9682 - acc: 0.009 - ETA: 5:10 - loss: 4.9677 - acc: 0.008 - ETA: 5:09 - loss: 4.9670 - acc: 0.009 - ETA: 5:07 - loss: 4.9668 - acc: 0.009 - ETA: 5:05 - loss: 4.9665 - acc: 0.009 - ETA: 5:03 - loss: 4.9660 - acc: 0.009 - ETA: 5:02 - loss: 4.9653 - acc: 0.009 - ETA: 5:00 - loss: 4.9648 - acc: 0.009 - ETA: 4:58 - loss: 4.9639 - acc: 0.009 - ETA: 4:57 - loss: 4.9631 - acc: 0.009 - ETA: 4:55 - loss: 4.9622 - acc: 0.009 - ETA: 4:53 - loss: 4.9611 - acc: 0.009 - ETA: 4:51 - loss: 4.9613 - acc: 0.009 - ETA: 4:49 - loss: 4.9608 - acc: 0.009 - ETA: 4:47 - loss: 4.9603 - acc: 0.009 - ETA: 4:45 - loss: 4.9592 - acc: 0.009 - ETA: 4:43 - loss: 4.9591 - acc: 0.009 - ETA: 4:41 - loss: 4.9587 - acc: 0.009 - ETA: 4:39 - loss: 4.9580 - acc: 0.008 - ETA: 4:37 - loss: 4.9571 - acc: 0.009 - ETA: 4:35 - loss: 4.9563 - acc: 0.009 - ETA: 4:32 - loss: 4.9563 - acc: 0.009 - ETA: 4:30 - loss: 4.9555 - acc: 0.009 - ETA: 4:28 - loss: 4.9545 - acc: 0.009 - ETA: 4:26 - loss: 4.9540 - acc: 0.009 - ETA: 4:24 - loss: 4.9524 - acc: 0.009 - ETA: 4:22 - loss: 4.9527 - acc: 0.009 - ETA: 4:20 - loss: 4.9520 - acc: 0.009 - ETA: 4:18 - loss: 4.9520 - acc: 0.009 - ETA: 4:16 - loss: 4.9515 - acc: 0.009 - ETA: 4:14 - loss: 4.9512 - acc: 0.009 - ETA: 4:12 - loss: 4.9506 - acc: 0.009 - ETA: 4:10 - loss: 4.9500 - acc: 0.009 - ETA: 4:08 - loss: 4.9492 - acc: 0.009 - ETA: 4:06 - loss: 4.9484 - acc: 0.010 - ETA: 4:04 - loss: 4.9472 - acc: 0.010 - ETA: 4:02 - loss: 4.9473 - acc: 0.009 - ETA: 4:00 - loss: 4.9470 - acc: 0.009 - ETA: 3:58 - loss: 4.9471 - acc: 0.009 - ETA: 3:56 - loss: 4.9466 - acc: 0.009 - ETA: 3:54 - loss: 4.9462 - acc: 0.009 - ETA: 3:52 - loss: 4.9461 - acc: 0.009 - ETA: 3:50 - loss: 4.9458 - acc: 0.009 - ETA: 3:48 - loss: 4.9451 - acc: 0.009 - ETA: 3:46 - loss: 4.9444 - acc: 0.010 - ETA: 3:44 - loss: 4.9439 - acc: 0.010 - ETA: 3:42 - loss: 4.9436 - acc: 0.009 - ETA: 3:40 - loss: 4.9433 - acc: 0.010 - ETA: 3:39 - loss: 4.9435 - acc: 0.010 - ETA: 3:37 - loss: 4.9432 - acc: 0.010 - ETA: 3:35 - loss: 4.9427 - acc: 0.010 - ETA: 3:33 - loss: 4.9422 - acc: 0.009 - ETA: 3:31 - loss: 4.9414 - acc: 0.010 - ETA: 3:29 - loss: 4.9402 - acc: 0.010 - ETA: 3:27 - loss: 4.9410 - acc: 0.010 - ETA: 3:26 - loss: 4.9409 - acc: 0.010 - ETA: 3:24 - loss: 4.9405 - acc: 0.010 - ETA: 3:22 - loss: 4.9403 - acc: 0.0106680/6680 [==============================] - ETA: 3:20 - loss: 4.9394 - acc: 0.010 - ETA: 3:18 - loss: 4.9385 - acc: 0.010 - ETA: 3:16 - loss: 4.9389 - acc: 0.010 - ETA: 3:15 - loss: 4.9386 - acc: 0.010 - ETA: 3:13 - loss: 4.9382 - acc: 0.010 - ETA: 3:11 - loss: 4.9374 - acc: 0.010 - ETA: 3:10 - loss: 4.9369 - acc: 0.010 - ETA: 3:08 - loss: 4.9369 - acc: 0.010 - ETA: 3:06 - loss: 4.9362 - acc: 0.010 - ETA: 3:04 - loss: 4.9353 - acc: 0.011 - ETA: 3:03 - loss: 4.9343 - acc: 0.011 - ETA: 3:01 - loss: 4.9337 - acc: 0.011 - ETA: 2:59 - loss: 4.9333 - acc: 0.010 - ETA: 2:57 - loss: 4.9329 - acc: 0.010 - ETA: 2:56 - loss: 4.9322 - acc: 0.010 - ETA: 2:54 - loss: 4.9318 - acc: 0.010 - ETA: 2:52 - loss: 4.9312 - acc: 0.010 - ETA: 2:51 - loss: 4.9309 - acc: 0.010 - ETA: 2:49 - loss: 4.9301 - acc: 0.010 - ETA: 2:47 - loss: 4.9305 - acc: 0.010 - ETA: 2:46 - loss: 4.9302 - acc: 0.010 - ETA: 2:44 - loss: 4.9297 - acc: 0.010 - ETA: 2:42 - loss: 4.9293 - acc: 0.010 - ETA: 2:40 - loss: 4.9289 - acc: 0.011 - ETA: 2:39 - loss: 4.9280 - acc: 0.011 - ETA: 2:37 - loss: 4.9269 - acc: 0.011 - ETA: 2:35 - loss: 4.9278 - acc: 0.011 - ETA: 2:34 - loss: 4.9270 - acc: 0.011 - ETA: 2:32 - loss: 4.9264 - acc: 0.011 - ETA: 2:30 - loss: 4.9256 - acc: 0.011 - ETA: 2:29 - loss: 4.9251 - acc: 0.010 - ETA: 2:27 - loss: 4.9244 - acc: 0.011 - ETA: 2:26 - loss: 4.9247 - acc: 0.011 - ETA: 2:24 - loss: 4.9244 - acc: 0.011 - ETA: 2:23 - loss: 4.9237 - acc: 0.011 - ETA: 2:21 - loss: 4.9234 - acc: 0.011 - ETA: 2:20 - loss: 4.9225 - acc: 0.011 - ETA: 2:18 - loss: 4.9215 - acc: 0.011 - ETA: 2:16 - loss: 4.9213 - acc: 0.011 - ETA: 2:15 - loss: 4.9207 - acc: 0.011 - ETA: 2:13 - loss: 4.9208 - acc: 0.011 - ETA: 2:12 - loss: 4.9203 - acc: 0.011 - ETA: 2:11 - loss: 4.9198 - acc: 0.011 - ETA: 2:09 - loss: 4.9187 - acc: 0.011 - ETA: 2:08 - loss: 4.9177 - acc: 0.011 - ETA: 2:06 - loss: 4.9179 - acc: 0.011 - ETA: 2:04 - loss: 4.9174 - acc: 0.011 - ETA: 2:03 - loss: 4.9167 - acc: 0.011 - ETA: 2:01 - loss: 4.9157 - acc: 0.012 - ETA: 2:00 - loss: 4.9151 - acc: 0.012 - ETA: 1:58 - loss: 4.9133 - acc: 0.012 - ETA: 1:57 - loss: 4.9126 - acc: 0.012 - ETA: 1:55 - loss: 4.9126 - acc: 0.012 - ETA: 1:54 - loss: 4.9128 - acc: 0.012 - ETA: 1:52 - loss: 4.9118 - acc: 0.012 - ETA: 1:50 - loss: 4.9107 - acc: 0.012 - ETA: 1:49 - loss: 4.9096 - acc: 0.012 - ETA: 1:48 - loss: 4.9101 - acc: 0.012 - ETA: 1:46 - loss: 4.9098 - acc: 0.013 - ETA: 1:45 - loss: 4.9092 - acc: 0.013 - ETA: 1:43 - loss: 4.9086 - acc: 0.012 - ETA: 1:42 - loss: 4.9074 - acc: 0.012 - ETA: 1:40 - loss: 4.9068 - acc: 0.012 - ETA: 1:38 - loss: 4.9062 - acc: 0.012 - ETA: 1:37 - loss: 4.9056 - acc: 0.012 - ETA: 1:36 - loss: 4.9042 - acc: 0.013 - ETA: 1:34 - loss: 4.9044 - acc: 0.013 - ETA: 1:33 - loss: 4.9041 - acc: 0.013 - ETA: 1:31 - loss: 4.9038 - acc: 0.013 - ETA: 1:30 - loss: 4.9031 - acc: 0.013 - ETA: 1:28 - loss: 4.9030 - acc: 0.013 - ETA: 1:27 - loss: 4.9021 - acc: 0.013 - ETA: 1:25 - loss: 4.9015 - acc: 0.013 - ETA: 1:24 - loss: 4.9001 - acc: 0.013 - ETA: 1:22 - loss: 4.8995 - acc: 0.013 - ETA: 1:21 - loss: 4.8994 - acc: 0.012 - ETA: 1:20 - loss: 4.8983 - acc: 0.012 - ETA: 1:18 - loss: 4.8978 - acc: 0.013 - ETA: 1:17 - loss: 4.8973 - acc: 0.013 - ETA: 1:15 - loss: 4.8965 - acc: 0.013 - ETA: 1:14 - loss: 4.8962 - acc: 0.013 - ETA: 1:12 - loss: 4.8960 - acc: 0.013 - ETA: 1:11 - loss: 4.8959 - acc: 0.013 - ETA: 1:09 - loss: 4.8958 - acc: 0.013 - ETA: 1:08 - loss: 4.8961 - acc: 0.013 - ETA: 1:06 - loss: 4.8960 - acc: 0.013 - ETA: 1:05 - loss: 4.8954 - acc: 0.013 - ETA: 1:03 - loss: 4.8948 - acc: 0.013 - ETA: 1:02 - loss: 4.8936 - acc: 0.013 - ETA: 1:00 - loss: 4.8930 - acc: 0.013 - ETA: 59s - loss: 4.8932 - acc: 0.013 - ETA: 57s - loss: 4.8925 - acc: 0.01 - ETA: 56s - loss: 4.8920 - acc: 0.01 - ETA: 54s - loss: 4.8913 - acc: 0.01 - ETA: 53s - loss: 4.8910 - acc: 0.01 - ETA: 51s - loss: 4.8905 - acc: 0.01 - ETA: 50s - loss: 4.8898 - acc: 0.01 - ETA: 49s - loss: 4.8891 - acc: 0.01 - ETA: 47s - loss: 4.8885 - acc: 0.01 - ETA: 46s - loss: 4.8887 - acc: 0.01 - ETA: 44s - loss: 4.8889 - acc: 0.01 - ETA: 43s - loss: 4.8888 - acc: 0.01 - ETA: 41s - loss: 4.8879 - acc: 0.01 - ETA: 40s - loss: 4.8881 - acc: 0.01 - ETA: 38s - loss: 4.8877 - acc: 0.01 - ETA: 37s - loss: 4.8868 - acc: 0.01 - ETA: 36s - loss: 4.8859 - acc: 0.01 - ETA: 34s - loss: 4.8850 - acc: 0.01 - ETA: 33s - loss: 4.8850 - acc: 0.01 - ETA: 31s - loss: 4.8847 - acc: 0.01 - ETA: 30s - loss: 4.8840 - acc: 0.01 - ETA: 28s - loss: 4.8834 - acc: 0.01 - ETA: 27s - loss: 4.8832 - acc: 0.01 - ETA: 26s - loss: 4.8825 - acc: 0.01 - ETA: 24s - loss: 4.8818 - acc: 0.01 - ETA: 23s - loss: 4.8812 - acc: 0.01 - ETA: 21s - loss: 4.8813 - acc: 0.01 - ETA: 20s - loss: 4.8808 - acc: 0.01 - ETA: 18s - loss: 4.8806 - acc: 0.01 - ETA: 17s - loss: 4.8799 - acc: 0.01 - ETA: 15s - loss: 4.8794 - acc: 0.01 - ETA: 14s - loss: 4.8785 - acc: 0.01 - ETA: 13s - loss: 4.8779 - acc: 0.01 - ETA: 11s - loss: 4.8772 - acc: 0.01 - ETA: 10s - loss: 4.8765 - acc: 0.01 - ETA: 8s - loss: 4.8756 - acc: 0.0155 - ETA: 7s - loss: 4.8742 - acc: 0.015 - ETA: 5s - loss: 4.8738 - acc: 0.015 - ETA: 4s - loss: 4.8739 - acc: 0.015 - ETA: 2s - loss: 4.8739 - acc: 0.015 - ETA: 1s - loss: 4.8735 - acc: 0.015 - 496s 74ms/step - loss: 4.8727 - acc: 0.0157 - val\_loss: 4.6281 - val\_acc: 0.0287

Epoch 00001: val\_loss improved from inf to 4.62813, saving model to saved\_models/weights.best.from\_scratch.hdf5
Epoch 2/10
4080/6680 [=================>{\ldots}] - ETA: 6:29 - loss: 4.5035 - acc: 0.150 - ETA: 6:47 - loss: 4.6062 - acc: 0.075 - ETA: 6:52 - loss: 4.5812 - acc: 0.066 - ETA: 6:45 - loss: 4.5489 - acc: 0.050 - ETA: 6:44 - loss: 4.5769 - acc: 0.050 - ETA: 6:41 - loss: 4.5842 - acc: 0.058 - ETA: 6:43 - loss: 4.5910 - acc: 0.057 - ETA: 6:44 - loss: 4.6173 - acc: 0.050 - ETA: 6:41 - loss: 4.6207 - acc: 0.044 - ETA: 6:40 - loss: 4.6136 - acc: 0.045 - ETA: 6:36 - loss: 4.6252 - acc: 0.045 - ETA: 6:34 - loss: 4.6235 - acc: 0.045 - ETA: 6:31 - loss: 4.6324 - acc: 0.042 - ETA: 6:29 - loss: 4.6139 - acc: 0.046 - ETA: 6:28 - loss: 4.6304 - acc: 0.043 - ETA: 6:25 - loss: 4.6349 - acc: 0.043 - ETA: 6:23 - loss: 4.6222 - acc: 0.044 - ETA: 6:22 - loss: 4.6289 - acc: 0.041 - ETA: 6:21 - loss: 4.6109 - acc: 0.044 - ETA: 6:20 - loss: 4.6100 - acc: 0.042 - ETA: 6:22 - loss: 4.6146 - acc: 0.040 - ETA: 6:20 - loss: 4.6089 - acc: 0.038 - ETA: 6:19 - loss: 4.6089 - acc: 0.037 - ETA: 6:17 - loss: 4.6017 - acc: 0.037 - ETA: 6:14 - loss: 4.5976 - acc: 0.036 - ETA: 6:13 - loss: 4.6037 - acc: 0.034 - ETA: 6:11 - loss: 4.5928 - acc: 0.035 - ETA: 6:09 - loss: 4.5967 - acc: 0.033 - ETA: 6:07 - loss: 4.6001 - acc: 0.034 - ETA: 6:05 - loss: 4.6058 - acc: 0.033 - ETA: 6:05 - loss: 4.6047 - acc: 0.032 - ETA: 6:05 - loss: 4.6030 - acc: 0.032 - ETA: 6:03 - loss: 4.5960 - acc: 0.031 - ETA: 6:02 - loss: 4.5990 - acc: 0.030 - ETA: 5:59 - loss: 4.6003 - acc: 0.030 - ETA: 5:59 - loss: 4.6045 - acc: 0.029 - ETA: 5:58 - loss: 4.6104 - acc: 0.029 - ETA: 5:56 - loss: 4.6050 - acc: 0.031 - ETA: 5:54 - loss: 4.6053 - acc: 0.030 - ETA: 5:52 - loss: 4.6048 - acc: 0.031 - ETA: 5:50 - loss: 4.6087 - acc: 0.030 - ETA: 5:48 - loss: 4.6081 - acc: 0.031 - ETA: 5:48 - loss: 4.6051 - acc: 0.031 - ETA: 5:50 - loss: 4.6030 - acc: 0.031 - ETA: 5:50 - loss: 4.6021 - acc: 0.031 - ETA: 5:49 - loss: 4.6008 - acc: 0.030 - ETA: 5:49 - loss: 4.6016 - acc: 0.029 - ETA: 5:48 - loss: 4.5968 - acc: 0.029 - ETA: 5:47 - loss: 4.5961 - acc: 0.029 - ETA: 5:45 - loss: 4.5899 - acc: 0.029 - ETA: 5:43 - loss: 4.5848 - acc: 0.028 - ETA: 5:41 - loss: 4.5837 - acc: 0.028 - ETA: 5:40 - loss: 4.5898 - acc: 0.028 - ETA: 5:38 - loss: 4.5822 - acc: 0.029 - ETA: 5:36 - loss: 4.5810 - acc: 0.030 - ETA: 5:35 - loss: 4.5856 - acc: 0.030 - ETA: 5:33 - loss: 4.5830 - acc: 0.032 - ETA: 5:32 - loss: 4.5795 - acc: 0.033 - ETA: 5:31 - loss: 4.5788 - acc: 0.033 - ETA: 5:29 - loss: 4.5792 - acc: 0.034 - ETA: 5:29 - loss: 4.5784 - acc: 0.033 - ETA: 5:29 - loss: 4.5813 - acc: 0.033 - ETA: 5:27 - loss: 4.5809 - acc: 0.033 - ETA: 5:26 - loss: 4.5810 - acc: 0.033 - ETA: 5:25 - loss: 4.5806 - acc: 0.033 - ETA: 5:23 - loss: 4.5814 - acc: 0.032 - ETA: 5:22 - loss: 4.5789 - acc: 0.033 - ETA: 5:20 - loss: 4.5781 - acc: 0.033 - ETA: 5:19 - loss: 4.5777 - acc: 0.034 - ETA: 5:18 - loss: 4.5808 - acc: 0.034 - ETA: 5:17 - loss: 4.5793 - acc: 0.034 - ETA: 5:16 - loss: 4.5799 - acc: 0.034 - ETA: 5:15 - loss: 4.5796 - acc: 0.033 - ETA: 5:14 - loss: 4.5803 - acc: 0.033 - ETA: 5:13 - loss: 4.5797 - acc: 0.033 - ETA: 5:12 - loss: 4.5780 - acc: 0.032 - ETA: 5:12 - loss: 4.5768 - acc: 0.033 - ETA: 5:10 - loss: 4.5748 - acc: 0.032 - ETA: 5:09 - loss: 4.5741 - acc: 0.032 - ETA: 5:08 - loss: 4.5740 - acc: 0.033 - ETA: 5:07 - loss: 4.5722 - acc: 0.034 - ETA: 5:05 - loss: 4.5709 - acc: 0.034 - ETA: 5:04 - loss: 4.5697 - acc: 0.034 - ETA: 5:03 - loss: 4.5687 - acc: 0.034 - ETA: 5:01 - loss: 4.5734 - acc: 0.034 - ETA: 5:00 - loss: 4.5725 - acc: 0.036 - ETA: 4:58 - loss: 4.5710 - acc: 0.036 - ETA: 4:57 - loss: 4.5710 - acc: 0.036 - ETA: 4:56 - loss: 4.5741 - acc: 0.036 - ETA: 4:55 - loss: 4.5735 - acc: 0.036 - ETA: 4:54 - loss: 4.5726 - acc: 0.036 - ETA: 4:53 - loss: 4.5751 - acc: 0.036 - ETA: 4:52 - loss: 4.5726 - acc: 0.036 - ETA: 4:51 - loss: 4.5760 - acc: 0.036 - ETA: 4:49 - loss: 4.5778 - acc: 0.035 - ETA: 4:48 - loss: 4.5784 - acc: 0.035 - ETA: 4:46 - loss: 4.5779 - acc: 0.036 - ETA: 4:45 - loss: 4.5778 - acc: 0.036 - ETA: 4:43 - loss: 4.5783 - acc: 0.035 - ETA: 4:42 - loss: 4.5787 - acc: 0.036 - ETA: 4:41 - loss: 4.5774 - acc: 0.035 - ETA: 4:39 - loss: 4.5796 - acc: 0.035 - ETA: 4:38 - loss: 4.5811 - acc: 0.035 - ETA: 4:37 - loss: 4.5796 - acc: 0.035 - ETA: 4:35 - loss: 4.5794 - acc: 0.035 - ETA: 4:34 - loss: 4.5809 - acc: 0.034 - ETA: 4:32 - loss: 4.5826 - acc: 0.035 - ETA: 4:31 - loss: 4.5801 - acc: 0.036 - ETA: 4:29 - loss: 4.5832 - acc: 0.036 - ETA: 4:28 - loss: 4.5834 - acc: 0.036 - ETA: 4:27 - loss: 4.5843 - acc: 0.036 - ETA: 4:25 - loss: 4.5876 - acc: 0.036 - ETA: 4:24 - loss: 4.5874 - acc: 0.036 - ETA: 4:23 - loss: 4.5862 - acc: 0.036 - ETA: 4:21 - loss: 4.5856 - acc: 0.036 - ETA: 4:20 - loss: 4.5847 - acc: 0.037 - ETA: 4:19 - loss: 4.5829 - acc: 0.037 - ETA: 4:17 - loss: 4.5838 - acc: 0.037 - ETA: 4:16 - loss: 4.5832 - acc: 0.037 - ETA: 4:14 - loss: 4.5819 - acc: 0.038 - ETA: 4:13 - loss: 4.5821 - acc: 0.038 - ETA: 4:12 - loss: 4.5805 - acc: 0.038 - ETA: 4:10 - loss: 4.5812 - acc: 0.038 - ETA: 4:09 - loss: 4.5801 - acc: 0.038 - ETA: 4:08 - loss: 4.5775 - acc: 0.038 - ETA: 4:06 - loss: 4.5788 - acc: 0.038 - ETA: 4:05 - loss: 4.5789 - acc: 0.037 - ETA: 4:04 - loss: 4.5778 - acc: 0.038 - ETA: 4:02 - loss: 4.5791 - acc: 0.038 - ETA: 4:01 - loss: 4.5791 - acc: 0.037 - ETA: 4:00 - loss: 4.5783 - acc: 0.037 - ETA: 3:58 - loss: 4.5760 - acc: 0.037 - ETA: 3:57 - loss: 4.5726 - acc: 0.037 - ETA: 3:56 - loss: 4.5750 - acc: 0.037 - ETA: 3:54 - loss: 4.5762 - acc: 0.037 - ETA: 3:53 - loss: 4.5758 - acc: 0.037 - ETA: 3:52 - loss: 4.5748 - acc: 0.036 - ETA: 3:51 - loss: 4.5747 - acc: 0.036 - ETA: 3:50 - loss: 4.5719 - acc: 0.037 - ETA: 3:49 - loss: 4.5713 - acc: 0.036 - ETA: 3:48 - loss: 4.5720 - acc: 0.036 - ETA: 3:47 - loss: 4.5734 - acc: 0.036 - ETA: 3:46 - loss: 4.5717 - acc: 0.036 - ETA: 3:45 - loss: 4.5707 - acc: 0.036 - ETA: 3:44 - loss: 4.5720 - acc: 0.036 - ETA: 3:43 - loss: 4.5733 - acc: 0.036 - ETA: 3:42 - loss: 4.5729 - acc: 0.036 - ETA: 3:41 - loss: 4.5718 - acc: 0.036 - ETA: 3:39 - loss: 4.5709 - acc: 0.036 - ETA: 3:38 - loss: 4.5705 - acc: 0.036 - ETA: 3:37 - loss: 4.5681 - acc: 0.037 - ETA: 3:35 - loss: 4.5706 - acc: 0.036 - ETA: 3:34 - loss: 4.5704 - acc: 0.036 - ETA: 3:33 - loss: 4.5721 - acc: 0.036 - ETA: 3:32 - loss: 4.5719 - acc: 0.036 - ETA: 3:30 - loss: 4.5719 - acc: 0.036 - ETA: 3:29 - loss: 4.5708 - acc: 0.036 - ETA: 3:28 - loss: 4.5700 - acc: 0.037 - ETA: 3:27 - loss: 4.5679 - acc: 0.037 - ETA: 3:25 - loss: 4.5675 - acc: 0.037 - ETA: 3:24 - loss: 4.5686 - acc: 0.037 - ETA: 3:23 - loss: 4.5681 - acc: 0.037 - ETA: 3:22 - loss: 4.5670 - acc: 0.037 - ETA: 3:20 - loss: 4.5656 - acc: 0.037 - ETA: 3:19 - loss: 4.5663 - acc: 0.037 - ETA: 3:18 - loss: 4.5649 - acc: 0.038 - ETA: 3:17 - loss: 4.5651 - acc: 0.038 - ETA: 3:15 - loss: 4.5644 - acc: 0.039 - ETA: 3:14 - loss: 4.5641 - acc: 0.039 - ETA: 3:13 - loss: 4.5643 - acc: 0.038 - ETA: 3:12 - loss: 4.5646 - acc: 0.038 - ETA: 3:10 - loss: 4.5648 - acc: 0.038 - ETA: 3:09 - loss: 4.5640 - acc: 0.038 - ETA: 3:08 - loss: 4.5636 - acc: 0.038 - ETA: 3:07 - loss: 4.5629 - acc: 0.038 - ETA: 3:06 - loss: 4.5627 - acc: 0.038 - ETA: 3:04 - loss: 4.5605 - acc: 0.039 - ETA: 3:03 - loss: 4.5609 - acc: 0.038 - ETA: 3:02 - loss: 4.5600 - acc: 0.038 - ETA: 3:01 - loss: 4.5605 - acc: 0.038 - ETA: 3:00 - loss: 4.5598 - acc: 0.039 - ETA: 2:59 - loss: 4.5600 - acc: 0.039 - ETA: 2:58 - loss: 4.5606 - acc: 0.039 - ETA: 2:57 - loss: 4.5599 - acc: 0.039 - ETA: 2:56 - loss: 4.5590 - acc: 0.038 - ETA: 2:55 - loss: 4.5597 - acc: 0.038 - ETA: 2:54 - loss: 4.5595 - acc: 0.038 - ETA: 2:53 - loss: 4.5567 - acc: 0.039 - ETA: 2:52 - loss: 4.5545 - acc: 0.039 - ETA: 2:51 - loss: 4.5564 - acc: 0.039 - ETA: 2:50 - loss: 4.5560 - acc: 0.039 - ETA: 2:49 - loss: 4.5552 - acc: 0.039 - ETA: 2:48 - loss: 4.5553 - acc: 0.039 - ETA: 2:47 - loss: 4.5563 - acc: 0.039 - ETA: 2:46 - loss: 4.5545 - acc: 0.039 - ETA: 2:45 - loss: 4.5542 - acc: 0.040 - ETA: 2:44 - loss: 4.5543 - acc: 0.040 - ETA: 2:42 - loss: 4.5548 - acc: 0.039 - ETA: 2:41 - loss: 4.5546 - acc: 0.039 - ETA: 2:40 - loss: 4.5530 - acc: 0.040 - ETA: 2:39 - loss: 4.5529 - acc: 0.040 - ETA: 2:38 - loss: 4.5531 - acc: 0.039 - ETA: 2:36 - loss: 4.5536 - acc: 0.039 - ETA: 2:35 - loss: 4.5528 - acc: 0.04006680/6680 [==============================] - ETA: 2:34 - loss: 4.5524 - acc: 0.039 - ETA: 2:33 - loss: 4.5519 - acc: 0.039 - ETA: 2:32 - loss: 4.5500 - acc: 0.039 - ETA: 2:30 - loss: 4.5481 - acc: 0.039 - ETA: 2:29 - loss: 4.5478 - acc: 0.040 - ETA: 2:28 - loss: 4.5475 - acc: 0.040 - ETA: 2:27 - loss: 4.5456 - acc: 0.040 - ETA: 2:25 - loss: 4.5444 - acc: 0.040 - ETA: 2:24 - loss: 4.5444 - acc: 0.040 - ETA: 2:23 - loss: 4.5457 - acc: 0.040 - ETA: 2:22 - loss: 4.5446 - acc: 0.040 - ETA: 2:20 - loss: 4.5451 - acc: 0.040 - ETA: 2:19 - loss: 4.5450 - acc: 0.040 - ETA: 2:18 - loss: 4.5442 - acc: 0.040 - ETA: 2:17 - loss: 4.5439 - acc: 0.041 - ETA: 2:16 - loss: 4.5435 - acc: 0.041 - ETA: 2:14 - loss: 4.5439 - acc: 0.041 - ETA: 2:13 - loss: 4.5429 - acc: 0.041 - ETA: 2:12 - loss: 4.5434 - acc: 0.041 - ETA: 2:11 - loss: 4.5438 - acc: 0.041 - ETA: 2:09 - loss: 4.5424 - acc: 0.041 - ETA: 2:08 - loss: 4.5412 - acc: 0.041 - ETA: 2:07 - loss: 4.5405 - acc: 0.041 - ETA: 2:06 - loss: 4.5417 - acc: 0.041 - ETA: 2:04 - loss: 4.5406 - acc: 0.041 - ETA: 2:03 - loss: 4.5410 - acc: 0.041 - ETA: 2:02 - loss: 4.5394 - acc: 0.041 - ETA: 2:01 - loss: 4.5389 - acc: 0.041 - ETA: 2:00 - loss: 4.5375 - acc: 0.041 - ETA: 1:58 - loss: 4.5363 - acc: 0.041 - ETA: 1:57 - loss: 4.5367 - acc: 0.041 - ETA: 1:56 - loss: 4.5368 - acc: 0.041 - ETA: 1:55 - loss: 4.5368 - acc: 0.041 - ETA: 1:54 - loss: 4.5367 - acc: 0.041 - ETA: 1:52 - loss: 4.5370 - acc: 0.041 - ETA: 1:51 - loss: 4.5370 - acc: 0.040 - ETA: 1:50 - loss: 4.5358 - acc: 0.041 - ETA: 1:49 - loss: 4.5349 - acc: 0.041 - ETA: 1:47 - loss: 4.5341 - acc: 0.041 - ETA: 1:46 - loss: 4.5342 - acc: 0.041 - ETA: 1:45 - loss: 4.5334 - acc: 0.041 - ETA: 1:44 - loss: 4.5326 - acc: 0.041 - ETA: 1:43 - loss: 4.5325 - acc: 0.041 - ETA: 1:41 - loss: 4.5323 - acc: 0.041 - ETA: 1:40 - loss: 4.5321 - acc: 0.041 - ETA: 1:39 - loss: 4.5311 - acc: 0.041 - ETA: 1:38 - loss: 4.5296 - acc: 0.041 - ETA: 1:37 - loss: 4.5287 - acc: 0.041 - ETA: 1:35 - loss: 4.5283 - acc: 0.041 - ETA: 1:34 - loss: 4.5277 - acc: 0.041 - ETA: 1:33 - loss: 4.5279 - acc: 0.041 - ETA: 1:32 - loss: 4.5270 - acc: 0.041 - ETA: 1:31 - loss: 4.5273 - acc: 0.041 - ETA: 1:29 - loss: 4.5262 - acc: 0.041 - ETA: 1:28 - loss: 4.5254 - acc: 0.041 - ETA: 1:27 - loss: 4.5254 - acc: 0.041 - ETA: 1:26 - loss: 4.5248 - acc: 0.041 - ETA: 1:25 - loss: 4.5245 - acc: 0.041 - ETA: 1:23 - loss: 4.5234 - acc: 0.041 - ETA: 1:22 - loss: 4.5237 - acc: 0.041 - ETA: 1:21 - loss: 4.5217 - acc: 0.041 - ETA: 1:20 - loss: 4.5207 - acc: 0.041 - ETA: 1:19 - loss: 4.5200 - acc: 0.041 - ETA: 1:17 - loss: 4.5186 - acc: 0.042 - ETA: 1:16 - loss: 4.5181 - acc: 0.042 - ETA: 1:15 - loss: 4.5174 - acc: 0.042 - ETA: 1:14 - loss: 4.5169 - acc: 0.042 - ETA: 1:13 - loss: 4.5168 - acc: 0.042 - ETA: 1:11 - loss: 4.5172 - acc: 0.042 - ETA: 1:10 - loss: 4.5179 - acc: 0.042 - ETA: 1:09 - loss: 4.5181 - acc: 0.041 - ETA: 1:08 - loss: 4.5176 - acc: 0.041 - ETA: 1:07 - loss: 4.5170 - acc: 0.041 - ETA: 1:06 - loss: 4.5163 - acc: 0.041 - ETA: 1:04 - loss: 4.5163 - acc: 0.041 - ETA: 1:03 - loss: 4.5145 - acc: 0.042 - ETA: 1:02 - loss: 4.5145 - acc: 0.042 - ETA: 1:01 - loss: 4.5149 - acc: 0.042 - ETA: 1:00 - loss: 4.5159 - acc: 0.042 - ETA: 58s - loss: 4.5138 - acc: 0.042 - ETA: 57s - loss: 4.5140 - acc: 0.04 - ETA: 56s - loss: 4.5146 - acc: 0.04 - ETA: 55s - loss: 4.5151 - acc: 0.04 - ETA: 54s - loss: 4.5146 - acc: 0.04 - ETA: 53s - loss: 4.5145 - acc: 0.04 - ETA: 51s - loss: 4.5143 - acc: 0.04 - ETA: 50s - loss: 4.5123 - acc: 0.04 - ETA: 49s - loss: 4.5132 - acc: 0.04 - ETA: 48s - loss: 4.5124 - acc: 0.04 - ETA: 47s - loss: 4.5110 - acc: 0.04 - ETA: 46s - loss: 4.5097 - acc: 0.04 - ETA: 45s - loss: 4.5087 - acc: 0.04 - ETA: 43s - loss: 4.5089 - acc: 0.04 - ETA: 42s - loss: 4.5076 - acc: 0.04 - ETA: 41s - loss: 4.5073 - acc: 0.04 - ETA: 40s - loss: 4.5075 - acc: 0.04 - ETA: 39s - loss: 4.5066 - acc: 0.04 - ETA: 38s - loss: 4.5049 - acc: 0.04 - ETA: 36s - loss: 4.5043 - acc: 0.04 - ETA: 35s - loss: 4.5038 - acc: 0.04 - ETA: 34s - loss: 4.5049 - acc: 0.04 - ETA: 33s - loss: 4.5041 - acc: 0.04 - ETA: 32s - loss: 4.5035 - acc: 0.04 - ETA: 30s - loss: 4.5035 - acc: 0.04 - ETA: 29s - loss: 4.5016 - acc: 0.04 - ETA: 28s - loss: 4.5005 - acc: 0.04 - ETA: 27s - loss: 4.4992 - acc: 0.04 - ETA: 26s - loss: 4.4993 - acc: 0.04 - ETA: 25s - loss: 4.4989 - acc: 0.04 - ETA: 23s - loss: 4.4995 - acc: 0.04 - ETA: 22s - loss: 4.4976 - acc: 0.04 - ETA: 21s - loss: 4.4971 - acc: 0.04 - ETA: 20s - loss: 4.4954 - acc: 0.04 - ETA: 19s - loss: 4.4947 - acc: 0.04 - ETA: 17s - loss: 4.4945 - acc: 0.04 - ETA: 16s - loss: 4.4941 - acc: 0.04 - ETA: 15s - loss: 4.4940 - acc: 0.04 - ETA: 14s - loss: 4.4941 - acc: 0.04 - ETA: 13s - loss: 4.4944 - acc: 0.04 - ETA: 11s - loss: 4.4937 - acc: 0.04 - ETA: 10s - loss: 4.4921 - acc: 0.04 - ETA: 9s - loss: 4.4921 - acc: 0.0445 - ETA: 8s - loss: 4.4912 - acc: 0.044 - ETA: 7s - loss: 4.4900 - acc: 0.044 - ETA: 6s - loss: 4.4896 - acc: 0.044 - ETA: 4s - loss: 4.4892 - acc: 0.044 - ETA: 3s - loss: 4.4878 - acc: 0.044 - ETA: 2s - loss: 4.4868 - acc: 0.044 - ETA: 1s - loss: 4.4876 - acc: 0.044 - 418s 63ms/step - loss: 4.4876 - acc: 0.0443 - val\_loss: 4.3061 - val\_acc: 0.0515

Epoch 00002: val\_loss improved from 4.62813 to 4.30611, saving model to saved\_models/weights.best.from\_scratch.hdf5
Epoch 3/10
4080/6680 [=================>{\ldots}] - ETA: 6:34 - loss: 4.1246 - acc: 0.050 - ETA: 7:24 - loss: 4.0965 - acc: 0.125 - ETA: 7:09 - loss: 4.1077 - acc: 0.116 - ETA: 6:54 - loss: 3.9659 - acc: 0.137 - ETA: 6:43 - loss: 4.0496 - acc: 0.130 - ETA: 6:36 - loss: 4.0129 - acc: 0.108 - ETA: 6:34 - loss: 4.0376 - acc: 0.107 - ETA: 6:40 - loss: 4.0409 - acc: 0.118 - ETA: 6:39 - loss: 4.1144 - acc: 0.105 - ETA: 6:36 - loss: 4.1097 - acc: 0.115 - ETA: 6:35 - loss: 4.0444 - acc: 0.136 - ETA: 6:32 - loss: 4.0230 - acc: 0.137 - ETA: 6:30 - loss: 4.0135 - acc: 0.134 - ETA: 6:28 - loss: 4.0361 - acc: 0.128 - ETA: 6:28 - loss: 4.0147 - acc: 0.123 - ETA: 6:26 - loss: 4.0105 - acc: 0.121 - ETA: 6:23 - loss: 4.0339 - acc: 0.114 - ETA: 6:25 - loss: 4.0365 - acc: 0.111 - ETA: 6:26 - loss: 4.0378 - acc: 0.110 - ETA: 6:26 - loss: 4.0450 - acc: 0.110 - ETA: 6:25 - loss: 4.0617 - acc: 0.107 - ETA: 6:23 - loss: 4.0741 - acc: 0.106 - ETA: 6:24 - loss: 4.0635 - acc: 0.104 - ETA: 6:21 - loss: 4.0582 - acc: 0.104 - ETA: 6:19 - loss: 4.0546 - acc: 0.102 - ETA: 6:18 - loss: 4.0444 - acc: 0.103 - ETA: 6:22 - loss: 4.0383 - acc: 0.107 - ETA: 6:23 - loss: 4.0377 - acc: 0.108 - ETA: 6:26 - loss: 4.0318 - acc: 0.110 - ETA: 6:27 - loss: 4.0280 - acc: 0.111 - ETA: 6:29 - loss: 4.0268 - acc: 0.111 - ETA: 6:29 - loss: 4.0114 - acc: 0.112 - ETA: 6:30 - loss: 4.0189 - acc: 0.112 - ETA: 6:33 - loss: 4.0197 - acc: 0.113 - ETA: 6:32 - loss: 4.0036 - acc: 0.114 - ETA: 6:33 - loss: 4.0229 - acc: 0.112 - ETA: 6:32 - loss: 4.0247 - acc: 0.112 - ETA: 6:34 - loss: 4.0317 - acc: 0.110 - ETA: 6:36 - loss: 4.0275 - acc: 0.115 - ETA: 6:37 - loss: 4.0224 - acc: 0.117 - ETA: 6:38 - loss: 4.0271 - acc: 0.114 - ETA: 6:44 - loss: 4.0232 - acc: 0.113 - ETA: 6:47 - loss: 4.0281 - acc: 0.110 - ETA: 6:49 - loss: 4.0295 - acc: 0.109 - ETA: 6:51 - loss: 4.0227 - acc: 0.111 - ETA: 6:50 - loss: 4.0294 - acc: 0.110 - ETA: 6:48 - loss: 4.0366 - acc: 0.108 - ETA: 6:48 - loss: 4.0366 - acc: 0.109 - ETA: 6:47 - loss: 4.0381 - acc: 0.108 - ETA: 6:46 - loss: 4.0312 - acc: 0.110 - ETA: 6:43 - loss: 4.0342 - acc: 0.109 - ETA: 6:41 - loss: 4.0345 - acc: 0.107 - ETA: 6:40 - loss: 4.0255 - acc: 0.107 - ETA: 6:39 - loss: 4.0278 - acc: 0.108 - ETA: 6:38 - loss: 4.0231 - acc: 0.110 - ETA: 6:35 - loss: 4.0195 - acc: 0.108 - ETA: 6:34 - loss: 4.0245 - acc: 0.108 - ETA: 6:33 - loss: 4.0277 - acc: 0.107 - ETA: 6:32 - loss: 4.0315 - acc: 0.106 - ETA: 6:30 - loss: 4.0247 - acc: 0.106 - ETA: 6:30 - loss: 4.0297 - acc: 0.105 - ETA: 6:31 - loss: 4.0346 - acc: 0.105 - ETA: 6:29 - loss: 4.0341 - acc: 0.106 - ETA: 6:28 - loss: 4.0330 - acc: 0.106 - ETA: 6:28 - loss: 4.0313 - acc: 0.104 - ETA: 6:26 - loss: 4.0319 - acc: 0.104 - ETA: 6:25 - loss: 4.0204 - acc: 0.106 - ETA: 6:23 - loss: 4.0255 - acc: 0.105 - ETA: 6:21 - loss: 4.0211 - acc: 0.105 - ETA: 6:19 - loss: 4.0232 - acc: 0.105 - ETA: 6:17 - loss: 4.0196 - acc: 0.106 - ETA: 6:15 - loss: 4.0205 - acc: 0.104 - ETA: 6:13 - loss: 4.0202 - acc: 0.104 - ETA: 6:12 - loss: 4.0198 - acc: 0.105 - ETA: 6:11 - loss: 4.0208 - acc: 0.106 - ETA: 6:09 - loss: 4.0158 - acc: 0.108 - ETA: 6:09 - loss: 4.0132 - acc: 0.108 - ETA: 6:08 - loss: 4.0095 - acc: 0.109 - ETA: 6:08 - loss: 4.0142 - acc: 0.107 - ETA: 6:07 - loss: 4.0156 - acc: 0.107 - ETA: 6:07 - loss: 4.0131 - acc: 0.106 - ETA: 6:06 - loss: 4.0092 - acc: 0.107 - ETA: 6:05 - loss: 4.0128 - acc: 0.107 - ETA: 6:03 - loss: 4.0141 - acc: 0.107 - ETA: 6:01 - loss: 4.0131 - acc: 0.106 - ETA: 5:59 - loss: 4.0177 - acc: 0.105 - ETA: 5:57 - loss: 4.0117 - acc: 0.105 - ETA: 5:55 - loss: 4.0067 - acc: 0.106 - ETA: 5:53 - loss: 4.0083 - acc: 0.106 - ETA: 5:51 - loss: 4.0048 - acc: 0.107 - ETA: 5:49 - loss: 4.0036 - acc: 0.106 - ETA: 5:47 - loss: 4.0067 - acc: 0.106 - ETA: 5:45 - loss: 4.0047 - acc: 0.107 - ETA: 5:43 - loss: 4.0036 - acc: 0.107 - ETA: 5:41 - loss: 4.0071 - acc: 0.106 - ETA: 5:40 - loss: 4.0053 - acc: 0.106 - ETA: 5:38 - loss: 4.0044 - acc: 0.106 - ETA: 5:36 - loss: 4.0050 - acc: 0.107 - ETA: 5:34 - loss: 4.0080 - acc: 0.107 - ETA: 5:33 - loss: 4.0097 - acc: 0.107 - ETA: 5:31 - loss: 4.0096 - acc: 0.107 - ETA: 5:30 - loss: 4.0069 - acc: 0.107 - ETA: 5:28 - loss: 4.0155 - acc: 0.106 - ETA: 5:26 - loss: 4.0129 - acc: 0.108 - ETA: 5:25 - loss: 4.0110 - acc: 0.107 - ETA: 5:24 - loss: 4.0100 - acc: 0.107 - ETA: 5:22 - loss: 4.0111 - acc: 0.106 - ETA: 5:20 - loss: 4.0123 - acc: 0.105 - ETA: 5:19 - loss: 4.0080 - acc: 0.106 - ETA: 5:17 - loss: 4.0073 - acc: 0.107 - ETA: 5:15 - loss: 4.0067 - acc: 0.107 - ETA: 5:13 - loss: 4.0057 - acc: 0.107 - ETA: 5:11 - loss: 4.0056 - acc: 0.106 - ETA: 5:10 - loss: 4.0055 - acc: 0.107 - ETA: 5:08 - loss: 4.0052 - acc: 0.107 - ETA: 5:06 - loss: 4.0064 - acc: 0.107 - ETA: 5:05 - loss: 4.0067 - acc: 0.108 - ETA: 5:03 - loss: 4.0053 - acc: 0.108 - ETA: 5:02 - loss: 4.0099 - acc: 0.108 - ETA: 5:00 - loss: 4.0066 - acc: 0.108 - ETA: 4:59 - loss: 4.0023 - acc: 0.109 - ETA: 4:57 - loss: 3.9990 - acc: 0.109 - ETA: 4:56 - loss: 3.9975 - acc: 0.108 - ETA: 4:54 - loss: 4.0005 - acc: 0.107 - ETA: 4:52 - loss: 3.9994 - acc: 0.107 - ETA: 4:51 - loss: 3.9954 - acc: 0.107 - ETA: 4:49 - loss: 3.9960 - acc: 0.107 - ETA: 4:47 - loss: 3.9941 - acc: 0.108 - ETA: 4:46 - loss: 3.9926 - acc: 0.108 - ETA: 4:44 - loss: 3.9971 - acc: 0.108 - ETA: 4:43 - loss: 3.9999 - acc: 0.108 - ETA: 4:41 - loss: 3.9988 - acc: 0.108 - ETA: 4:40 - loss: 4.0008 - acc: 0.108 - ETA: 4:38 - loss: 3.9993 - acc: 0.108 - ETA: 4:37 - loss: 3.9988 - acc: 0.108 - ETA: 4:35 - loss: 3.9993 - acc: 0.108 - ETA: 4:34 - loss: 4.0010 - acc: 0.108 - ETA: 4:32 - loss: 4.0042 - acc: 0.107 - ETA: 4:31 - loss: 4.0020 - acc: 0.106 - ETA: 4:29 - loss: 3.9988 - acc: 0.107 - ETA: 4:28 - loss: 4.0001 - acc: 0.106 - ETA: 4:26 - loss: 3.9976 - acc: 0.107 - ETA: 4:25 - loss: 3.9984 - acc: 0.107 - ETA: 4:23 - loss: 3.9989 - acc: 0.107 - ETA: 4:22 - loss: 3.9974 - acc: 0.108 - ETA: 4:20 - loss: 3.9982 - acc: 0.108 - ETA: 4:19 - loss: 3.9969 - acc: 0.108 - ETA: 4:17 - loss: 3.9963 - acc: 0.108 - ETA: 4:16 - loss: 3.9917 - acc: 0.109 - ETA: 4:14 - loss: 3.9916 - acc: 0.109 - ETA: 4:13 - loss: 3.9922 - acc: 0.108 - ETA: 4:11 - loss: 3.9909 - acc: 0.108 - ETA: 4:10 - loss: 3.9911 - acc: 0.108 - ETA: 4:08 - loss: 3.9904 - acc: 0.108 - ETA: 4:07 - loss: 3.9926 - acc: 0.108 - ETA: 4:05 - loss: 3.9913 - acc: 0.108 - ETA: 4:04 - loss: 3.9891 - acc: 0.108 - ETA: 4:02 - loss: 3.9897 - acc: 0.108 - ETA: 4:01 - loss: 3.9869 - acc: 0.109 - ETA: 3:59 - loss: 3.9862 - acc: 0.110 - ETA: 3:58 - loss: 3.9873 - acc: 0.110 - ETA: 3:56 - loss: 3.9886 - acc: 0.109 - ETA: 3:55 - loss: 3.9882 - acc: 0.109 - ETA: 3:53 - loss: 3.9891 - acc: 0.109 - ETA: 3:52 - loss: 3.9906 - acc: 0.109 - ETA: 3:50 - loss: 3.9909 - acc: 0.108 - ETA: 3:49 - loss: 3.9877 - acc: 0.109 - ETA: 3:47 - loss: 3.9881 - acc: 0.109 - ETA: 3:46 - loss: 3.9877 - acc: 0.108 - ETA: 3:44 - loss: 3.9884 - acc: 0.108 - ETA: 3:43 - loss: 3.9874 - acc: 0.108 - ETA: 3:42 - loss: 3.9885 - acc: 0.109 - ETA: 3:40 - loss: 3.9902 - acc: 0.108 - ETA: 3:39 - loss: 3.9897 - acc: 0.108 - ETA: 3:37 - loss: 3.9885 - acc: 0.108 - ETA: 3:36 - loss: 3.9917 - acc: 0.108 - ETA: 3:35 - loss: 3.9906 - acc: 0.108 - ETA: 3:34 - loss: 3.9896 - acc: 0.108 - ETA: 3:33 - loss: 3.9896 - acc: 0.109 - ETA: 3:31 - loss: 3.9894 - acc: 0.109 - ETA: 3:30 - loss: 3.9896 - acc: 0.109 - ETA: 3:29 - loss: 3.9896 - acc: 0.109 - ETA: 3:28 - loss: 3.9895 - acc: 0.109 - ETA: 3:26 - loss: 3.9916 - acc: 0.109 - ETA: 3:25 - loss: 3.9901 - acc: 0.109 - ETA: 3:23 - loss: 3.9906 - acc: 0.109 - ETA: 3:22 - loss: 3.9906 - acc: 0.109 - ETA: 3:20 - loss: 3.9899 - acc: 0.109 - ETA: 3:19 - loss: 3.9920 - acc: 0.109 - ETA: 3:17 - loss: 3.9923 - acc: 0.109 - ETA: 3:16 - loss: 3.9946 - acc: 0.108 - ETA: 3:14 - loss: 3.9942 - acc: 0.108 - ETA: 3:13 - loss: 3.9937 - acc: 0.108 - ETA: 3:11 - loss: 3.9958 - acc: 0.108 - ETA: 3:10 - loss: 3.9952 - acc: 0.107 - ETA: 3:08 - loss: 3.9963 - acc: 0.107 - ETA: 3:07 - loss: 3.9948 - acc: 0.108 - ETA: 3:06 - loss: 3.9960 - acc: 0.108 - ETA: 3:04 - loss: 3.9948 - acc: 0.108 - ETA: 3:03 - loss: 3.9947 - acc: 0.108 - ETA: 3:01 - loss: 3.9949 - acc: 0.108 - ETA: 3:00 - loss: 3.9942 - acc: 0.108 - ETA: 2:58 - loss: 3.9936 - acc: 0.108 - ETA: 2:57 - loss: 3.9924 - acc: 0.10816680/6680 [==============================] - ETA: 2:56 - loss: 3.9915 - acc: 0.108 - ETA: 2:54 - loss: 3.9928 - acc: 0.108 - ETA: 2:53 - loss: 3.9920 - acc: 0.108 - ETA: 2:52 - loss: 3.9919 - acc: 0.108 - ETA: 2:50 - loss: 3.9912 - acc: 0.108 - ETA: 2:49 - loss: 3.9917 - acc: 0.108 - ETA: 2:48 - loss: 3.9916 - acc: 0.108 - ETA: 2:46 - loss: 3.9905 - acc: 0.108 - ETA: 2:45 - loss: 3.9908 - acc: 0.108 - ETA: 2:44 - loss: 3.9919 - acc: 0.108 - ETA: 2:42 - loss: 3.9898 - acc: 0.109 - ETA: 2:41 - loss: 3.9885 - acc: 0.109 - ETA: 2:39 - loss: 3.9896 - acc: 0.109 - ETA: 2:38 - loss: 3.9892 - acc: 0.108 - ETA: 2:37 - loss: 3.9889 - acc: 0.108 - ETA: 2:35 - loss: 3.9905 - acc: 0.108 - ETA: 2:34 - loss: 3.9917 - acc: 0.108 - ETA: 2:32 - loss: 3.9922 - acc: 0.108 - ETA: 2:31 - loss: 3.9921 - acc: 0.108 - ETA: 2:30 - loss: 3.9940 - acc: 0.107 - ETA: 2:28 - loss: 3.9956 - acc: 0.107 - ETA: 2:27 - loss: 3.9952 - acc: 0.107 - ETA: 2:26 - loss: 3.9942 - acc: 0.108 - ETA: 2:24 - loss: 3.9949 - acc: 0.108 - ETA: 2:23 - loss: 3.9975 - acc: 0.108 - ETA: 2:21 - loss: 3.9978 - acc: 0.108 - ETA: 2:20 - loss: 3.9989 - acc: 0.108 - ETA: 2:19 - loss: 3.9973 - acc: 0.108 - ETA: 2:17 - loss: 3.9971 - acc: 0.108 - ETA: 2:16 - loss: 3.9961 - acc: 0.108 - ETA: 2:14 - loss: 3.9954 - acc: 0.108 - ETA: 2:13 - loss: 3.9951 - acc: 0.108 - ETA: 2:12 - loss: 3.9948 - acc: 0.108 - ETA: 2:10 - loss: 3.9935 - acc: 0.108 - ETA: 2:09 - loss: 3.9924 - acc: 0.108 - ETA: 2:08 - loss: 3.9940 - acc: 0.108 - ETA: 2:06 - loss: 3.9932 - acc: 0.108 - ETA: 2:05 - loss: 3.9904 - acc: 0.108 - ETA: 2:04 - loss: 3.9885 - acc: 0.109 - ETA: 2:03 - loss: 3.9886 - acc: 0.109 - ETA: 2:01 - loss: 3.9874 - acc: 0.109 - ETA: 2:00 - loss: 3.9859 - acc: 0.109 - ETA: 1:59 - loss: 3.9857 - acc: 0.109 - ETA: 1:57 - loss: 3.9838 - acc: 0.109 - ETA: 1:56 - loss: 3.9855 - acc: 0.109 - ETA: 1:55 - loss: 3.9850 - acc: 0.109 - ETA: 1:53 - loss: 3.9821 - acc: 0.110 - ETA: 1:52 - loss: 3.9819 - acc: 0.109 - ETA: 1:51 - loss: 3.9824 - acc: 0.109 - ETA: 1:49 - loss: 3.9845 - acc: 0.109 - ETA: 1:48 - loss: 3.9828 - acc: 0.109 - ETA: 1:47 - loss: 3.9816 - acc: 0.110 - ETA: 1:46 - loss: 3.9805 - acc: 0.109 - ETA: 1:44 - loss: 3.9807 - acc: 0.110 - ETA: 1:43 - loss: 3.9814 - acc: 0.110 - ETA: 1:42 - loss: 3.9814 - acc: 0.110 - ETA: 1:40 - loss: 3.9822 - acc: 0.110 - ETA: 1:39 - loss: 3.9800 - acc: 0.110 - ETA: 1:38 - loss: 3.9805 - acc: 0.110 - ETA: 1:36 - loss: 3.9811 - acc: 0.109 - ETA: 1:35 - loss: 3.9802 - acc: 0.109 - ETA: 1:33 - loss: 3.9826 - acc: 0.109 - ETA: 1:32 - loss: 3.9843 - acc: 0.109 - ETA: 1:31 - loss: 3.9851 - acc: 0.109 - ETA: 1:29 - loss: 3.9861 - acc: 0.109 - ETA: 1:28 - loss: 3.9863 - acc: 0.109 - ETA: 1:27 - loss: 3.9869 - acc: 0.108 - ETA: 1:25 - loss: 3.9862 - acc: 0.109 - ETA: 1:24 - loss: 3.9861 - acc: 0.108 - ETA: 1:23 - loss: 3.9873 - acc: 0.108 - ETA: 1:21 - loss: 3.9874 - acc: 0.108 - ETA: 1:20 - loss: 3.9874 - acc: 0.108 - ETA: 1:18 - loss: 3.9867 - acc: 0.108 - ETA: 1:17 - loss: 3.9872 - acc: 0.108 - ETA: 1:16 - loss: 3.9861 - acc: 0.108 - ETA: 1:14 - loss: 3.9843 - acc: 0.108 - ETA: 1:13 - loss: 3.9843 - acc: 0.108 - ETA: 1:12 - loss: 3.9872 - acc: 0.108 - ETA: 1:10 - loss: 3.9868 - acc: 0.108 - ETA: 1:09 - loss: 3.9855 - acc: 0.108 - ETA: 1:07 - loss: 3.9857 - acc: 0.107 - ETA: 1:06 - loss: 3.9850 - acc: 0.108 - ETA: 1:05 - loss: 3.9838 - acc: 0.108 - ETA: 1:03 - loss: 3.9821 - acc: 0.108 - ETA: 1:02 - loss: 3.9809 - acc: 0.108 - ETA: 1:01 - loss: 3.9795 - acc: 0.107 - ETA: 59s - loss: 3.9783 - acc: 0.107 - ETA: 58s - loss: 3.9784 - acc: 0.10 - ETA: 56s - loss: 3.9775 - acc: 0.10 - ETA: 55s - loss: 3.9782 - acc: 0.10 - ETA: 54s - loss: 3.9780 - acc: 0.10 - ETA: 52s - loss: 3.9762 - acc: 0.10 - ETA: 51s - loss: 3.9756 - acc: 0.10 - ETA: 49s - loss: 3.9763 - acc: 0.10 - ETA: 48s - loss: 3.9767 - acc: 0.10 - ETA: 47s - loss: 3.9773 - acc: 0.10 - ETA: 45s - loss: 3.9771 - acc: 0.10 - ETA: 44s - loss: 3.9780 - acc: 0.10 - ETA: 43s - loss: 3.9768 - acc: 0.10 - ETA: 41s - loss: 3.9733 - acc: 0.10 - ETA: 40s - loss: 3.9743 - acc: 0.10 - ETA: 38s - loss: 3.9752 - acc: 0.10 - ETA: 37s - loss: 3.9761 - acc: 0.10 - ETA: 36s - loss: 3.9757 - acc: 0.10 - ETA: 34s - loss: 3.9769 - acc: 0.10 - ETA: 33s - loss: 3.9761 - acc: 0.10 - ETA: 31s - loss: 3.9776 - acc: 0.10 - ETA: 30s - loss: 3.9775 - acc: 0.10 - ETA: 29s - loss: 3.9762 - acc: 0.10 - ETA: 27s - loss: 3.9755 - acc: 0.10 - ETA: 26s - loss: 3.9751 - acc: 0.10 - ETA: 24s - loss: 3.9758 - acc: 0.10 - ETA: 23s - loss: 3.9760 - acc: 0.10 - ETA: 22s - loss: 3.9760 - acc: 0.10 - ETA: 20s - loss: 3.9764 - acc: 0.10 - ETA: 19s - loss: 3.9756 - acc: 0.10 - ETA: 17s - loss: 3.9749 - acc: 0.10 - ETA: 16s - loss: 3.9741 - acc: 0.10 - ETA: 15s - loss: 3.9747 - acc: 0.10 - ETA: 13s - loss: 3.9755 - acc: 0.10 - ETA: 12s - loss: 3.9760 - acc: 0.10 - ETA: 11s - loss: 3.9763 - acc: 0.10 - ETA: 9s - loss: 3.9764 - acc: 0.1075 - ETA: 8s - loss: 3.9764 - acc: 0.107 - ETA: 6s - loss: 3.9746 - acc: 0.107 - ETA: 5s - loss: 3.9734 - acc: 0.108 - ETA: 4s - loss: 3.9728 - acc: 0.108 - ETA: 2s - loss: 3.9723 - acc: 0.108 - ETA: 1s - loss: 3.9714 - acc: 0.108 - 474s 71ms/step - loss: 3.9693 - acc: 0.1087 - val\_loss: 4.1693 - val\_acc: 0.0910

Epoch 00003: val\_loss improved from 4.30611 to 4.16925, saving model to saved\_models/weights.best.from\_scratch.hdf5
Epoch 4/10
4080/6680 [=================>{\ldots}] - ETA: 6:38 - loss: 3.3179 - acc: 0.250 - ETA: 6:39 - loss: 3.4320 - acc: 0.250 - ETA: 6:37 - loss: 3.2445 - acc: 0.266 - ETA: 6:41 - loss: 3.2382 - acc: 0.262 - ETA: 6:51 - loss: 3.2468 - acc: 0.260 - ETA: 6:51 - loss: 3.2586 - acc: 0.241 - ETA: 6:48 - loss: 3.2424 - acc: 0.257 - ETA: 7:00 - loss: 3.2958 - acc: 0.262 - ETA: 7:11 - loss: 3.3673 - acc: 0.250 - ETA: 7:19 - loss: 3.3912 - acc: 0.245 - ETA: 7:27 - loss: 3.3417 - acc: 0.240 - ETA: 7:26 - loss: 3.2877 - acc: 0.250 - ETA: 7:35 - loss: 3.2656 - acc: 0.253 - ETA: 7:40 - loss: 3.2987 - acc: 0.242 - ETA: 7:34 - loss: 3.2623 - acc: 0.253 - ETA: 7:29 - loss: 3.3044 - acc: 0.250 - ETA: 7:27 - loss: 3.2983 - acc: 0.241 - ETA: 7:30 - loss: 3.3110 - acc: 0.244 - ETA: 7:35 - loss: 3.3125 - acc: 0.242 - ETA: 7:31 - loss: 3.3212 - acc: 0.240 - ETA: 7:35 - loss: 3.3355 - acc: 0.233 - ETA: 7:44 - loss: 3.3366 - acc: 0.236 - ETA: 7:48 - loss: 3.3199 - acc: 0.237 - ETA: 7:43 - loss: 3.3295 - acc: 0.235 - ETA: 7:39 - loss: 3.3339 - acc: 0.232 - ETA: 7:37 - loss: 3.3267 - acc: 0.228 - ETA: 7:32 - loss: 3.3321 - acc: 0.229 - ETA: 7:30 - loss: 3.3212 - acc: 0.230 - ETA: 7:25 - loss: 3.3122 - acc: 0.232 - ETA: 7:22 - loss: 3.3286 - acc: 0.226 - ETA: 7:18 - loss: 3.3401 - acc: 0.225 - ETA: 7:15 - loss: 3.3417 - acc: 0.228 - ETA: 7:12 - loss: 3.3503 - acc: 0.222 - ETA: 7:08 - loss: 3.3571 - acc: 0.220 - ETA: 7:04 - loss: 3.3457 - acc: 0.224 - ETA: 7:01 - loss: 3.3400 - acc: 0.225 - ETA: 6:59 - loss: 3.3233 - acc: 0.225 - ETA: 6:57 - loss: 3.3350 - acc: 0.222 - ETA: 6:56 - loss: 3.3477 - acc: 0.219 - ETA: 6:54 - loss: 3.3389 - acc: 0.220 - ETA: 6:51 - loss: 3.3258 - acc: 0.220 - ETA: 6:49 - loss: 3.3261 - acc: 0.221 - ETA: 6:46 - loss: 3.3230 - acc: 0.222 - ETA: 6:44 - loss: 3.3147 - acc: 0.221 - ETA: 6:42 - loss: 3.3228 - acc: 0.221 - ETA: 6:39 - loss: 3.3238 - acc: 0.225 - ETA: 6:36 - loss: 3.3324 - acc: 0.223 - ETA: 6:34 - loss: 3.3317 - acc: 0.220 - ETA: 6:31 - loss: 3.3366 - acc: 0.218 - ETA: 6:30 - loss: 3.3501 - acc: 0.215 - ETA: 6:28 - loss: 3.3570 - acc: 0.213 - ETA: 6:26 - loss: 3.3422 - acc: 0.218 - ETA: 6:23 - loss: 3.3435 - acc: 0.217 - ETA: 6:21 - loss: 3.3444 - acc: 0.218 - ETA: 6:23 - loss: 3.3366 - acc: 0.222 - ETA: 6:23 - loss: 3.3275 - acc: 0.227 - ETA: 6:22 - loss: 3.3183 - acc: 0.230 - ETA: 6:21 - loss: 3.3251 - acc: 0.229 - ETA: 6:20 - loss: 3.3190 - acc: 0.229 - ETA: 6:20 - loss: 3.3185 - acc: 0.230 - ETA: 6:20 - loss: 3.3072 - acc: 0.232 - ETA: 6:19 - loss: 3.3032 - acc: 0.233 - ETA: 6:18 - loss: 3.3129 - acc: 0.232 - ETA: 6:16 - loss: 3.3107 - acc: 0.230 - ETA: 6:15 - loss: 3.3071 - acc: 0.232 - ETA: 6:15 - loss: 3.3033 - acc: 0.233 - ETA: 6:14 - loss: 3.3022 - acc: 0.234 - ETA: 6:14 - loss: 3.3071 - acc: 0.233 - ETA: 6:14 - loss: 3.3141 - acc: 0.232 - ETA: 6:13 - loss: 3.3135 - acc: 0.232 - ETA: 6:12 - loss: 3.3144 - acc: 0.233 - ETA: 6:11 - loss: 3.3091 - acc: 0.233 - ETA: 6:10 - loss: 3.3074 - acc: 0.233 - ETA: 6:09 - loss: 3.3167 - acc: 0.233 - ETA: 6:07 - loss: 3.3178 - acc: 0.234 - ETA: 6:06 - loss: 3.3254 - acc: 0.232 - ETA: 6:04 - loss: 3.3171 - acc: 0.232 - ETA: 6:02 - loss: 3.3150 - acc: 0.231 - ETA: 6:00 - loss: 3.3173 - acc: 0.229 - ETA: 5:58 - loss: 3.3185 - acc: 0.228 - ETA: 5:56 - loss: 3.3178 - acc: 0.227 - ETA: 5:54 - loss: 3.3166 - acc: 0.227 - ETA: 5:52 - loss: 3.3202 - acc: 0.225 - ETA: 5:50 - loss: 3.3195 - acc: 0.226 - ETA: 5:48 - loss: 3.3196 - acc: 0.224 - ETA: 5:46 - loss: 3.3171 - acc: 0.225 - ETA: 5:44 - loss: 3.3119 - acc: 0.226 - ETA: 5:41 - loss: 3.3210 - acc: 0.225 - ETA: 5:39 - loss: 3.3232 - acc: 0.224 - ETA: 5:37 - loss: 3.3217 - acc: 0.224 - ETA: 5:35 - loss: 3.3135 - acc: 0.225 - ETA: 5:33 - loss: 3.3066 - acc: 0.227 - ETA: 5:31 - loss: 3.3119 - acc: 0.225 - ETA: 5:29 - loss: 3.3113 - acc: 0.225 - ETA: 5:27 - loss: 3.3119 - acc: 0.224 - ETA: 5:26 - loss: 3.3067 - acc: 0.224 - ETA: 5:24 - loss: 3.3026 - acc: 0.225 - ETA: 5:23 - loss: 3.3024 - acc: 0.226 - ETA: 5:21 - loss: 3.3069 - acc: 0.224 - ETA: 5:20 - loss: 3.3034 - acc: 0.225 - ETA: 5:18 - loss: 3.3063 - acc: 0.225 - ETA: 5:16 - loss: 3.3054 - acc: 0.225 - ETA: 5:14 - loss: 3.3022 - acc: 0.225 - ETA: 5:13 - loss: 3.3045 - acc: 0.225 - ETA: 5:12 - loss: 3.3077 - acc: 0.224 - ETA: 5:10 - loss: 3.3057 - acc: 0.224 - ETA: 5:10 - loss: 3.3107 - acc: 0.222 - ETA: 5:09 - loss: 3.3105 - acc: 0.223 - ETA: 5:08 - loss: 3.3122 - acc: 0.222 - ETA: 5:07 - loss: 3.3131 - acc: 0.223 - ETA: 5:06 - loss: 3.3106 - acc: 0.224 - ETA: 5:05 - loss: 3.3092 - acc: 0.225 - ETA: 5:04 - loss: 3.3088 - acc: 0.225 - ETA: 5:03 - loss: 3.3090 - acc: 0.225 - ETA: 5:02 - loss: 3.3066 - acc: 0.225 - ETA: 5:01 - loss: 3.3050 - acc: 0.223 - ETA: 4:59 - loss: 3.3040 - acc: 0.223 - ETA: 4:58 - loss: 3.3051 - acc: 0.224 - ETA: 4:56 - loss: 3.3036 - acc: 0.223 - ETA: 4:55 - loss: 3.2998 - acc: 0.225 - ETA: 4:54 - loss: 3.2990 - acc: 0.226 - ETA: 4:52 - loss: 3.2964 - acc: 0.225 - ETA: 4:51 - loss: 3.2901 - acc: 0.226 - ETA: 4:49 - loss: 3.2945 - acc: 0.225 - ETA: 4:47 - loss: 3.2958 - acc: 0.224 - ETA: 4:45 - loss: 3.2977 - acc: 0.225 - ETA: 4:44 - loss: 3.2951 - acc: 0.225 - ETA: 4:43 - loss: 3.2972 - acc: 0.224 - ETA: 4:42 - loss: 3.2944 - acc: 0.225 - ETA: 4:41 - loss: 3.2912 - acc: 0.225 - ETA: 4:41 - loss: 3.2913 - acc: 0.224 - ETA: 4:41 - loss: 3.2903 - acc: 0.224 - ETA: 4:40 - loss: 3.2918 - acc: 0.225 - ETA: 4:39 - loss: 3.2909 - acc: 0.225 - ETA: 4:39 - loss: 3.2867 - acc: 0.226 - ETA: 4:38 - loss: 3.2851 - acc: 0.226 - ETA: 4:36 - loss: 3.2837 - acc: 0.226 - ETA: 4:36 - loss: 3.2892 - acc: 0.225 - ETA: 4:35 - loss: 3.2919 - acc: 0.225 - ETA: 4:33 - loss: 3.2929 - acc: 0.224 - ETA: 4:32 - loss: 3.2941 - acc: 0.224 - ETA: 4:31 - loss: 3.2922 - acc: 0.225 - ETA: 4:29 - loss: 3.2888 - acc: 0.225 - ETA: 4:28 - loss: 3.2898 - acc: 0.225 - ETA: 4:26 - loss: 3.2885 - acc: 0.225 - ETA: 4:25 - loss: 3.2862 - acc: 0.226 - ETA: 4:24 - loss: 3.2863 - acc: 0.226 - ETA: 4:22 - loss: 3.2875 - acc: 0.225 - ETA: 4:21 - loss: 3.2918 - acc: 0.225 - ETA: 4:19 - loss: 3.2913 - acc: 0.225 - ETA: 4:18 - loss: 3.2913 - acc: 0.225 - ETA: 4:16 - loss: 3.2874 - acc: 0.226 - ETA: 4:14 - loss: 3.2887 - acc: 0.226 - ETA: 4:13 - loss: 3.2827 - acc: 0.226 - ETA: 4:11 - loss: 3.2828 - acc: 0.227 - ETA: 4:10 - loss: 3.2827 - acc: 0.227 - ETA: 4:08 - loss: 3.2820 - acc: 0.228 - ETA: 4:06 - loss: 3.2789 - acc: 0.228 - ETA: 4:04 - loss: 3.2807 - acc: 0.227 - ETA: 4:03 - loss: 3.2811 - acc: 0.227 - ETA: 4:01 - loss: 3.2775 - acc: 0.227 - ETA: 3:59 - loss: 3.2747 - acc: 0.227 - ETA: 3:58 - loss: 3.2746 - acc: 0.227 - ETA: 3:56 - loss: 3.2779 - acc: 0.227 - ETA: 3:54 - loss: 3.2786 - acc: 0.227 - ETA: 3:53 - loss: 3.2788 - acc: 0.228 - ETA: 3:51 - loss: 3.2811 - acc: 0.227 - ETA: 3:49 - loss: 3.2797 - acc: 0.228 - ETA: 3:48 - loss: 3.2801 - acc: 0.228 - ETA: 3:46 - loss: 3.2816 - acc: 0.228 - ETA: 3:44 - loss: 3.2815 - acc: 0.228 - ETA: 3:43 - loss: 3.2841 - acc: 0.228 - ETA: 3:41 - loss: 3.2865 - acc: 0.228 - ETA: 3:40 - loss: 3.2870 - acc: 0.227 - ETA: 3:38 - loss: 3.2857 - acc: 0.227 - ETA: 3:36 - loss: 3.2887 - acc: 0.227 - ETA: 3:35 - loss: 3.2907 - acc: 0.227 - ETA: 3:33 - loss: 3.2886 - acc: 0.227 - ETA: 3:31 - loss: 3.2885 - acc: 0.227 - ETA: 3:30 - loss: 3.2887 - acc: 0.228 - ETA: 3:28 - loss: 3.2902 - acc: 0.228 - ETA: 3:27 - loss: 3.2909 - acc: 0.228 - ETA: 3:25 - loss: 3.2878 - acc: 0.227 - ETA: 3:24 - loss: 3.2882 - acc: 0.227 - ETA: 3:22 - loss: 3.2888 - acc: 0.227 - ETA: 3:20 - loss: 3.2912 - acc: 0.226 - ETA: 3:19 - loss: 3.2886 - acc: 0.227 - ETA: 3:17 - loss: 3.2881 - acc: 0.227 - ETA: 3:16 - loss: 3.2878 - acc: 0.226 - ETA: 3:14 - loss: 3.2899 - acc: 0.226 - ETA: 3:13 - loss: 3.2890 - acc: 0.227 - ETA: 3:11 - loss: 3.2898 - acc: 0.227 - ETA: 3:10 - loss: 3.2880 - acc: 0.227 - ETA: 3:08 - loss: 3.2910 - acc: 0.227 - ETA: 3:07 - loss: 3.2925 - acc: 0.226 - ETA: 3:05 - loss: 3.2942 - acc: 0.226 - ETA: 3:04 - loss: 3.2935 - acc: 0.226 - ETA: 3:02 - loss: 3.2922 - acc: 0.226 - ETA: 3:01 - loss: 3.2885 - acc: 0.227 - ETA: 2:59 - loss: 3.2867 - acc: 0.227 - ETA: 2:58 - loss: 3.2886 - acc: 0.226 - ETA: 2:56 - loss: 3.2883 - acc: 0.227 - ETA: 2:55 - loss: 3.2885 - acc: 0.226 - ETA: 2:53 - loss: 3.2867 - acc: 0.22756680/6680 [==============================] - ETA: 2:52 - loss: 3.2870 - acc: 0.226 - ETA: 2:50 - loss: 3.2902 - acc: 0.226 - ETA: 2:49 - loss: 3.2909 - acc: 0.226 - ETA: 2:47 - loss: 3.2952 - acc: 0.226 - ETA: 2:46 - loss: 3.2946 - acc: 0.226 - ETA: 2:44 - loss: 3.2983 - acc: 0.226 - ETA: 2:43 - loss: 3.2981 - acc: 0.226 - ETA: 2:41 - loss: 3.3002 - acc: 0.226 - ETA: 2:40 - loss: 3.2986 - acc: 0.226 - ETA: 2:38 - loss: 3.2980 - acc: 0.226 - ETA: 2:37 - loss: 3.2987 - acc: 0.227 - ETA: 2:36 - loss: 3.2989 - acc: 0.226 - ETA: 2:34 - loss: 3.2971 - acc: 0.227 - ETA: 2:33 - loss: 3.2978 - acc: 0.227 - ETA: 2:31 - loss: 3.2977 - acc: 0.227 - ETA: 2:30 - loss: 3.2978 - acc: 0.227 - ETA: 2:28 - loss: 3.2993 - acc: 0.226 - ETA: 2:27 - loss: 3.2999 - acc: 0.227 - ETA: 2:26 - loss: 3.3006 - acc: 0.226 - ETA: 2:24 - loss: 3.2991 - acc: 0.227 - ETA: 2:23 - loss: 3.2966 - acc: 0.227 - ETA: 2:21 - loss: 3.2988 - acc: 0.227 - ETA: 2:20 - loss: 3.2981 - acc: 0.227 - ETA: 2:19 - loss: 3.2991 - acc: 0.227 - ETA: 2:17 - loss: 3.2969 - acc: 0.227 - ETA: 2:16 - loss: 3.2979 - acc: 0.227 - ETA: 2:14 - loss: 3.2987 - acc: 0.226 - ETA: 2:13 - loss: 3.2986 - acc: 0.226 - ETA: 2:12 - loss: 3.2960 - acc: 0.226 - ETA: 2:10 - loss: 3.2962 - acc: 0.226 - ETA: 2:09 - loss: 3.2963 - acc: 0.226 - ETA: 2:07 - loss: 3.2961 - acc: 0.226 - ETA: 2:06 - loss: 3.2959 - acc: 0.226 - ETA: 2:05 - loss: 3.2958 - acc: 0.226 - ETA: 2:03 - loss: 3.2975 - acc: 0.227 - ETA: 2:02 - loss: 3.2960 - acc: 0.227 - ETA: 2:01 - loss: 3.2970 - acc: 0.227 - ETA: 1:59 - loss: 3.2980 - acc: 0.226 - ETA: 1:58 - loss: 3.2999 - acc: 0.227 - ETA: 1:56 - loss: 3.2995 - acc: 0.226 - ETA: 1:55 - loss: 3.2989 - acc: 0.226 - ETA: 1:54 - loss: 3.2974 - acc: 0.227 - ETA: 1:52 - loss: 3.2971 - acc: 0.227 - ETA: 1:51 - loss: 3.2964 - acc: 0.227 - ETA: 1:50 - loss: 3.2991 - acc: 0.226 - ETA: 1:48 - loss: 3.2986 - acc: 0.226 - ETA: 1:47 - loss: 3.2981 - acc: 0.226 - ETA: 1:46 - loss: 3.2989 - acc: 0.226 - ETA: 1:44 - loss: 3.2984 - acc: 0.226 - ETA: 1:43 - loss: 3.2962 - acc: 0.226 - ETA: 1:42 - loss: 3.2956 - acc: 0.226 - ETA: 1:40 - loss: 3.2963 - acc: 0.225 - ETA: 1:39 - loss: 3.2960 - acc: 0.225 - ETA: 1:38 - loss: 3.2935 - acc: 0.226 - ETA: 1:36 - loss: 3.2938 - acc: 0.226 - ETA: 1:35 - loss: 3.2942 - acc: 0.226 - ETA: 1:34 - loss: 3.2916 - acc: 0.227 - ETA: 1:32 - loss: 3.2928 - acc: 0.227 - ETA: 1:31 - loss: 3.2916 - acc: 0.227 - ETA: 1:30 - loss: 3.2927 - acc: 0.226 - ETA: 1:28 - loss: 3.2912 - acc: 0.227 - ETA: 1:27 - loss: 3.2899 - acc: 0.226 - ETA: 1:26 - loss: 3.2921 - acc: 0.227 - ETA: 1:24 - loss: 3.2929 - acc: 0.226 - ETA: 1:23 - loss: 3.2923 - acc: 0.226 - ETA: 1:22 - loss: 3.2914 - acc: 0.226 - ETA: 1:20 - loss: 3.2939 - acc: 0.226 - ETA: 1:19 - loss: 3.2919 - acc: 0.227 - ETA: 1:18 - loss: 3.2915 - acc: 0.227 - ETA: 1:16 - loss: 3.2917 - acc: 0.227 - ETA: 1:15 - loss: 3.2898 - acc: 0.228 - ETA: 1:14 - loss: 3.2911 - acc: 0.228 - ETA: 1:12 - loss: 3.2892 - acc: 0.228 - ETA: 1:11 - loss: 3.2906 - acc: 0.228 - ETA: 1:10 - loss: 3.2901 - acc: 0.228 - ETA: 1:08 - loss: 3.2896 - acc: 0.228 - ETA: 1:07 - loss: 3.2909 - acc: 0.227 - ETA: 1:06 - loss: 3.2895 - acc: 0.228 - ETA: 1:05 - loss: 3.2890 - acc: 0.228 - ETA: 1:03 - loss: 3.2889 - acc: 0.228 - ETA: 1:02 - loss: 3.2888 - acc: 0.228 - ETA: 1:01 - loss: 3.2870 - acc: 0.229 - ETA: 59s - loss: 3.2875 - acc: 0.229 - ETA: 58s - loss: 3.2860 - acc: 0.22 - ETA: 57s - loss: 3.2851 - acc: 0.22 - ETA: 55s - loss: 3.2865 - acc: 0.22 - ETA: 54s - loss: 3.2867 - acc: 0.22 - ETA: 53s - loss: 3.2856 - acc: 0.23 - ETA: 52s - loss: 3.2867 - acc: 0.23 - ETA: 50s - loss: 3.2856 - acc: 0.23 - ETA: 49s - loss: 3.2867 - acc: 0.22 - ETA: 48s - loss: 3.2880 - acc: 0.22 - ETA: 46s - loss: 3.2848 - acc: 0.22 - ETA: 45s - loss: 3.2835 - acc: 0.23 - ETA: 44s - loss: 3.2820 - acc: 0.23 - ETA: 43s - loss: 3.2815 - acc: 0.23 - ETA: 41s - loss: 3.2798 - acc: 0.23 - ETA: 40s - loss: 3.2800 - acc: 0.23 - ETA: 39s - loss: 3.2816 - acc: 0.22 - ETA: 37s - loss: 3.2819 - acc: 0.22 - ETA: 36s - loss: 3.2813 - acc: 0.22 - ETA: 35s - loss: 3.2808 - acc: 0.23 - ETA: 34s - loss: 3.2811 - acc: 0.23 - ETA: 32s - loss: 3.2795 - acc: 0.23 - ETA: 31s - loss: 3.2819 - acc: 0.22 - ETA: 30s - loss: 3.2807 - acc: 0.22 - ETA: 28s - loss: 3.2801 - acc: 0.22 - ETA: 27s - loss: 3.2793 - acc: 0.22 - ETA: 26s - loss: 3.2779 - acc: 0.23 - ETA: 25s - loss: 3.2782 - acc: 0.23 - ETA: 23s - loss: 3.2770 - acc: 0.23 - ETA: 22s - loss: 3.2755 - acc: 0.23 - ETA: 21s - loss: 3.2755 - acc: 0.23 - ETA: 20s - loss: 3.2746 - acc: 0.23 - ETA: 18s - loss: 3.2750 - acc: 0.22 - ETA: 17s - loss: 3.2746 - acc: 0.22 - ETA: 16s - loss: 3.2761 - acc: 0.22 - ETA: 15s - loss: 3.2733 - acc: 0.23 - ETA: 13s - loss: 3.2728 - acc: 0.23 - ETA: 12s - loss: 3.2735 - acc: 0.23 - ETA: 11s - loss: 3.2737 - acc: 0.22 - ETA: 10s - loss: 3.2745 - acc: 0.22 - ETA: 8s - loss: 3.2736 - acc: 0.2303 - ETA: 7s - loss: 3.2742 - acc: 0.230 - ETA: 6s - loss: 3.2746 - acc: 0.230 - ETA: 5s - loss: 3.2760 - acc: 0.230 - ETA: 3s - loss: 3.2768 - acc: 0.229 - ETA: 2s - loss: 3.2774 - acc: 0.229 - ETA: 1s - loss: 3.2755 - acc: 0.229 - 431s 65ms/step - loss: 3.2759 - acc: 0.2299 - val\_loss: 4.1516 - val\_acc: 0.0898

Epoch 00004: val\_loss improved from 4.16925 to 4.15157, saving model to saved\_models/weights.best.from\_scratch.hdf5
Epoch 5/10
4080/6680 [=================>{\ldots}] - ETA: 6:00 - loss: 2.1637 - acc: 0.450 - ETA: 6:04 - loss: 2.3570 - acc: 0.450 - ETA: 6:03 - loss: 2.4705 - acc: 0.433 - ETA: 6:03 - loss: 2.3725 - acc: 0.450 - ETA: 6:01 - loss: 2.3836 - acc: 0.430 - ETA: 6:00 - loss: 2.4775 - acc: 0.416 - ETA: 5:59 - loss: 2.4972 - acc: 0.407 - ETA: 5:57 - loss: 2.5033 - acc: 0.406 - ETA: 5:57 - loss: 2.4320 - acc: 0.422 - ETA: 5:56 - loss: 2.3955 - acc: 0.425 - ETA: 5:54 - loss: 2.3772 - acc: 0.427 - ETA: 5:53 - loss: 2.3644 - acc: 0.433 - ETA: 5:52 - loss: 2.4243 - acc: 0.423 - ETA: 5:50 - loss: 2.3918 - acc: 0.425 - ETA: 5:50 - loss: 2.3692 - acc: 0.433 - ETA: 5:50 - loss: 2.3977 - acc: 0.425 - ETA: 5:49 - loss: 2.3991 - acc: 0.429 - ETA: 5:48 - loss: 2.3621 - acc: 0.436 - ETA: 5:47 - loss: 2.3856 - acc: 0.426 - ETA: 5:46 - loss: 2.4126 - acc: 0.422 - ETA: 5:45 - loss: 2.4252 - acc: 0.419 - ETA: 5:44 - loss: 2.3910 - acc: 0.427 - ETA: 5:45 - loss: 2.3619 - acc: 0.432 - ETA: 5:46 - loss: 2.3568 - acc: 0.427 - ETA: 5:45 - loss: 2.3561 - acc: 0.422 - ETA: 5:44 - loss: 2.3630 - acc: 0.425 - ETA: 5:42 - loss: 2.3388 - acc: 0.433 - ETA: 5:41 - loss: 2.3377 - acc: 0.435 - ETA: 5:40 - loss: 2.3447 - acc: 0.436 - ETA: 5:38 - loss: 2.3494 - acc: 0.433 - ETA: 5:37 - loss: 2.3414 - acc: 0.435 - ETA: 5:36 - loss: 2.3309 - acc: 0.440 - ETA: 5:35 - loss: 2.3361 - acc: 0.436 - ETA: 5:34 - loss: 2.3324 - acc: 0.433 - ETA: 5:32 - loss: 2.3386 - acc: 0.430 - ETA: 5:32 - loss: 2.3258 - acc: 0.431 - ETA: 5:31 - loss: 2.3277 - acc: 0.431 - ETA: 5:29 - loss: 2.3327 - acc: 0.428 - ETA: 5:29 - loss: 2.3152 - acc: 0.433 - ETA: 5:28 - loss: 2.3268 - acc: 0.428 - ETA: 5:28 - loss: 2.3403 - acc: 0.424 - ETA: 5:28 - loss: 2.3250 - acc: 0.429 - ETA: 5:27 - loss: 2.3339 - acc: 0.429 - ETA: 5:27 - loss: 2.3359 - acc: 0.431 - ETA: 5:26 - loss: 2.3511 - acc: 0.427 - ETA: 5:24 - loss: 2.3517 - acc: 0.429 - ETA: 5:23 - loss: 2.3376 - acc: 0.430 - ETA: 5:21 - loss: 2.3314 - acc: 0.430 - ETA: 5:20 - loss: 2.3364 - acc: 0.429 - ETA: 5:19 - loss: 2.3370 - acc: 0.428 - ETA: 5:17 - loss: 2.3337 - acc: 0.429 - ETA: 5:16 - loss: 2.3390 - acc: 0.426 - ETA: 5:15 - loss: 2.3387 - acc: 0.425 - ETA: 5:14 - loss: 2.3392 - acc: 0.426 - ETA: 5:12 - loss: 2.3347 - acc: 0.424 - ETA: 5:11 - loss: 2.3302 - acc: 0.425 - ETA: 5:09 - loss: 2.3382 - acc: 0.421 - ETA: 5:08 - loss: 2.3335 - acc: 0.424 - ETA: 5:07 - loss: 2.3349 - acc: 0.422 - ETA: 5:06 - loss: 2.3312 - acc: 0.424 - ETA: 5:04 - loss: 2.3299 - acc: 0.425 - ETA: 5:03 - loss: 2.3197 - acc: 0.428 - ETA: 5:02 - loss: 2.3285 - acc: 0.427 - ETA: 5:00 - loss: 2.3226 - acc: 0.428 - ETA: 4:59 - loss: 2.3219 - acc: 0.428 - ETA: 4:58 - loss: 2.3110 - acc: 0.431 - ETA: 4:57 - loss: 2.2985 - acc: 0.435 - ETA: 4:55 - loss: 2.2922 - acc: 0.436 - ETA: 4:54 - loss: 2.2920 - acc: 0.438 - ETA: 4:53 - loss: 2.2923 - acc: 0.437 - ETA: 4:52 - loss: 2.2865 - acc: 0.437 - ETA: 4:51 - loss: 2.2920 - acc: 0.435 - ETA: 4:49 - loss: 2.2969 - acc: 0.434 - ETA: 4:48 - loss: 2.3070 - acc: 0.431 - ETA: 4:47 - loss: 2.3099 - acc: 0.431 - ETA: 4:46 - loss: 2.3084 - acc: 0.431 - ETA: 4:45 - loss: 2.3067 - acc: 0.431 - ETA: 4:43 - loss: 2.3099 - acc: 0.430 - ETA: 4:42 - loss: 2.3129 - acc: 0.429 - ETA: 4:41 - loss: 2.3131 - acc: 0.429 - ETA: 4:40 - loss: 2.3221 - acc: 0.428 - ETA: 4:39 - loss: 2.3188 - acc: 0.428 - ETA: 4:37 - loss: 2.3228 - acc: 0.427 - ETA: 4:36 - loss: 2.3223 - acc: 0.426 - ETA: 4:35 - loss: 2.3138 - acc: 0.428 - ETA: 4:34 - loss: 2.3145 - acc: 0.428 - ETA: 4:33 - loss: 2.3194 - acc: 0.427 - ETA: 4:32 - loss: 2.3190 - acc: 0.428 - ETA: 4:30 - loss: 2.3194 - acc: 0.428 - ETA: 4:29 - loss: 2.3083 - acc: 0.431 - ETA: 4:28 - loss: 2.3081 - acc: 0.431 - ETA: 4:27 - loss: 2.3109 - acc: 0.431 - ETA: 4:26 - loss: 2.3025 - acc: 0.433 - ETA: 4:25 - loss: 2.2999 - acc: 0.433 - ETA: 4:23 - loss: 2.3018 - acc: 0.432 - ETA: 4:22 - loss: 2.3098 - acc: 0.431 - ETA: 4:21 - loss: 2.3023 - acc: 0.432 - ETA: 4:20 - loss: 2.3011 - acc: 0.432 - ETA: 4:19 - loss: 2.3025 - acc: 0.432 - ETA: 4:17 - loss: 2.3123 - acc: 0.430 - ETA: 4:16 - loss: 2.3155 - acc: 0.429 - ETA: 4:15 - loss: 2.3188 - acc: 0.428 - ETA: 4:14 - loss: 2.3182 - acc: 0.427 - ETA: 4:13 - loss: 2.3224 - acc: 0.428 - ETA: 4:12 - loss: 2.3294 - acc: 0.427 - ETA: 4:11 - loss: 2.3286 - acc: 0.427 - ETA: 4:10 - loss: 2.3264 - acc: 0.429 - ETA: 4:08 - loss: 2.3242 - acc: 0.429 - ETA: 4:07 - loss: 2.3234 - acc: 0.428 - ETA: 4:06 - loss: 2.3263 - acc: 0.428 - ETA: 4:05 - loss: 2.3261 - acc: 0.428 - ETA: 4:04 - loss: 2.3240 - acc: 0.429 - ETA: 4:03 - loss: 2.3220 - acc: 0.429 - ETA: 4:02 - loss: 2.3233 - acc: 0.429 - ETA: 4:01 - loss: 2.3183 - acc: 0.430 - ETA: 3:59 - loss: 2.3216 - acc: 0.429 - ETA: 3:58 - loss: 2.3223 - acc: 0.429 - ETA: 3:57 - loss: 2.3179 - acc: 0.431 - ETA: 3:56 - loss: 2.3132 - acc: 0.432 - ETA: 3:55 - loss: 2.3154 - acc: 0.433 - ETA: 3:54 - loss: 2.3204 - acc: 0.433 - ETA: 3:53 - loss: 2.3224 - acc: 0.432 - ETA: 3:51 - loss: 2.3243 - acc: 0.432 - ETA: 3:50 - loss: 2.3235 - acc: 0.431 - ETA: 3:49 - loss: 2.3217 - acc: 0.431 - ETA: 3:48 - loss: 2.3184 - acc: 0.431 - ETA: 3:47 - loss: 2.3143 - acc: 0.431 - ETA: 3:46 - loss: 2.3163 - acc: 0.431 - ETA: 3:45 - loss: 2.3158 - acc: 0.431 - ETA: 3:44 - loss: 2.3170 - acc: 0.430 - ETA: 3:43 - loss: 2.3192 - acc: 0.430 - ETA: 3:41 - loss: 2.3197 - acc: 0.431 - ETA: 3:40 - loss: 2.3174 - acc: 0.432 - ETA: 3:39 - loss: 2.3170 - acc: 0.432 - ETA: 3:38 - loss: 2.3194 - acc: 0.432 - ETA: 3:37 - loss: 2.3164 - acc: 0.432 - ETA: 3:36 - loss: 2.3200 - acc: 0.431 - ETA: 3:35 - loss: 2.3182 - acc: 0.431 - ETA: 3:34 - loss: 2.3177 - acc: 0.432 - ETA: 3:33 - loss: 2.3209 - acc: 0.431 - ETA: 3:31 - loss: 2.3209 - acc: 0.431 - ETA: 3:30 - loss: 2.3225 - acc: 0.431 - ETA: 3:29 - loss: 2.3247 - acc: 0.430 - ETA: 3:28 - loss: 2.3269 - acc: 0.429 - ETA: 3:27 - loss: 2.3304 - acc: 0.427 - ETA: 3:26 - loss: 2.3290 - acc: 0.428 - ETA: 3:25 - loss: 2.3307 - acc: 0.427 - ETA: 3:24 - loss: 2.3315 - acc: 0.427 - ETA: 3:23 - loss: 2.3299 - acc: 0.426 - ETA: 3:22 - loss: 2.3294 - acc: 0.426 - ETA: 3:20 - loss: 2.3270 - acc: 0.426 - ETA: 3:19 - loss: 2.3274 - acc: 0.426 - ETA: 3:18 - loss: 2.3251 - acc: 0.426 - ETA: 3:17 - loss: 2.3275 - acc: 0.426 - ETA: 3:16 - loss: 2.3265 - acc: 0.427 - ETA: 3:15 - loss: 2.3252 - acc: 0.427 - ETA: 3:14 - loss: 2.3276 - acc: 0.427 - ETA: 3:13 - loss: 2.3304 - acc: 0.426 - ETA: 3:12 - loss: 2.3293 - acc: 0.426 - ETA: 3:11 - loss: 2.3302 - acc: 0.425 - ETA: 3:10 - loss: 2.3282 - acc: 0.426 - ETA: 3:08 - loss: 2.3293 - acc: 0.425 - ETA: 3:07 - loss: 2.3303 - acc: 0.425 - ETA: 3:06 - loss: 2.3308 - acc: 0.424 - ETA: 3:05 - loss: 2.3325 - acc: 0.424 - ETA: 3:04 - loss: 2.3364 - acc: 0.424 - ETA: 3:03 - loss: 2.3351 - acc: 0.424 - ETA: 3:02 - loss: 2.3321 - acc: 0.425 - ETA: 3:01 - loss: 2.3320 - acc: 0.425 - ETA: 3:00 - loss: 2.3314 - acc: 0.425 - ETA: 2:59 - loss: 2.3366 - acc: 0.424 - ETA: 2:57 - loss: 2.3347 - acc: 0.424 - ETA: 2:56 - loss: 2.3322 - acc: 0.424 - ETA: 2:55 - loss: 2.3310 - acc: 0.424 - ETA: 2:54 - loss: 2.3316 - acc: 0.424 - ETA: 2:53 - loss: 2.3371 - acc: 0.423 - ETA: 2:52 - loss: 2.3357 - acc: 0.423 - ETA: 2:51 - loss: 2.3380 - acc: 0.422 - ETA: 2:50 - loss: 2.3349 - acc: 0.422 - ETA: 2:49 - loss: 2.3376 - acc: 0.421 - ETA: 2:48 - loss: 2.3375 - acc: 0.421 - ETA: 2:46 - loss: 2.3382 - acc: 0.422 - ETA: 2:45 - loss: 2.3402 - acc: 0.421 - ETA: 2:44 - loss: 2.3415 - acc: 0.420 - ETA: 2:43 - loss: 2.3383 - acc: 0.420 - ETA: 2:42 - loss: 2.3376 - acc: 0.420 - ETA: 2:41 - loss: 2.3368 - acc: 0.420 - ETA: 2:40 - loss: 2.3365 - acc: 0.420 - ETA: 2:39 - loss: 2.3401 - acc: 0.419 - ETA: 2:38 - loss: 2.3366 - acc: 0.420 - ETA: 2:36 - loss: 2.3362 - acc: 0.420 - ETA: 2:35 - loss: 2.3365 - acc: 0.421 - ETA: 2:34 - loss: 2.3324 - acc: 0.421 - ETA: 2:33 - loss: 2.3307 - acc: 0.421 - ETA: 2:32 - loss: 2.3308 - acc: 0.422 - ETA: 2:31 - loss: 2.3300 - acc: 0.422 - ETA: 2:30 - loss: 2.3391 - acc: 0.421 - ETA: 2:29 - loss: 2.3382 - acc: 0.421 - ETA: 2:28 - loss: 2.3373 - acc: 0.421 - ETA: 2:27 - loss: 2.3369 - acc: 0.421 - ETA: 2:25 - loss: 2.3345 - acc: 0.421 - ETA: 2:24 - loss: 2.3373 - acc: 0.421 - ETA: 2:23 - loss: 2.3367 - acc: 0.421 - ETA: 2:22 - loss: 2.3370 - acc: 0.42186680/6680 [==============================] - ETA: 2:21 - loss: 2.3356 - acc: 0.422 - ETA: 2:20 - loss: 2.3349 - acc: 0.422 - ETA: 2:19 - loss: 2.3351 - acc: 0.422 - ETA: 2:18 - loss: 2.3339 - acc: 0.422 - ETA: 2:17 - loss: 2.3337 - acc: 0.423 - ETA: 2:16 - loss: 2.3322 - acc: 0.422 - ETA: 2:15 - loss: 2.3330 - acc: 0.422 - ETA: 2:13 - loss: 2.3360 - acc: 0.422 - ETA: 2:12 - loss: 2.3403 - acc: 0.422 - ETA: 2:11 - loss: 2.3421 - acc: 0.421 - ETA: 2:10 - loss: 2.3415 - acc: 0.421 - ETA: 2:09 - loss: 2.3416 - acc: 0.421 - ETA: 2:08 - loss: 2.3431 - acc: 0.421 - ETA: 2:07 - loss: 2.3427 - acc: 0.421 - ETA: 2:06 - loss: 2.3418 - acc: 0.421 - ETA: 2:05 - loss: 2.3431 - acc: 0.421 - ETA: 2:04 - loss: 2.3409 - acc: 0.421 - ETA: 2:03 - loss: 2.3406 - acc: 0.421 - ETA: 2:02 - loss: 2.3410 - acc: 0.421 - ETA: 2:00 - loss: 2.3384 - acc: 0.421 - ETA: 1:59 - loss: 2.3371 - acc: 0.422 - ETA: 1:58 - loss: 2.3350 - acc: 0.422 - ETA: 1:57 - loss: 2.3359 - acc: 0.421 - ETA: 1:56 - loss: 2.3345 - acc: 0.421 - ETA: 1:55 - loss: 2.3342 - acc: 0.421 - ETA: 1:54 - loss: 2.3351 - acc: 0.421 - ETA: 1:53 - loss: 2.3362 - acc: 0.422 - ETA: 1:52 - loss: 2.3406 - acc: 0.421 - ETA: 1:51 - loss: 2.3385 - acc: 0.421 - ETA: 1:49 - loss: 2.3419 - acc: 0.421 - ETA: 1:48 - loss: 2.3424 - acc: 0.421 - ETA: 1:47 - loss: 2.3411 - acc: 0.421 - ETA: 1:46 - loss: 2.3422 - acc: 0.421 - ETA: 1:45 - loss: 2.3408 - acc: 0.421 - ETA: 1:44 - loss: 2.3412 - acc: 0.421 - ETA: 1:43 - loss: 2.3403 - acc: 0.421 - ETA: 1:42 - loss: 2.3394 - acc: 0.422 - ETA: 1:41 - loss: 2.3382 - acc: 0.422 - ETA: 1:40 - loss: 2.3403 - acc: 0.421 - ETA: 1:39 - loss: 2.3398 - acc: 0.421 - ETA: 1:37 - loss: 2.3401 - acc: 0.421 - ETA: 1:36 - loss: 2.3397 - acc: 0.422 - ETA: 1:35 - loss: 2.3402 - acc: 0.421 - ETA: 1:34 - loss: 2.3412 - acc: 0.421 - ETA: 1:33 - loss: 2.3398 - acc: 0.421 - ETA: 1:32 - loss: 2.3432 - acc: 0.421 - ETA: 1:31 - loss: 2.3427 - acc: 0.421 - ETA: 1:30 - loss: 2.3469 - acc: 0.421 - ETA: 1:29 - loss: 2.3486 - acc: 0.421 - ETA: 1:28 - loss: 2.3481 - acc: 0.421 - ETA: 1:26 - loss: 2.3479 - acc: 0.422 - ETA: 1:25 - loss: 2.3476 - acc: 0.421 - ETA: 1:24 - loss: 2.3499 - acc: 0.421 - ETA: 1:23 - loss: 2.3512 - acc: 0.421 - ETA: 1:22 - loss: 2.3524 - acc: 0.420 - ETA: 1:21 - loss: 2.3524 - acc: 0.420 - ETA: 1:20 - loss: 2.3514 - acc: 0.420 - ETA: 1:19 - loss: 2.3514 - acc: 0.420 - ETA: 1:18 - loss: 2.3530 - acc: 0.420 - ETA: 1:17 - loss: 2.3545 - acc: 0.419 - ETA: 1:16 - loss: 2.3557 - acc: 0.419 - ETA: 1:14 - loss: 2.3591 - acc: 0.418 - ETA: 1:13 - loss: 2.3588 - acc: 0.418 - ETA: 1:12 - loss: 2.3582 - acc: 0.419 - ETA: 1:11 - loss: 2.3581 - acc: 0.419 - ETA: 1:10 - loss: 2.3602 - acc: 0.418 - ETA: 1:09 - loss: 2.3596 - acc: 0.418 - ETA: 1:08 - loss: 2.3605 - acc: 0.418 - ETA: 1:07 - loss: 2.3617 - acc: 0.418 - ETA: 1:06 - loss: 2.3617 - acc: 0.418 - ETA: 1:05 - loss: 2.3613 - acc: 0.418 - ETA: 1:03 - loss: 2.3597 - acc: 0.418 - ETA: 1:02 - loss: 2.3596 - acc: 0.419 - ETA: 1:01 - loss: 2.3588 - acc: 0.419 - ETA: 1:00 - loss: 2.3578 - acc: 0.419 - ETA: 59s - loss: 2.3583 - acc: 0.419 - ETA: 58s - loss: 2.3601 - acc: 0.41 - ETA: 57s - loss: 2.3622 - acc: 0.41 - ETA: 56s - loss: 2.3619 - acc: 0.41 - ETA: 55s - loss: 2.3630 - acc: 0.41 - ETA: 54s - loss: 2.3625 - acc: 0.41 - ETA: 53s - loss: 2.3655 - acc: 0.41 - ETA: 51s - loss: 2.3647 - acc: 0.41 - ETA: 50s - loss: 2.3633 - acc: 0.41 - ETA: 49s - loss: 2.3611 - acc: 0.41 - ETA: 48s - loss: 2.3618 - acc: 0.41 - ETA: 47s - loss: 2.3610 - acc: 0.41 - ETA: 46s - loss: 2.3596 - acc: 0.41 - ETA: 45s - loss: 2.3619 - acc: 0.41 - ETA: 44s - loss: 2.3607 - acc: 0.41 - ETA: 43s - loss: 2.3581 - acc: 0.41 - ETA: 42s - loss: 2.3586 - acc: 0.41 - ETA: 40s - loss: 2.3587 - acc: 0.41 - ETA: 39s - loss: 2.3596 - acc: 0.41 - ETA: 38s - loss: 2.3616 - acc: 0.41 - ETA: 37s - loss: 2.3655 - acc: 0.41 - ETA: 36s - loss: 2.3658 - acc: 0.41 - ETA: 35s - loss: 2.3647 - acc: 0.41 - ETA: 34s - loss: 2.3638 - acc: 0.41 - ETA: 33s - loss: 2.3636 - acc: 0.41 - ETA: 32s - loss: 2.3631 - acc: 0.41 - ETA: 31s - loss: 2.3619 - acc: 0.41 - ETA: 29s - loss: 2.3606 - acc: 0.41 - ETA: 28s - loss: 2.3596 - acc: 0.41 - ETA: 27s - loss: 2.3607 - acc: 0.41 - ETA: 26s - loss: 2.3630 - acc: 0.41 - ETA: 25s - loss: 2.3629 - acc: 0.41 - ETA: 24s - loss: 2.3632 - acc: 0.41 - ETA: 23s - loss: 2.3641 - acc: 0.41 - ETA: 22s - loss: 2.3647 - acc: 0.41 - ETA: 21s - loss: 2.3638 - acc: 0.41 - ETA: 19s - loss: 2.3659 - acc: 0.41 - ETA: 18s - loss: 2.3639 - acc: 0.41 - ETA: 17s - loss: 2.3643 - acc: 0.41 - ETA: 16s - loss: 2.3631 - acc: 0.41 - ETA: 15s - loss: 2.3657 - acc: 0.41 - ETA: 14s - loss: 2.3644 - acc: 0.41 - ETA: 13s - loss: 2.3646 - acc: 0.41 - ETA: 12s - loss: 2.3661 - acc: 0.41 - ETA: 11s - loss: 2.3685 - acc: 0.41 - ETA: 10s - loss: 2.3693 - acc: 0.41 - ETA: 8s - loss: 2.3688 - acc: 0.4170 - ETA: 7s - loss: 2.3687 - acc: 0.417 - ETA: 6s - loss: 2.3681 - acc: 0.417 - ETA: 5s - loss: 2.3694 - acc: 0.416 - ETA: 4s - loss: 2.3696 - acc: 0.416 - ETA: 3s - loss: 2.3690 - acc: 0.416 - ETA: 2s - loss: 2.3688 - acc: 0.416 - ETA: 1s - loss: 2.3685 - acc: 0.416 - 384s 57ms/step - loss: 2.3675 - acc: 0.4166 - val\_loss: 4.5403 - val\_acc: 0.1042

Epoch 00005: val\_loss did not improve from 4.15157
Epoch 6/10
4080/6680 [=================>{\ldots}] - ETA: 6:02 - loss: 1.6355 - acc: 0.500 - ETA: 6:05 - loss: 1.5501 - acc: 0.600 - ETA: 6:08 - loss: 1.3647 - acc: 0.650 - ETA: 6:07 - loss: 1.4245 - acc: 0.637 - ETA: 6:06 - loss: 1.4170 - acc: 0.640 - ETA: 6:05 - loss: 1.4720 - acc: 0.625 - ETA: 6:05 - loss: 1.4322 - acc: 0.628 - ETA: 6:04 - loss: 1.4203 - acc: 0.637 - ETA: 6:04 - loss: 1.3953 - acc: 0.638 - ETA: 6:03 - loss: 1.4134 - acc: 0.640 - ETA: 6:01 - loss: 1.3852 - acc: 0.640 - ETA: 6:00 - loss: 1.4222 - acc: 0.633 - ETA: 5:58 - loss: 1.4252 - acc: 0.630 - ETA: 5:56 - loss: 1.4411 - acc: 0.628 - ETA: 5:55 - loss: 1.4193 - acc: 0.640 - ETA: 5:54 - loss: 1.4380 - acc: 0.637 - ETA: 5:52 - loss: 1.4206 - acc: 0.641 - ETA: 5:51 - loss: 1.3871 - acc: 0.652 - ETA: 5:50 - loss: 1.4111 - acc: 0.644 - ETA: 5:48 - loss: 1.4004 - acc: 0.652 - ETA: 5:47 - loss: 1.3694 - acc: 0.664 - ETA: 5:46 - loss: 1.3444 - acc: 0.672 - ETA: 5:45 - loss: 1.3515 - acc: 0.673 - ETA: 5:44 - loss: 1.3539 - acc: 0.668 - ETA: 5:43 - loss: 1.3556 - acc: 0.670 - ETA: 5:41 - loss: 1.3479 - acc: 0.669 - ETA: 5:40 - loss: 1.3511 - acc: 0.668 - ETA: 5:39 - loss: 1.3777 - acc: 0.660 - ETA: 5:38 - loss: 1.3976 - acc: 0.655 - ETA: 5:37 - loss: 1.4026 - acc: 0.655 - ETA: 5:36 - loss: 1.4101 - acc: 0.651 - ETA: 5:35 - loss: 1.4086 - acc: 0.650 - ETA: 5:33 - loss: 1.4030 - acc: 0.651 - ETA: 5:32 - loss: 1.4349 - acc: 0.642 - ETA: 5:31 - loss: 1.4486 - acc: 0.641 - ETA: 5:30 - loss: 1.4401 - acc: 0.643 - ETA: 5:29 - loss: 1.4259 - acc: 0.643 - ETA: 5:28 - loss: 1.4266 - acc: 0.639 - ETA: 5:26 - loss: 1.4337 - acc: 0.641 - ETA: 5:25 - loss: 1.4384 - acc: 0.638 - ETA: 5:24 - loss: 1.4329 - acc: 0.641 - ETA: 5:23 - loss: 1.4481 - acc: 0.638 - ETA: 5:22 - loss: 1.4570 - acc: 0.634 - ETA: 5:21 - loss: 1.4505 - acc: 0.638 - ETA: 5:19 - loss: 1.4453 - acc: 0.640 - ETA: 5:18 - loss: 1.4493 - acc: 0.639 - ETA: 5:17 - loss: 1.4442 - acc: 0.641 - ETA: 5:16 - loss: 1.4390 - acc: 0.642 - ETA: 5:15 - loss: 1.4443 - acc: 0.640 - ETA: 5:14 - loss: 1.4434 - acc: 0.639 - ETA: 5:13 - loss: 1.4388 - acc: 0.642 - ETA: 5:12 - loss: 1.4320 - acc: 0.643 - ETA: 5:10 - loss: 1.4248 - acc: 0.645 - ETA: 5:09 - loss: 1.4325 - acc: 0.643 - ETA: 5:08 - loss: 1.4357 - acc: 0.642 - ETA: 5:07 - loss: 1.4277 - acc: 0.644 - ETA: 5:06 - loss: 1.4305 - acc: 0.644 - ETA: 5:05 - loss: 1.4301 - acc: 0.643 - ETA: 5:04 - loss: 1.4346 - acc: 0.643 - ETA: 5:03 - loss: 1.4359 - acc: 0.642 - ETA: 5:01 - loss: 1.4248 - acc: 0.645 - ETA: 5:00 - loss: 1.4254 - acc: 0.645 - ETA: 4:59 - loss: 1.4262 - acc: 0.644 - ETA: 4:58 - loss: 1.4333 - acc: 0.640 - ETA: 4:57 - loss: 1.4209 - acc: 0.645 - ETA: 4:56 - loss: 1.4314 - acc: 0.640 - ETA: 4:55 - loss: 1.4418 - acc: 0.637 - ETA: 4:54 - loss: 1.4435 - acc: 0.637 - ETA: 4:53 - loss: 1.4487 - acc: 0.636 - ETA: 4:52 - loss: 1.4530 - acc: 0.635 - ETA: 4:51 - loss: 1.4573 - acc: 0.634 - ETA: 4:49 - loss: 1.4535 - acc: 0.635 - ETA: 4:48 - loss: 1.4478 - acc: 0.637 - ETA: 4:47 - loss: 1.4462 - acc: 0.637 - ETA: 4:46 - loss: 1.4466 - acc: 0.636 - ETA: 4:45 - loss: 1.4464 - acc: 0.635 - ETA: 4:44 - loss: 1.4405 - acc: 0.637 - ETA: 4:43 - loss: 1.4391 - acc: 0.637 - ETA: 4:42 - loss: 1.4412 - acc: 0.637 - ETA: 4:41 - loss: 1.4476 - acc: 0.636 - ETA: 4:40 - loss: 1.4448 - acc: 0.637 - ETA: 4:39 - loss: 1.4426 - acc: 0.638 - ETA: 4:38 - loss: 1.4397 - acc: 0.637 - ETA: 4:37 - loss: 1.4369 - acc: 0.638 - ETA: 4:36 - loss: 1.4370 - acc: 0.638 - ETA: 4:35 - loss: 1.4407 - acc: 0.637 - ETA: 4:34 - loss: 1.4413 - acc: 0.636 - ETA: 4:33 - loss: 1.4384 - acc: 0.637 - ETA: 4:32 - loss: 1.4378 - acc: 0.637 - ETA: 4:30 - loss: 1.4309 - acc: 0.640 - ETA: 4:29 - loss: 1.4280 - acc: 0.641 - ETA: 4:28 - loss: 1.4280 - acc: 0.642 - ETA: 4:27 - loss: 1.4230 - acc: 0.642 - ETA: 4:26 - loss: 1.4258 - acc: 0.642 - ETA: 4:25 - loss: 1.4384 - acc: 0.637 - ETA: 4:24 - loss: 1.4437 - acc: 0.634 - ETA: 4:23 - loss: 1.4392 - acc: 0.636 - ETA: 4:22 - loss: 1.4371 - acc: 0.636 - ETA: 4:21 - loss: 1.4373 - acc: 0.636 - ETA: 4:19 - loss: 1.4390 - acc: 0.636 - ETA: 4:18 - loss: 1.4444 - acc: 0.634 - ETA: 4:17 - loss: 1.4480 - acc: 0.633 - ETA: 4:16 - loss: 1.4568 - acc: 0.631 - ETA: 4:15 - loss: 1.4586 - acc: 0.630 - ETA: 4:14 - loss: 1.4581 - acc: 0.631 - ETA: 4:13 - loss: 1.4555 - acc: 0.632 - ETA: 4:12 - loss: 1.4565 - acc: 0.632 - ETA: 4:10 - loss: 1.4566 - acc: 0.631 - ETA: 4:09 - loss: 1.4503 - acc: 0.633 - ETA: 4:08 - loss: 1.4536 - acc: 0.632 - ETA: 4:07 - loss: 1.4522 - acc: 0.632 - ETA: 4:06 - loss: 1.4524 - acc: 0.633 - ETA: 4:05 - loss: 1.4495 - acc: 0.635 - ETA: 4:04 - loss: 1.4486 - acc: 0.634 - ETA: 4:02 - loss: 1.4482 - acc: 0.634 - ETA: 4:01 - loss: 1.4432 - acc: 0.636 - ETA: 4:00 - loss: 1.4398 - acc: 0.637 - ETA: 3:59 - loss: 1.4339 - acc: 0.638 - ETA: 3:58 - loss: 1.4303 - acc: 0.639 - ETA: 3:57 - loss: 1.4291 - acc: 0.640 - ETA: 3:56 - loss: 1.4314 - acc: 0.638 - ETA: 3:55 - loss: 1.4322 - acc: 0.637 - ETA: 3:53 - loss: 1.4319 - acc: 0.637 - ETA: 3:52 - loss: 1.4315 - acc: 0.638 - ETA: 3:51 - loss: 1.4282 - acc: 0.638 - ETA: 3:50 - loss: 1.4326 - acc: 0.636 - ETA: 3:49 - loss: 1.4311 - acc: 0.636 - ETA: 3:48 - loss: 1.4303 - acc: 0.636 - ETA: 3:47 - loss: 1.4361 - acc: 0.633 - ETA: 3:46 - loss: 1.4397 - acc: 0.632 - ETA: 3:44 - loss: 1.4379 - acc: 0.633 - ETA: 3:43 - loss: 1.4378 - acc: 0.634 - ETA: 3:42 - loss: 1.4355 - acc: 0.634 - ETA: 3:41 - loss: 1.4327 - acc: 0.635 - ETA: 3:40 - loss: 1.4325 - acc: 0.635 - ETA: 3:39 - loss: 1.4302 - acc: 0.636 - ETA: 3:38 - loss: 1.4280 - acc: 0.636 - ETA: 3:37 - loss: 1.4306 - acc: 0.635 - ETA: 3:36 - loss: 1.4337 - acc: 0.634 - ETA: 3:34 - loss: 1.4303 - acc: 0.635 - ETA: 3:33 - loss: 1.4315 - acc: 0.634 - ETA: 3:32 - loss: 1.4298 - acc: 0.634 - ETA: 3:31 - loss: 1.4307 - acc: 0.635 - ETA: 3:30 - loss: 1.4322 - acc: 0.634 - ETA: 3:29 - loss: 1.4300 - acc: 0.634 - ETA: 3:28 - loss: 1.4294 - acc: 0.634 - ETA: 3:27 - loss: 1.4283 - acc: 0.635 - ETA: 3:25 - loss: 1.4301 - acc: 0.634 - ETA: 3:24 - loss: 1.4273 - acc: 0.634 - ETA: 3:23 - loss: 1.4290 - acc: 0.635 - ETA: 3:22 - loss: 1.4310 - acc: 0.634 - ETA: 3:21 - loss: 1.4294 - acc: 0.634 - ETA: 3:20 - loss: 1.4325 - acc: 0.634 - ETA: 3:19 - loss: 1.4349 - acc: 0.632 - ETA: 3:18 - loss: 1.4359 - acc: 0.632 - ETA: 3:16 - loss: 1.4345 - acc: 0.632 - ETA: 3:15 - loss: 1.4321 - acc: 0.632 - ETA: 3:14 - loss: 1.4298 - acc: 0.632 - ETA: 3:13 - loss: 1.4301 - acc: 0.632 - ETA: 3:12 - loss: 1.4308 - acc: 0.632 - ETA: 3:11 - loss: 1.4287 - acc: 0.632 - ETA: 3:10 - loss: 1.4286 - acc: 0.632 - ETA: 3:09 - loss: 1.4305 - acc: 0.631 - ETA: 3:08 - loss: 1.4351 - acc: 0.631 - ETA: 3:06 - loss: 1.4343 - acc: 0.632 - ETA: 3:05 - loss: 1.4359 - acc: 0.631 - ETA: 3:04 - loss: 1.4390 - acc: 0.631 - ETA: 3:03 - loss: 1.4373 - acc: 0.631 - ETA: 3:02 - loss: 1.4357 - acc: 0.631 - ETA: 3:01 - loss: 1.4391 - acc: 0.630 - ETA: 3:00 - loss: 1.4373 - acc: 0.631 - ETA: 2:59 - loss: 1.4379 - acc: 0.630 - ETA: 2:57 - loss: 1.4375 - acc: 0.629 - ETA: 2:56 - loss: 1.4390 - acc: 0.629 - ETA: 2:55 - loss: 1.4403 - acc: 0.628 - ETA: 2:54 - loss: 1.4390 - acc: 0.628 - ETA: 2:53 - loss: 1.4370 - acc: 0.628 - ETA: 2:52 - loss: 1.4353 - acc: 0.629 - ETA: 2:51 - loss: 1.4378 - acc: 0.627 - ETA: 2:50 - loss: 1.4406 - acc: 0.627 - ETA: 2:49 - loss: 1.4418 - acc: 0.626 - ETA: 2:47 - loss: 1.4451 - acc: 0.625 - ETA: 2:46 - loss: 1.4462 - acc: 0.625 - ETA: 2:45 - loss: 1.4470 - acc: 0.625 - ETA: 2:44 - loss: 1.4453 - acc: 0.625 - ETA: 2:43 - loss: 1.4447 - acc: 0.626 - ETA: 2:42 - loss: 1.4439 - acc: 0.626 - ETA: 2:41 - loss: 1.4422 - acc: 0.626 - ETA: 2:40 - loss: 1.4402 - acc: 0.626 - ETA: 2:39 - loss: 1.4416 - acc: 0.626 - ETA: 2:37 - loss: 1.4401 - acc: 0.627 - ETA: 2:36 - loss: 1.4388 - acc: 0.627 - ETA: 2:35 - loss: 1.4423 - acc: 0.628 - ETA: 2:34 - loss: 1.4420 - acc: 0.628 - ETA: 2:33 - loss: 1.4442 - acc: 0.627 - ETA: 2:32 - loss: 1.4484 - acc: 0.626 - ETA: 2:31 - loss: 1.4474 - acc: 0.626 - ETA: 2:30 - loss: 1.4456 - acc: 0.627 - ETA: 2:29 - loss: 1.4488 - acc: 0.627 - ETA: 2:28 - loss: 1.4506 - acc: 0.626 - ETA: 2:26 - loss: 1.4537 - acc: 0.625 - ETA: 2:25 - loss: 1.4528 - acc: 0.626 - ETA: 2:24 - loss: 1.4551 - acc: 0.625 - ETA: 2:23 - loss: 1.4550 - acc: 0.62556680/6680 [==============================] - ETA: 2:22 - loss: 1.4568 - acc: 0.625 - ETA: 2:21 - loss: 1.4545 - acc: 0.626 - ETA: 2:20 - loss: 1.4540 - acc: 0.626 - ETA: 2:19 - loss: 1.4554 - acc: 0.626 - ETA: 2:18 - loss: 1.4544 - acc: 0.626 - ETA: 2:16 - loss: 1.4536 - acc: 0.627 - ETA: 2:15 - loss: 1.4537 - acc: 0.627 - ETA: 2:14 - loss: 1.4546 - acc: 0.626 - ETA: 2:13 - loss: 1.4571 - acc: 0.626 - ETA: 2:12 - loss: 1.4596 - acc: 0.625 - ETA: 2:11 - loss: 1.4619 - acc: 0.625 - ETA: 2:10 - loss: 1.4642 - acc: 0.624 - ETA: 2:09 - loss: 1.4631 - acc: 0.624 - ETA: 2:07 - loss: 1.4612 - acc: 0.625 - ETA: 2:06 - loss: 1.4607 - acc: 0.625 - ETA: 2:05 - loss: 1.4592 - acc: 0.626 - ETA: 2:04 - loss: 1.4627 - acc: 0.624 - ETA: 2:03 - loss: 1.4613 - acc: 0.625 - ETA: 2:02 - loss: 1.4635 - acc: 0.624 - ETA: 2:01 - loss: 1.4625 - acc: 0.625 - ETA: 2:00 - loss: 1.4657 - acc: 0.625 - ETA: 1:59 - loss: 1.4680 - acc: 0.625 - ETA: 1:57 - loss: 1.4696 - acc: 0.625 - ETA: 1:56 - loss: 1.4694 - acc: 0.624 - ETA: 1:55 - loss: 1.4698 - acc: 0.624 - ETA: 1:54 - loss: 1.4704 - acc: 0.624 - ETA: 1:53 - loss: 1.4734 - acc: 0.623 - ETA: 1:52 - loss: 1.4777 - acc: 0.622 - ETA: 1:51 - loss: 1.4762 - acc: 0.623 - ETA: 1:50 - loss: 1.4779 - acc: 0.622 - ETA: 1:49 - loss: 1.4807 - acc: 0.622 - ETA: 1:48 - loss: 1.4809 - acc: 0.622 - ETA: 1:46 - loss: 1.4804 - acc: 0.621 - ETA: 1:45 - loss: 1.4802 - acc: 0.621 - ETA: 1:44 - loss: 1.4795 - acc: 0.622 - ETA: 1:43 - loss: 1.4814 - acc: 0.621 - ETA: 1:42 - loss: 1.4816 - acc: 0.621 - ETA: 1:41 - loss: 1.4840 - acc: 0.620 - ETA: 1:40 - loss: 1.4829 - acc: 0.620 - ETA: 1:39 - loss: 1.4842 - acc: 0.620 - ETA: 1:38 - loss: 1.4842 - acc: 0.620 - ETA: 1:37 - loss: 1.4831 - acc: 0.621 - ETA: 1:35 - loss: 1.4833 - acc: 0.621 - ETA: 1:34 - loss: 1.4829 - acc: 0.620 - ETA: 1:33 - loss: 1.4856 - acc: 0.619 - ETA: 1:32 - loss: 1.4855 - acc: 0.619 - ETA: 1:31 - loss: 1.4857 - acc: 0.618 - ETA: 1:30 - loss: 1.4870 - acc: 0.619 - ETA: 1:29 - loss: 1.4855 - acc: 0.619 - ETA: 1:28 - loss: 1.4865 - acc: 0.619 - ETA: 1:27 - loss: 1.4840 - acc: 0.620 - ETA: 1:26 - loss: 1.4830 - acc: 0.620 - ETA: 1:24 - loss: 1.4821 - acc: 0.620 - ETA: 1:23 - loss: 1.4822 - acc: 0.620 - ETA: 1:22 - loss: 1.4834 - acc: 0.620 - ETA: 1:21 - loss: 1.4836 - acc: 0.620 - ETA: 1:20 - loss: 1.4843 - acc: 0.619 - ETA: 1:19 - loss: 1.4859 - acc: 0.619 - ETA: 1:18 - loss: 1.4853 - acc: 0.619 - ETA: 1:17 - loss: 1.4860 - acc: 0.618 - ETA: 1:16 - loss: 1.4854 - acc: 0.618 - ETA: 1:15 - loss: 1.4846 - acc: 0.619 - ETA: 1:13 - loss: 1.4865 - acc: 0.618 - ETA: 1:12 - loss: 1.4887 - acc: 0.617 - ETA: 1:11 - loss: 1.4894 - acc: 0.617 - ETA: 1:10 - loss: 1.4899 - acc: 0.617 - ETA: 1:09 - loss: 1.4923 - acc: 0.616 - ETA: 1:08 - loss: 1.4922 - acc: 0.616 - ETA: 1:07 - loss: 1.4923 - acc: 0.616 - ETA: 1:06 - loss: 1.4933 - acc: 0.616 - ETA: 1:05 - loss: 1.4945 - acc: 0.615 - ETA: 1:04 - loss: 1.4973 - acc: 0.614 - ETA: 1:03 - loss: 1.4970 - acc: 0.614 - ETA: 1:02 - loss: 1.4970 - acc: 0.614 - ETA: 1:00 - loss: 1.4967 - acc: 0.614 - ETA: 59s - loss: 1.4993 - acc: 0.613 - ETA: 58s - loss: 1.4967 - acc: 0.61 - ETA: 57s - loss: 1.4977 - acc: 0.61 - ETA: 56s - loss: 1.4984 - acc: 0.61 - ETA: 55s - loss: 1.4997 - acc: 0.61 - ETA: 54s - loss: 1.5002 - acc: 0.61 - ETA: 53s - loss: 1.5000 - acc: 0.61 - ETA: 52s - loss: 1.5002 - acc: 0.61 - ETA: 51s - loss: 1.4988 - acc: 0.61 - ETA: 49s - loss: 1.4978 - acc: 0.61 - ETA: 48s - loss: 1.4995 - acc: 0.61 - ETA: 47s - loss: 1.4994 - acc: 0.61 - ETA: 46s - loss: 1.5010 - acc: 0.61 - ETA: 45s - loss: 1.5016 - acc: 0.61 - ETA: 44s - loss: 1.5003 - acc: 0.61 - ETA: 43s - loss: 1.5014 - acc: 0.61 - ETA: 42s - loss: 1.5031 - acc: 0.61 - ETA: 41s - loss: 1.5014 - acc: 0.61 - ETA: 39s - loss: 1.5029 - acc: 0.61 - ETA: 38s - loss: 1.5027 - acc: 0.61 - ETA: 37s - loss: 1.5029 - acc: 0.61 - ETA: 36s - loss: 1.5026 - acc: 0.61 - ETA: 35s - loss: 1.5032 - acc: 0.61 - ETA: 34s - loss: 1.5041 - acc: 0.61 - ETA: 33s - loss: 1.5053 - acc: 0.61 - ETA: 32s - loss: 1.5060 - acc: 0.61 - ETA: 31s - loss: 1.5064 - acc: 0.61 - ETA: 29s - loss: 1.5059 - acc: 0.61 - ETA: 28s - loss: 1.5064 - acc: 0.61 - ETA: 27s - loss: 1.5057 - acc: 0.61 - ETA: 26s - loss: 1.5073 - acc: 0.61 - ETA: 25s - loss: 1.5055 - acc: 0.61 - ETA: 24s - loss: 1.5066 - acc: 0.61 - ETA: 23s - loss: 1.5089 - acc: 0.60 - ETA: 22s - loss: 1.5096 - acc: 0.61 - ETA: 21s - loss: 1.5113 - acc: 0.60 - ETA: 20s - loss: 1.5096 - acc: 0.60 - ETA: 18s - loss: 1.5107 - acc: 0.60 - ETA: 17s - loss: 1.5104 - acc: 0.60 - ETA: 16s - loss: 1.5093 - acc: 0.61 - ETA: 15s - loss: 1.5097 - acc: 0.60 - ETA: 14s - loss: 1.5110 - acc: 0.60 - ETA: 13s - loss: 1.5111 - acc: 0.60 - ETA: 12s - loss: 1.5142 - acc: 0.60 - ETA: 11s - loss: 1.5160 - acc: 0.60 - ETA: 10s - loss: 1.5163 - acc: 0.60 - ETA: 8s - loss: 1.5177 - acc: 0.6074 - ETA: 7s - loss: 1.5195 - acc: 0.606 - ETA: 6s - loss: 1.5198 - acc: 0.606 - ETA: 5s - loss: 1.5192 - acc: 0.607 - ETA: 4s - loss: 1.5208 - acc: 0.606 - ETA: 3s - loss: 1.5215 - acc: 0.606 - ETA: 2s - loss: 1.5213 - acc: 0.606 - ETA: 1s - loss: 1.5208 - acc: 0.607 - 389s 58ms/step - loss: 1.5194 - acc: 0.6073 - val\_loss: 5.1595 - val\_acc: 0.0982

Epoch 00006: val\_loss did not improve from 4.15157
Epoch 7/10
4080/6680 [=================>{\ldots}] - ETA: 6:30 - loss: 0.8863 - acc: 0.750 - ETA: 6:49 - loss: 0.9258 - acc: 0.775 - ETA: 6:50 - loss: 0.9384 - acc: 0.783 - ETA: 6:43 - loss: 0.9197 - acc: 0.787 - ETA: 6:47 - loss: 0.8916 - acc: 0.770 - ETA: 6:45 - loss: 0.8769 - acc: 0.791 - ETA: 7:05 - loss: 0.8798 - acc: 0.792 - ETA: 7:18 - loss: 0.8639 - acc: 0.787 - ETA: 7:22 - loss: 0.8169 - acc: 0.783 - ETA: 7:34 - loss: 0.7749 - acc: 0.795 - ETA: 7:27 - loss: 0.7564 - acc: 0.800 - ETA: 7:19 - loss: 0.7763 - acc: 0.804 - ETA: 7:13 - loss: 0.8120 - acc: 0.796 - ETA: 7:08 - loss: 0.8176 - acc: 0.789 - ETA: 7:03 - loss: 0.8339 - acc: 0.786 - ETA: 7:00 - loss: 0.8334 - acc: 0.790 - ETA: 7:07 - loss: 0.8147 - acc: 0.797 - ETA: 7:09 - loss: 0.7840 - acc: 0.808 - ETA: 7:05 - loss: 0.7755 - acc: 0.813 - ETA: 7:01 - loss: 0.8087 - acc: 0.810 - ETA: 6:59 - loss: 0.8071 - acc: 0.809 - ETA: 6:55 - loss: 0.8200 - acc: 0.804 - ETA: 6:58 - loss: 0.8346 - acc: 0.804 - ETA: 7:01 - loss: 0.8345 - acc: 0.806 - ETA: 7:00 - loss: 0.8215 - acc: 0.808 - ETA: 6:59 - loss: 0.8209 - acc: 0.807 - ETA: 6:58 - loss: 0.8142 - acc: 0.809 - ETA: 6:54 - loss: 0.8108 - acc: 0.808 - ETA: 6:52 - loss: 0.8050 - acc: 0.806 - ETA: 6:48 - loss: 0.8096 - acc: 0.805 - ETA: 6:45 - loss: 0.8144 - acc: 0.803 - ETA: 6:42 - loss: 0.8020 - acc: 0.807 - ETA: 6:39 - loss: 0.8149 - acc: 0.804 - ETA: 6:36 - loss: 0.8139 - acc: 0.804 - ETA: 6:33 - loss: 0.8299 - acc: 0.798 - ETA: 6:30 - loss: 0.8215 - acc: 0.798 - ETA: 6:27 - loss: 0.8366 - acc: 0.795 - ETA: 6:25 - loss: 0.8359 - acc: 0.794 - ETA: 6:23 - loss: 0.8344 - acc: 0.793 - ETA: 6:23 - loss: 0.8315 - acc: 0.793 - ETA: 6:20 - loss: 0.8228 - acc: 0.795 - ETA: 6:18 - loss: 0.8323 - acc: 0.792 - ETA: 6:16 - loss: 0.8300 - acc: 0.793 - ETA: 6:13 - loss: 0.8299 - acc: 0.792 - ETA: 6:11 - loss: 0.8390 - acc: 0.788 - ETA: 6:08 - loss: 0.8494 - acc: 0.788 - ETA: 6:07 - loss: 0.8404 - acc: 0.790 - ETA: 6:05 - loss: 0.8318 - acc: 0.792 - ETA: 6:03 - loss: 0.8272 - acc: 0.794 - ETA: 6:00 - loss: 0.8231 - acc: 0.794 - ETA: 5:59 - loss: 0.8221 - acc: 0.793 - ETA: 5:57 - loss: 0.8300 - acc: 0.791 - ETA: 5:56 - loss: 0.8222 - acc: 0.792 - ETA: 5:54 - loss: 0.8273 - acc: 0.791 - ETA: 5:52 - loss: 0.8317 - acc: 0.790 - ETA: 5:50 - loss: 0.8318 - acc: 0.792 - ETA: 5:48 - loss: 0.8276 - acc: 0.793 - ETA: 5:46 - loss: 0.8240 - acc: 0.794 - ETA: 5:45 - loss: 0.8198 - acc: 0.794 - ETA: 5:45 - loss: 0.8135 - acc: 0.795 - ETA: 5:46 - loss: 0.8095 - acc: 0.795 - ETA: 5:47 - loss: 0.8122 - acc: 0.793 - ETA: 5:45 - loss: 0.8125 - acc: 0.791 - ETA: 5:43 - loss: 0.8129 - acc: 0.792 - ETA: 5:42 - loss: 0.8198 - acc: 0.790 - ETA: 5:43 - loss: 0.8249 - acc: 0.789 - ETA: 5:44 - loss: 0.8261 - acc: 0.789 - ETA: 5:43 - loss: 0.8271 - acc: 0.789 - ETA: 5:42 - loss: 0.8282 - acc: 0.787 - ETA: 5:41 - loss: 0.8251 - acc: 0.787 - ETA: 5:39 - loss: 0.8313 - acc: 0.783 - ETA: 5:38 - loss: 0.8264 - acc: 0.786 - ETA: 5:36 - loss: 0.8226 - acc: 0.787 - ETA: 5:34 - loss: 0.8224 - acc: 0.787 - ETA: 5:33 - loss: 0.8230 - acc: 0.788 - ETA: 5:31 - loss: 0.8219 - acc: 0.788 - ETA: 5:29 - loss: 0.8217 - acc: 0.788 - ETA: 5:27 - loss: 0.8249 - acc: 0.787 - ETA: 5:25 - loss: 0.8239 - acc: 0.787 - ETA: 5:23 - loss: 0.8222 - acc: 0.787 - ETA: 5:21 - loss: 0.8194 - acc: 0.787 - ETA: 5:20 - loss: 0.8172 - acc: 0.787 - ETA: 5:18 - loss: 0.8172 - acc: 0.788 - ETA: 5:16 - loss: 0.8174 - acc: 0.787 - ETA: 5:14 - loss: 0.8111 - acc: 0.789 - ETA: 5:12 - loss: 0.8078 - acc: 0.790 - ETA: 5:11 - loss: 0.8092 - acc: 0.789 - ETA: 5:09 - loss: 0.8086 - acc: 0.788 - ETA: 5:07 - loss: 0.8090 - acc: 0.788 - ETA: 5:06 - loss: 0.8157 - acc: 0.785 - ETA: 5:04 - loss: 0.8172 - acc: 0.785 - ETA: 5:02 - loss: 0.8197 - acc: 0.784 - ETA: 5:01 - loss: 0.8168 - acc: 0.785 - ETA: 4:59 - loss: 0.8223 - acc: 0.784 - ETA: 4:57 - loss: 0.8221 - acc: 0.784 - ETA: 4:56 - loss: 0.8206 - acc: 0.784 - ETA: 4:54 - loss: 0.8206 - acc: 0.784 - ETA: 4:53 - loss: 0.8235 - acc: 0.782 - ETA: 4:51 - loss: 0.8257 - acc: 0.781 - ETA: 4:49 - loss: 0.8271 - acc: 0.780 - ETA: 4:48 - loss: 0.8349 - acc: 0.779 - ETA: 4:46 - loss: 0.8382 - acc: 0.778 - ETA: 4:45 - loss: 0.8386 - acc: 0.778 - ETA: 4:43 - loss: 0.8368 - acc: 0.779 - ETA: 4:42 - loss: 0.8368 - acc: 0.779 - ETA: 4:40 - loss: 0.8351 - acc: 0.779 - ETA: 4:39 - loss: 0.8341 - acc: 0.779 - ETA: 4:37 - loss: 0.8366 - acc: 0.779 - ETA: 4:36 - loss: 0.8439 - acc: 0.778 - ETA: 4:34 - loss: 0.8523 - acc: 0.775 - ETA: 4:33 - loss: 0.8537 - acc: 0.775 - ETA: 4:31 - loss: 0.8500 - acc: 0.776 - ETA: 4:30 - loss: 0.8518 - acc: 0.775 - ETA: 4:28 - loss: 0.8549 - acc: 0.773 - ETA: 4:27 - loss: 0.8534 - acc: 0.773 - ETA: 4:25 - loss: 0.8520 - acc: 0.773 - ETA: 4:24 - loss: 0.8516 - acc: 0.772 - ETA: 4:22 - loss: 0.8522 - acc: 0.773 - ETA: 4:21 - loss: 0.8528 - acc: 0.771 - ETA: 4:19 - loss: 0.8511 - acc: 0.772 - ETA: 4:18 - loss: 0.8526 - acc: 0.771 - ETA: 4:17 - loss: 0.8543 - acc: 0.771 - ETA: 4:15 - loss: 0.8556 - acc: 0.770 - ETA: 4:14 - loss: 0.8530 - acc: 0.771 - ETA: 4:12 - loss: 0.8544 - acc: 0.770 - ETA: 4:11 - loss: 0.8545 - acc: 0.770 - ETA: 4:10 - loss: 0.8532 - acc: 0.769 - ETA: 4:08 - loss: 0.8539 - acc: 0.769 - ETA: 4:07 - loss: 0.8556 - acc: 0.769 - ETA: 4:06 - loss: 0.8571 - acc: 0.769 - ETA: 4:04 - loss: 0.8554 - acc: 0.769 - ETA: 4:03 - loss: 0.8562 - acc: 0.768 - ETA: 4:01 - loss: 0.8581 - acc: 0.767 - ETA: 4:00 - loss: 0.8605 - acc: 0.767 - ETA: 3:59 - loss: 0.8572 - acc: 0.768 - ETA: 3:57 - loss: 0.8579 - acc: 0.768 - ETA: 3:56 - loss: 0.8559 - acc: 0.768 - ETA: 3:55 - loss: 0.8545 - acc: 0.768 - ETA: 3:53 - loss: 0.8508 - acc: 0.769 - ETA: 3:52 - loss: 0.8562 - acc: 0.769 - ETA: 3:51 - loss: 0.8557 - acc: 0.769 - ETA: 3:49 - loss: 0.8547 - acc: 0.769 - ETA: 3:48 - loss: 0.8529 - acc: 0.770 - ETA: 3:47 - loss: 0.8522 - acc: 0.770 - ETA: 3:45 - loss: 0.8526 - acc: 0.770 - ETA: 3:44 - loss: 0.8532 - acc: 0.770 - ETA: 3:43 - loss: 0.8561 - acc: 0.769 - ETA: 3:41 - loss: 0.8571 - acc: 0.768 - ETA: 3:40 - loss: 0.8553 - acc: 0.769 - ETA: 3:39 - loss: 0.8551 - acc: 0.769 - ETA: 3:37 - loss: 0.8560 - acc: 0.769 - ETA: 3:36 - loss: 0.8553 - acc: 0.769 - ETA: 3:35 - loss: 0.8527 - acc: 0.770 - ETA: 3:33 - loss: 0.8529 - acc: 0.770 - ETA: 3:32 - loss: 0.8541 - acc: 0.769 - ETA: 3:31 - loss: 0.8555 - acc: 0.769 - ETA: 3:30 - loss: 0.8584 - acc: 0.768 - ETA: 3:28 - loss: 0.8591 - acc: 0.768 - ETA: 3:27 - loss: 0.8611 - acc: 0.768 - ETA: 3:26 - loss: 0.8606 - acc: 0.768 - ETA: 3:24 - loss: 0.8611 - acc: 0.768 - ETA: 3:23 - loss: 0.8603 - acc: 0.769 - ETA: 3:22 - loss: 0.8618 - acc: 0.768 - ETA: 3:21 - loss: 0.8629 - acc: 0.768 - ETA: 3:19 - loss: 0.8651 - acc: 0.767 - ETA: 3:18 - loss: 0.8656 - acc: 0.767 - ETA: 3:17 - loss: 0.8688 - acc: 0.767 - ETA: 3:16 - loss: 0.8715 - acc: 0.765 - ETA: 3:14 - loss: 0.8716 - acc: 0.765 - ETA: 3:13 - loss: 0.8703 - acc: 0.765 - ETA: 3:12 - loss: 0.8675 - acc: 0.766 - ETA: 3:10 - loss: 0.8663 - acc: 0.766 - ETA: 3:09 - loss: 0.8638 - acc: 0.767 - ETA: 3:08 - loss: 0.8717 - acc: 0.765 - ETA: 3:07 - loss: 0.8735 - acc: 0.764 - ETA: 3:05 - loss: 0.8755 - acc: 0.764 - ETA: 3:04 - loss: 0.8744 - acc: 0.765 - ETA: 3:03 - loss: 0.8799 - acc: 0.764 - ETA: 3:02 - loss: 0.8809 - acc: 0.765 - ETA: 3:00 - loss: 0.8843 - acc: 0.764 - ETA: 2:59 - loss: 0.8829 - acc: 0.764 - ETA: 2:58 - loss: 0.8827 - acc: 0.765 - ETA: 2:57 - loss: 0.8843 - acc: 0.765 - ETA: 2:55 - loss: 0.8808 - acc: 0.766 - ETA: 2:54 - loss: 0.8813 - acc: 0.765 - ETA: 2:53 - loss: 0.8798 - acc: 0.766 - ETA: 2:52 - loss: 0.8797 - acc: 0.766 - ETA: 2:50 - loss: 0.8823 - acc: 0.766 - ETA: 2:49 - loss: 0.8819 - acc: 0.766 - ETA: 2:48 - loss: 0.8805 - acc: 0.766 - ETA: 2:47 - loss: 0.8808 - acc: 0.766 - ETA: 2:46 - loss: 0.8796 - acc: 0.766 - ETA: 2:44 - loss: 0.8792 - acc: 0.766 - ETA: 2:43 - loss: 0.8781 - acc: 0.766 - ETA: 2:42 - loss: 0.8794 - acc: 0.765 - ETA: 2:41 - loss: 0.8816 - acc: 0.765 - ETA: 2:39 - loss: 0.8809 - acc: 0.765 - ETA: 2:38 - loss: 0.8846 - acc: 0.764 - ETA: 2:37 - loss: 0.8841 - acc: 0.764 - ETA: 2:36 - loss: 0.8857 - acc: 0.764 - ETA: 2:35 - loss: 0.8879 - acc: 0.764 - ETA: 2:33 - loss: 0.8872 - acc: 0.764 - ETA: 2:32 - loss: 0.8875 - acc: 0.764 - ETA: 2:31 - loss: 0.8874 - acc: 0.76456680/6680 [==============================] - ETA: 2:30 - loss: 0.8895 - acc: 0.764 - ETA: 2:29 - loss: 0.8881 - acc: 0.764 - ETA: 2:27 - loss: 0.8895 - acc: 0.764 - ETA: 2:26 - loss: 0.8909 - acc: 0.763 - ETA: 2:25 - loss: 0.8926 - acc: 0.763 - ETA: 2:24 - loss: 0.8930 - acc: 0.762 - ETA: 2:23 - loss: 0.8952 - acc: 0.762 - ETA: 2:21 - loss: 0.8950 - acc: 0.762 - ETA: 2:20 - loss: 0.8969 - acc: 0.761 - ETA: 2:19 - loss: 0.8989 - acc: 0.761 - ETA: 2:18 - loss: 0.8975 - acc: 0.761 - ETA: 2:17 - loss: 0.8985 - acc: 0.761 - ETA: 2:15 - loss: 0.9000 - acc: 0.761 - ETA: 2:14 - loss: 0.8989 - acc: 0.761 - ETA: 2:13 - loss: 0.9001 - acc: 0.761 - ETA: 2:12 - loss: 0.9015 - acc: 0.760 - ETA: 2:11 - loss: 0.9027 - acc: 0.760 - ETA: 2:09 - loss: 0.9027 - acc: 0.761 - ETA: 2:08 - loss: 0.9056 - acc: 0.760 - ETA: 2:07 - loss: 0.9044 - acc: 0.761 - ETA: 2:06 - loss: 0.9048 - acc: 0.760 - ETA: 2:05 - loss: 0.9055 - acc: 0.760 - ETA: 2:03 - loss: 0.9057 - acc: 0.760 - ETA: 2:02 - loss: 0.9053 - acc: 0.760 - ETA: 2:01 - loss: 0.9044 - acc: 0.759 - ETA: 2:00 - loss: 0.9039 - acc: 0.760 - ETA: 1:59 - loss: 0.9023 - acc: 0.760 - ETA: 1:57 - loss: 0.9032 - acc: 0.760 - ETA: 1:56 - loss: 0.9037 - acc: 0.759 - ETA: 1:55 - loss: 0.9045 - acc: 0.759 - ETA: 1:54 - loss: 0.9039 - acc: 0.760 - ETA: 1:53 - loss: 0.9028 - acc: 0.760 - ETA: 1:52 - loss: 0.9041 - acc: 0.760 - ETA: 1:50 - loss: 0.9035 - acc: 0.760 - ETA: 1:49 - loss: 0.9045 - acc: 0.760 - ETA: 1:48 - loss: 0.9034 - acc: 0.760 - ETA: 1:47 - loss: 0.9024 - acc: 0.760 - ETA: 1:46 - loss: 0.9040 - acc: 0.760 - ETA: 1:44 - loss: 0.9032 - acc: 0.760 - ETA: 1:43 - loss: 0.9043 - acc: 0.760 - ETA: 1:42 - loss: 0.9030 - acc: 0.760 - ETA: 1:41 - loss: 0.9026 - acc: 0.760 - ETA: 1:40 - loss: 0.9021 - acc: 0.760 - ETA: 1:39 - loss: 0.9015 - acc: 0.760 - ETA: 1:37 - loss: 0.8996 - acc: 0.761 - ETA: 1:36 - loss: 0.8978 - acc: 0.761 - ETA: 1:35 - loss: 0.8975 - acc: 0.762 - ETA: 1:34 - loss: 0.8977 - acc: 0.761 - ETA: 1:33 - loss: 0.8962 - acc: 0.761 - ETA: 1:32 - loss: 0.8954 - acc: 0.762 - ETA: 1:30 - loss: 0.8974 - acc: 0.761 - ETA: 1:29 - loss: 0.8999 - acc: 0.760 - ETA: 1:28 - loss: 0.9005 - acc: 0.760 - ETA: 1:27 - loss: 0.9022 - acc: 0.760 - ETA: 1:26 - loss: 0.9043 - acc: 0.759 - ETA: 1:25 - loss: 0.9023 - acc: 0.760 - ETA: 1:23 - loss: 0.9017 - acc: 0.760 - ETA: 1:22 - loss: 0.9021 - acc: 0.760 - ETA: 1:21 - loss: 0.9017 - acc: 0.760 - ETA: 1:20 - loss: 0.9019 - acc: 0.760 - ETA: 1:19 - loss: 0.9031 - acc: 0.759 - ETA: 1:18 - loss: 0.9031 - acc: 0.759 - ETA: 1:16 - loss: 0.9040 - acc: 0.759 - ETA: 1:15 - loss: 0.9039 - acc: 0.759 - ETA: 1:14 - loss: 0.9053 - acc: 0.758 - ETA: 1:13 - loss: 0.9056 - acc: 0.758 - ETA: 1:12 - loss: 0.9066 - acc: 0.758 - ETA: 1:11 - loss: 0.9084 - acc: 0.758 - ETA: 1:10 - loss: 0.9073 - acc: 0.758 - ETA: 1:08 - loss: 0.9056 - acc: 0.759 - ETA: 1:07 - loss: 0.9080 - acc: 0.758 - ETA: 1:06 - loss: 0.9079 - acc: 0.758 - ETA: 1:05 - loss: 0.9084 - acc: 0.758 - ETA: 1:04 - loss: 0.9094 - acc: 0.758 - ETA: 1:03 - loss: 0.9089 - acc: 0.758 - ETA: 1:01 - loss: 0.9091 - acc: 0.758 - ETA: 1:00 - loss: 0.9107 - acc: 0.758 - ETA: 59s - loss: 0.9099 - acc: 0.758 - ETA: 58s - loss: 0.9111 - acc: 0.75 - ETA: 57s - loss: 0.9140 - acc: 0.75 - ETA: 56s - loss: 0.9127 - acc: 0.75 - ETA: 55s - loss: 0.9127 - acc: 0.75 - ETA: 53s - loss: 0.9126 - acc: 0.75 - ETA: 52s - loss: 0.9118 - acc: 0.75 - ETA: 51s - loss: 0.9107 - acc: 0.75 - ETA: 50s - loss: 0.9097 - acc: 0.75 - ETA: 49s - loss: 0.9095 - acc: 0.75 - ETA: 48s - loss: 0.9104 - acc: 0.75 - ETA: 46s - loss: 0.9101 - acc: 0.75 - ETA: 45s - loss: 0.9109 - acc: 0.75 - ETA: 44s - loss: 0.9121 - acc: 0.75 - ETA: 43s - loss: 0.9132 - acc: 0.75 - ETA: 42s - loss: 0.9126 - acc: 0.75 - ETA: 41s - loss: 0.9124 - acc: 0.75 - ETA: 40s - loss: 0.9121 - acc: 0.75 - ETA: 38s - loss: 0.9114 - acc: 0.75 - ETA: 37s - loss: 0.9112 - acc: 0.75 - ETA: 36s - loss: 0.9127 - acc: 0.75 - ETA: 35s - loss: 0.9126 - acc: 0.75 - ETA: 34s - loss: 0.9111 - acc: 0.75 - ETA: 33s - loss: 0.9109 - acc: 0.75 - ETA: 32s - loss: 0.9098 - acc: 0.75 - ETA: 30s - loss: 0.9098 - acc: 0.75 - ETA: 29s - loss: 0.9099 - acc: 0.75 - ETA: 28s - loss: 0.9094 - acc: 0.75 - ETA: 27s - loss: 0.9116 - acc: 0.75 - ETA: 26s - loss: 0.9127 - acc: 0.75 - ETA: 25s - loss: 0.9120 - acc: 0.75 - ETA: 24s - loss: 0.9119 - acc: 0.75 - ETA: 22s - loss: 0.9144 - acc: 0.75 - ETA: 21s - loss: 0.9155 - acc: 0.75 - ETA: 20s - loss: 0.9145 - acc: 0.75 - ETA: 19s - loss: 0.9158 - acc: 0.75 - ETA: 18s - loss: 0.9157 - acc: 0.75 - ETA: 17s - loss: 0.9141 - acc: 0.75 - ETA: 15s - loss: 0.9142 - acc: 0.75 - ETA: 14s - loss: 0.9139 - acc: 0.75 - ETA: 13s - loss: 0.9146 - acc: 0.75 - ETA: 12s - loss: 0.9147 - acc: 0.75 - ETA: 11s - loss: 0.9154 - acc: 0.75 - ETA: 10s - loss: 0.9152 - acc: 0.75 - ETA: 9s - loss: 0.9161 - acc: 0.7567 - ETA: 7s - loss: 0.9160 - acc: 0.756 - ETA: 6s - loss: 0.9157 - acc: 0.756 - ETA: 5s - loss: 0.9172 - acc: 0.756 - ETA: 4s - loss: 0.9165 - acc: 0.756 - ETA: 3s - loss: 0.9207 - acc: 0.755 - ETA: 2s - loss: 0.9198 - acc: 0.756 - ETA: 1s - loss: 0.9182 - acc: 0.756 - 393s 59ms/step - loss: 0.9186 - acc: 0.7563 - val\_loss: 5.8626 - val\_acc: 0.0934

Epoch 00007: val\_loss did not improve from 4.15157
Epoch 8/10
4080/6680 [=================>{\ldots}] - ETA: 6:03 - loss: 0.4199 - acc: 0.950 - ETA: 6:01 - loss: 0.5493 - acc: 0.925 - ETA: 6:02 - loss: 0.6154 - acc: 0.866 - ETA: 5:59 - loss: 0.6358 - acc: 0.850 - ETA: 5:57 - loss: 0.5858 - acc: 0.860 - ETA: 5:57 - loss: 0.5505 - acc: 0.866 - ETA: 5:55 - loss: 0.4977 - acc: 0.878 - ETA: 5:54 - loss: 0.5036 - acc: 0.875 - ETA: 5:53 - loss: 0.4746 - acc: 0.888 - ETA: 5:52 - loss: 0.4704 - acc: 0.890 - ETA: 5:51 - loss: 0.4392 - acc: 0.900 - ETA: 5:50 - loss: 0.4193 - acc: 0.904 - ETA: 5:49 - loss: 0.4148 - acc: 0.900 - ETA: 5:47 - loss: 0.4237 - acc: 0.892 - ETA: 5:46 - loss: 0.4081 - acc: 0.896 - ETA: 5:45 - loss: 0.3954 - acc: 0.900 - ETA: 5:44 - loss: 0.3942 - acc: 0.902 - ETA: 5:43 - loss: 0.3983 - acc: 0.900 - ETA: 5:42 - loss: 0.4069 - acc: 0.894 - ETA: 5:41 - loss: 0.4175 - acc: 0.892 - ETA: 5:40 - loss: 0.4186 - acc: 0.890 - ETA: 5:39 - loss: 0.4678 - acc: 0.881 - ETA: 5:38 - loss: 0.4615 - acc: 0.882 - ETA: 5:37 - loss: 0.4750 - acc: 0.877 - ETA: 5:36 - loss: 0.4707 - acc: 0.880 - ETA: 5:35 - loss: 0.4589 - acc: 0.882 - ETA: 5:34 - loss: 0.4648 - acc: 0.877 - ETA: 5:33 - loss: 0.4613 - acc: 0.880 - ETA: 5:32 - loss: 0.4749 - acc: 0.877 - ETA: 5:31 - loss: 0.4734 - acc: 0.875 - ETA: 5:30 - loss: 0.4663 - acc: 0.877 - ETA: 5:29 - loss: 0.4619 - acc: 0.878 - ETA: 5:28 - loss: 0.4564 - acc: 0.878 - ETA: 5:27 - loss: 0.4610 - acc: 0.873 - ETA: 5:26 - loss: 0.4771 - acc: 0.871 - ETA: 5:25 - loss: 0.4692 - acc: 0.875 - ETA: 5:24 - loss: 0.4728 - acc: 0.875 - ETA: 5:23 - loss: 0.4673 - acc: 0.876 - ETA: 5:22 - loss: 0.4708 - acc: 0.876 - ETA: 5:21 - loss: 0.4729 - acc: 0.876 - ETA: 5:19 - loss: 0.4709 - acc: 0.878 - ETA: 5:18 - loss: 0.4745 - acc: 0.877 - ETA: 5:17 - loss: 0.4727 - acc: 0.876 - ETA: 5:16 - loss: 0.4755 - acc: 0.875 - ETA: 5:15 - loss: 0.4732 - acc: 0.874 - ETA: 5:14 - loss: 0.4739 - acc: 0.872 - ETA: 5:13 - loss: 0.4768 - acc: 0.869 - ETA: 5:12 - loss: 0.4763 - acc: 0.870 - ETA: 5:11 - loss: 0.4783 - acc: 0.871 - ETA: 5:10 - loss: 0.4756 - acc: 0.872 - ETA: 5:08 - loss: 0.4811 - acc: 0.871 - ETA: 5:07 - loss: 0.4791 - acc: 0.872 - ETA: 5:06 - loss: 0.4805 - acc: 0.871 - ETA: 5:05 - loss: 0.4735 - acc: 0.874 - ETA: 5:04 - loss: 0.4798 - acc: 0.872 - ETA: 5:03 - loss: 0.4762 - acc: 0.874 - ETA: 5:02 - loss: 0.4753 - acc: 0.874 - ETA: 5:00 - loss: 0.4885 - acc: 0.871 - ETA: 4:59 - loss: 0.4970 - acc: 0.866 - ETA: 4:58 - loss: 0.5003 - acc: 0.865 - ETA: 4:57 - loss: 0.4978 - acc: 0.866 - ETA: 4:56 - loss: 0.5006 - acc: 0.866 - ETA: 4:55 - loss: 0.4986 - acc: 0.867 - ETA: 4:54 - loss: 0.5054 - acc: 0.865 - ETA: 4:53 - loss: 0.5069 - acc: 0.865 - ETA: 4:52 - loss: 0.5096 - acc: 0.865 - ETA: 4:51 - loss: 0.5127 - acc: 0.865 - ETA: 4:50 - loss: 0.5135 - acc: 0.865 - ETA: 4:49 - loss: 0.5130 - acc: 0.865 - ETA: 4:47 - loss: 0.5103 - acc: 0.866 - ETA: 4:46 - loss: 0.5082 - acc: 0.864 - ETA: 4:45 - loss: 0.5069 - acc: 0.864 - ETA: 4:44 - loss: 0.5026 - acc: 0.866 - ETA: 4:43 - loss: 0.5005 - acc: 0.866 - ETA: 4:42 - loss: 0.5046 - acc: 0.866 - ETA: 4:41 - loss: 0.5056 - acc: 0.865 - ETA: 4:40 - loss: 0.5037 - acc: 0.866 - ETA: 4:38 - loss: 0.5068 - acc: 0.866 - ETA: 4:37 - loss: 0.5069 - acc: 0.866 - ETA: 4:36 - loss: 0.5042 - acc: 0.866 - ETA: 4:35 - loss: 0.5074 - acc: 0.864 - ETA: 4:34 - loss: 0.5053 - acc: 0.864 - ETA: 4:33 - loss: 0.5060 - acc: 0.863 - ETA: 4:32 - loss: 0.5116 - acc: 0.862 - ETA: 4:31 - loss: 0.5123 - acc: 0.862 - ETA: 4:30 - loss: 0.5182 - acc: 0.860 - ETA: 4:29 - loss: 0.5152 - acc: 0.861 - ETA: 4:28 - loss: 0.5137 - acc: 0.861 - ETA: 4:27 - loss: 0.5145 - acc: 0.862 - ETA: 4:26 - loss: 0.5137 - acc: 0.862 - ETA: 4:25 - loss: 0.5124 - acc: 0.862 - ETA: 4:23 - loss: 0.5103 - acc: 0.862 - ETA: 4:22 - loss: 0.5141 - acc: 0.861 - ETA: 4:21 - loss: 0.5181 - acc: 0.861 - ETA: 4:20 - loss: 0.5163 - acc: 0.861 - ETA: 4:19 - loss: 0.5141 - acc: 0.862 - ETA: 4:18 - loss: 0.5115 - acc: 0.863 - ETA: 4:17 - loss: 0.5155 - acc: 0.861 - ETA: 4:16 - loss: 0.5141 - acc: 0.862 - ETA: 4:15 - loss: 0.5124 - acc: 0.862 - ETA: 4:13 - loss: 0.5127 - acc: 0.862 - ETA: 4:12 - loss: 0.5137 - acc: 0.861 - ETA: 4:11 - loss: 0.5105 - acc: 0.862 - ETA: 4:10 - loss: 0.5089 - acc: 0.862 - ETA: 4:09 - loss: 0.5090 - acc: 0.861 - ETA: 4:08 - loss: 0.5097 - acc: 0.862 - ETA: 4:07 - loss: 0.5103 - acc: 0.862 - ETA: 4:06 - loss: 0.5084 - acc: 0.863 - ETA: 4:05 - loss: 0.5100 - acc: 0.863 - ETA: 4:03 - loss: 0.5120 - acc: 0.862 - ETA: 4:02 - loss: 0.5136 - acc: 0.862 - ETA: 4:01 - loss: 0.5162 - acc: 0.861 - ETA: 4:01 - loss: 0.5269 - acc: 0.858 - ETA: 4:00 - loss: 0.5277 - acc: 0.858 - ETA: 3:59 - loss: 0.5290 - acc: 0.857 - ETA: 3:58 - loss: 0.5289 - acc: 0.857 - ETA: 3:56 - loss: 0.5294 - acc: 0.857 - ETA: 3:55 - loss: 0.5310 - acc: 0.856 - ETA: 3:54 - loss: 0.5288 - acc: 0.857 - ETA: 3:53 - loss: 0.5330 - acc: 0.855 - ETA: 3:52 - loss: 0.5340 - acc: 0.855 - ETA: 3:51 - loss: 0.5356 - acc: 0.854 - ETA: 3:50 - loss: 0.5357 - acc: 0.854 - ETA: 3:49 - loss: 0.5365 - acc: 0.853 - ETA: 3:48 - loss: 0.5399 - acc: 0.853 - ETA: 3:47 - loss: 0.5397 - acc: 0.852 - ETA: 3:46 - loss: 0.5428 - acc: 0.852 - ETA: 3:45 - loss: 0.5407 - acc: 0.852 - ETA: 3:44 - loss: 0.5469 - acc: 0.850 - ETA: 3:43 - loss: 0.5487 - acc: 0.850 - ETA: 3:42 - loss: 0.5520 - acc: 0.849 - ETA: 3:40 - loss: 0.5540 - acc: 0.849 - ETA: 3:39 - loss: 0.5543 - acc: 0.849 - ETA: 3:38 - loss: 0.5580 - acc: 0.848 - ETA: 3:37 - loss: 0.5582 - acc: 0.848 - ETA: 3:36 - loss: 0.5597 - acc: 0.848 - ETA: 3:35 - loss: 0.5602 - acc: 0.848 - ETA: 3:34 - loss: 0.5630 - acc: 0.848 - ETA: 3:33 - loss: 0.5624 - acc: 0.848 - ETA: 3:32 - loss: 0.5624 - acc: 0.847 - ETA: 3:31 - loss: 0.5602 - acc: 0.848 - ETA: 3:30 - loss: 0.5587 - acc: 0.848 - ETA: 3:29 - loss: 0.5580 - acc: 0.849 - ETA: 3:28 - loss: 0.5568 - acc: 0.849 - ETA: 3:26 - loss: 0.5599 - acc: 0.848 - ETA: 3:25 - loss: 0.5625 - acc: 0.847 - ETA: 3:24 - loss: 0.5641 - acc: 0.846 - ETA: 3:23 - loss: 0.5656 - acc: 0.846 - ETA: 3:22 - loss: 0.5667 - acc: 0.846 - ETA: 3:21 - loss: 0.5668 - acc: 0.846 - ETA: 3:20 - loss: 0.5646 - acc: 0.847 - ETA: 3:19 - loss: 0.5663 - acc: 0.846 - ETA: 3:18 - loss: 0.5662 - acc: 0.847 - ETA: 3:16 - loss: 0.5704 - acc: 0.846 - ETA: 3:15 - loss: 0.5692 - acc: 0.846 - ETA: 3:14 - loss: 0.5667 - acc: 0.847 - ETA: 3:13 - loss: 0.5669 - acc: 0.847 - ETA: 3:12 - loss: 0.5690 - acc: 0.846 - ETA: 3:11 - loss: 0.5659 - acc: 0.847 - ETA: 3:10 - loss: 0.5658 - acc: 0.847 - ETA: 3:09 - loss: 0.5686 - acc: 0.848 - ETA: 3:08 - loss: 0.5668 - acc: 0.848 - ETA: 3:07 - loss: 0.5669 - acc: 0.847 - ETA: 3:05 - loss: 0.5678 - acc: 0.847 - ETA: 3:04 - loss: 0.5661 - acc: 0.848 - ETA: 3:03 - loss: 0.5658 - acc: 0.848 - ETA: 3:02 - loss: 0.5671 - acc: 0.847 - ETA: 3:01 - loss: 0.5674 - acc: 0.848 - ETA: 3:00 - loss: 0.5683 - acc: 0.847 - ETA: 2:59 - loss: 0.5679 - acc: 0.847 - ETA: 2:58 - loss: 0.5682 - acc: 0.847 - ETA: 2:57 - loss: 0.5678 - acc: 0.848 - ETA: 2:56 - loss: 0.5761 - acc: 0.847 - ETA: 2:54 - loss: 0.5795 - acc: 0.846 - ETA: 2:53 - loss: 0.5814 - acc: 0.845 - ETA: 2:52 - loss: 0.5784 - acc: 0.846 - ETA: 2:51 - loss: 0.5797 - acc: 0.846 - ETA: 2:50 - loss: 0.5799 - acc: 0.846 - ETA: 2:49 - loss: 0.5776 - acc: 0.847 - ETA: 2:48 - loss: 0.5798 - acc: 0.847 - ETA: 2:47 - loss: 0.5796 - acc: 0.847 - ETA: 2:46 - loss: 0.5790 - acc: 0.847 - ETA: 2:45 - loss: 0.5802 - acc: 0.847 - ETA: 2:43 - loss: 0.5852 - acc: 0.846 - ETA: 2:42 - loss: 0.5858 - acc: 0.846 - ETA: 2:41 - loss: 0.5849 - acc: 0.847 - ETA: 2:40 - loss: 0.5866 - acc: 0.846 - ETA: 2:39 - loss: 0.5863 - acc: 0.846 - ETA: 2:38 - loss: 0.5848 - acc: 0.847 - ETA: 2:37 - loss: 0.5840 - acc: 0.847 - ETA: 2:36 - loss: 0.5828 - acc: 0.847 - ETA: 2:35 - loss: 0.5814 - acc: 0.848 - ETA: 2:34 - loss: 0.5846 - acc: 0.847 - ETA: 2:33 - loss: 0.5848 - acc: 0.847 - ETA: 2:32 - loss: 0.5857 - acc: 0.846 - ETA: 2:31 - loss: 0.5873 - acc: 0.845 - ETA: 2:29 - loss: 0.5877 - acc: 0.845 - ETA: 2:28 - loss: 0.5863 - acc: 0.846 - ETA: 2:27 - loss: 0.5841 - acc: 0.846 - ETA: 2:26 - loss: 0.5827 - acc: 0.847 - ETA: 2:25 - loss: 0.5840 - acc: 0.846 - ETA: 2:24 - loss: 0.5836 - acc: 0.846 - ETA: 2:23 - loss: 0.5832 - acc: 0.846 - ETA: 2:22 - loss: 0.5847 - acc: 0.84616680/6680 [==============================] - ETA: 2:21 - loss: 0.5860 - acc: 0.845 - ETA: 2:20 - loss: 0.5890 - acc: 0.844 - ETA: 2:19 - loss: 0.5901 - acc: 0.844 - ETA: 2:18 - loss: 0.5920 - acc: 0.843 - ETA: 2:16 - loss: 0.5907 - acc: 0.844 - ETA: 2:15 - loss: 0.5913 - acc: 0.843 - ETA: 2:14 - loss: 0.5926 - acc: 0.843 - ETA: 2:13 - loss: 0.5917 - acc: 0.843 - ETA: 2:12 - loss: 0.5928 - acc: 0.843 - ETA: 2:11 - loss: 0.5918 - acc: 0.843 - ETA: 2:10 - loss: 0.5911 - acc: 0.843 - ETA: 2:09 - loss: 0.5892 - acc: 0.843 - ETA: 2:08 - loss: 0.5883 - acc: 0.844 - ETA: 2:07 - loss: 0.5888 - acc: 0.843 - ETA: 2:06 - loss: 0.5883 - acc: 0.843 - ETA: 2:04 - loss: 0.5892 - acc: 0.843 - ETA: 2:03 - loss: 0.5883 - acc: 0.843 - ETA: 2:02 - loss: 0.5859 - acc: 0.843 - ETA: 2:01 - loss: 0.5845 - acc: 0.843 - ETA: 2:00 - loss: 0.5851 - acc: 0.844 - ETA: 1:59 - loss: 0.5863 - acc: 0.843 - ETA: 1:58 - loss: 0.5864 - acc: 0.843 - ETA: 1:57 - loss: 0.5862 - acc: 0.843 - ETA: 1:56 - loss: 0.5870 - acc: 0.843 - ETA: 1:55 - loss: 0.5890 - acc: 0.842 - ETA: 1:54 - loss: 0.5904 - acc: 0.842 - ETA: 1:52 - loss: 0.5915 - acc: 0.842 - ETA: 1:51 - loss: 0.5914 - acc: 0.842 - ETA: 1:50 - loss: 0.5905 - acc: 0.842 - ETA: 1:49 - loss: 0.5907 - acc: 0.841 - ETA: 1:48 - loss: 0.5946 - acc: 0.841 - ETA: 1:47 - loss: 0.5934 - acc: 0.841 - ETA: 1:46 - loss: 0.5938 - acc: 0.841 - ETA: 1:45 - loss: 0.5933 - acc: 0.841 - ETA: 1:44 - loss: 0.5927 - acc: 0.841 - ETA: 1:43 - loss: 0.5950 - acc: 0.841 - ETA: 1:42 - loss: 0.5948 - acc: 0.841 - ETA: 1:40 - loss: 0.5953 - acc: 0.840 - ETA: 1:39 - loss: 0.5942 - acc: 0.840 - ETA: 1:38 - loss: 0.5933 - acc: 0.841 - ETA: 1:37 - loss: 0.5927 - acc: 0.841 - ETA: 1:36 - loss: 0.5930 - acc: 0.841 - ETA: 1:35 - loss: 0.5936 - acc: 0.841 - ETA: 1:34 - loss: 0.5935 - acc: 0.841 - ETA: 1:33 - loss: 0.5941 - acc: 0.841 - ETA: 1:32 - loss: 0.5938 - acc: 0.841 - ETA: 1:31 - loss: 0.5936 - acc: 0.840 - ETA: 1:30 - loss: 0.5941 - acc: 0.840 - ETA: 1:28 - loss: 0.5943 - acc: 0.841 - ETA: 1:27 - loss: 0.5959 - acc: 0.840 - ETA: 1:26 - loss: 0.5966 - acc: 0.840 - ETA: 1:25 - loss: 0.5961 - acc: 0.840 - ETA: 1:24 - loss: 0.5961 - acc: 0.840 - ETA: 1:23 - loss: 0.5972 - acc: 0.840 - ETA: 1:22 - loss: 0.5974 - acc: 0.840 - ETA: 1:21 - loss: 0.6002 - acc: 0.839 - ETA: 1:20 - loss: 0.6020 - acc: 0.838 - ETA: 1:19 - loss: 0.6024 - acc: 0.838 - ETA: 1:18 - loss: 0.6010 - acc: 0.838 - ETA: 1:16 - loss: 0.6033 - acc: 0.838 - ETA: 1:15 - loss: 0.6038 - acc: 0.838 - ETA: 1:14 - loss: 0.6044 - acc: 0.837 - ETA: 1:13 - loss: 0.6070 - acc: 0.837 - ETA: 1:12 - loss: 0.6057 - acc: 0.837 - ETA: 1:11 - loss: 0.6048 - acc: 0.837 - ETA: 1:10 - loss: 0.6029 - acc: 0.838 - ETA: 1:09 - loss: 0.6016 - acc: 0.838 - ETA: 1:08 - loss: 0.6008 - acc: 0.838 - ETA: 1:07 - loss: 0.6008 - acc: 0.838 - ETA: 1:05 - loss: 0.6001 - acc: 0.838 - ETA: 1:04 - loss: 0.6013 - acc: 0.838 - ETA: 1:03 - loss: 0.6023 - acc: 0.838 - ETA: 1:02 - loss: 0.6023 - acc: 0.838 - ETA: 1:01 - loss: 0.6034 - acc: 0.837 - ETA: 1:00 - loss: 0.6038 - acc: 0.838 - ETA: 59s - loss: 0.6047 - acc: 0.837 - ETA: 58s - loss: 0.6049 - acc: 0.83 - ETA: 57s - loss: 0.6050 - acc: 0.83 - ETA: 56s - loss: 0.6055 - acc: 0.83 - ETA: 54s - loss: 0.6041 - acc: 0.83 - ETA: 53s - loss: 0.6033 - acc: 0.83 - ETA: 52s - loss: 0.6043 - acc: 0.83 - ETA: 51s - loss: 0.6036 - acc: 0.83 - ETA: 50s - loss: 0.6030 - acc: 0.83 - ETA: 49s - loss: 0.6060 - acc: 0.83 - ETA: 48s - loss: 0.6071 - acc: 0.83 - ETA: 47s - loss: 0.6073 - acc: 0.83 - ETA: 46s - loss: 0.6079 - acc: 0.83 - ETA: 45s - loss: 0.6084 - acc: 0.83 - ETA: 43s - loss: 0.6098 - acc: 0.83 - ETA: 42s - loss: 0.6114 - acc: 0.83 - ETA: 41s - loss: 0.6112 - acc: 0.83 - ETA: 40s - loss: 0.6111 - acc: 0.83 - ETA: 39s - loss: 0.6110 - acc: 0.83 - ETA: 38s - loss: 0.6144 - acc: 0.83 - ETA: 37s - loss: 0.6146 - acc: 0.83 - ETA: 36s - loss: 0.6144 - acc: 0.83 - ETA: 35s - loss: 0.6153 - acc: 0.83 - ETA: 34s - loss: 0.6161 - acc: 0.83 - ETA: 32s - loss: 0.6175 - acc: 0.83 - ETA: 31s - loss: 0.6170 - acc: 0.83 - ETA: 30s - loss: 0.6176 - acc: 0.83 - ETA: 29s - loss: 0.6176 - acc: 0.83 - ETA: 28s - loss: 0.6171 - acc: 0.83 - ETA: 27s - loss: 0.6172 - acc: 0.83 - ETA: 26s - loss: 0.6179 - acc: 0.83 - ETA: 25s - loss: 0.6172 - acc: 0.83 - ETA: 24s - loss: 0.6183 - acc: 0.83 - ETA: 23s - loss: 0.6173 - acc: 0.83 - ETA: 21s - loss: 0.6190 - acc: 0.83 - ETA: 20s - loss: 0.6195 - acc: 0.83 - ETA: 19s - loss: 0.6203 - acc: 0.83 - ETA: 18s - loss: 0.6227 - acc: 0.83 - ETA: 17s - loss: 0.6219 - acc: 0.83 - ETA: 16s - loss: 0.6214 - acc: 0.83 - ETA: 15s - loss: 0.6206 - acc: 0.83 - ETA: 14s - loss: 0.6200 - acc: 0.83 - ETA: 13s - loss: 0.6192 - acc: 0.83 - ETA: 12s - loss: 0.6180 - acc: 0.83 - ETA: 10s - loss: 0.6170 - acc: 0.83 - ETA: 9s - loss: 0.6169 - acc: 0.8355 - ETA: 8s - loss: 0.6172 - acc: 0.835 - ETA: 7s - loss: 0.6161 - acc: 0.835 - ETA: 6s - loss: 0.6160 - acc: 0.835 - ETA: 5s - loss: 0.6149 - acc: 0.835 - ETA: 4s - loss: 0.6144 - acc: 0.836 - ETA: 3s - loss: 0.6138 - acc: 0.836 - ETA: 2s - loss: 0.6137 - acc: 0.836 - ETA: 1s - loss: 0.6125 - acc: 0.836 - 378s 57ms/step - loss: 0.6136 - acc: 0.8368 - val\_loss: 6.2037 - val\_acc: 0.0731

Epoch 00008: val\_loss did not improve from 4.15157
Epoch 9/10
4080/6680 [=================>{\ldots}] - ETA: 5:59 - loss: 0.3418 - acc: 0.850 - ETA: 5:59 - loss: 0.2782 - acc: 0.925 - ETA: 5:58 - loss: 0.3067 - acc: 0.883 - ETA: 5:58 - loss: 0.3219 - acc: 0.887 - ETA: 5:55 - loss: 0.3322 - acc: 0.880 - ETA: 5:54 - loss: 0.3299 - acc: 0.883 - ETA: 5:53 - loss: 0.4200 - acc: 0.864 - ETA: 5:52 - loss: 0.4023 - acc: 0.875 - ETA: 5:51 - loss: 0.4223 - acc: 0.872 - ETA: 5:50 - loss: 0.4014 - acc: 0.875 - ETA: 5:49 - loss: 0.3898 - acc: 0.877 - ETA: 5:49 - loss: 0.3892 - acc: 0.875 - ETA: 5:49 - loss: 0.3979 - acc: 0.869 - ETA: 5:48 - loss: 0.3865 - acc: 0.875 - ETA: 5:47 - loss: 0.3713 - acc: 0.880 - ETA: 5:45 - loss: 0.3555 - acc: 0.884 - ETA: 5:45 - loss: 0.3514 - acc: 0.888 - ETA: 5:44 - loss: 0.3701 - acc: 0.886 - ETA: 5:43 - loss: 0.3646 - acc: 0.889 - ETA: 5:41 - loss: 0.3512 - acc: 0.892 - ETA: 5:40 - loss: 0.3479 - acc: 0.895 - ETA: 5:39 - loss: 0.3600 - acc: 0.890 - ETA: 5:38 - loss: 0.3488 - acc: 0.895 - ETA: 5:37 - loss: 0.3417 - acc: 0.895 - ETA: 5:37 - loss: 0.3508 - acc: 0.894 - ETA: 5:35 - loss: 0.3447 - acc: 0.896 - ETA: 5:34 - loss: 0.3482 - acc: 0.898 - ETA: 5:33 - loss: 0.3452 - acc: 0.898 - ETA: 5:32 - loss: 0.3443 - acc: 0.900 - ETA: 5:31 - loss: 0.3390 - acc: 0.901 - ETA: 5:30 - loss: 0.3449 - acc: 0.900 - ETA: 5:28 - loss: 0.3434 - acc: 0.900 - ETA: 5:27 - loss: 0.3522 - acc: 0.900 - ETA: 5:26 - loss: 0.3429 - acc: 0.902 - ETA: 5:25 - loss: 0.3629 - acc: 0.901 - ETA: 5:24 - loss: 0.3745 - acc: 0.901 - ETA: 5:23 - loss: 0.3725 - acc: 0.901 - ETA: 5:22 - loss: 0.3697 - acc: 0.901 - ETA: 5:21 - loss: 0.3818 - acc: 0.898 - ETA: 5:20 - loss: 0.3791 - acc: 0.900 - ETA: 5:18 - loss: 0.3764 - acc: 0.900 - ETA: 5:17 - loss: 0.3702 - acc: 0.901 - ETA: 5:16 - loss: 0.3736 - acc: 0.901 - ETA: 5:15 - loss: 0.3861 - acc: 0.900 - ETA: 5:14 - loss: 0.3829 - acc: 0.901 - ETA: 5:13 - loss: 0.3848 - acc: 0.900 - ETA: 5:12 - loss: 0.3820 - acc: 0.900 - ETA: 5:11 - loss: 0.3812 - acc: 0.900 - ETA: 5:10 - loss: 0.3750 - acc: 0.902 - ETA: 5:09 - loss: 0.3730 - acc: 0.902 - ETA: 5:08 - loss: 0.3723 - acc: 0.901 - ETA: 5:07 - loss: 0.3676 - acc: 0.901 - ETA: 5:05 - loss: 0.3810 - acc: 0.898 - ETA: 5:04 - loss: 0.3791 - acc: 0.899 - ETA: 5:03 - loss: 0.3814 - acc: 0.899 - ETA: 5:02 - loss: 0.3868 - acc: 0.897 - ETA: 5:01 - loss: 0.3845 - acc: 0.897 - ETA: 5:00 - loss: 0.3849 - acc: 0.897 - ETA: 4:59 - loss: 0.3807 - acc: 0.899 - ETA: 4:58 - loss: 0.3761 - acc: 0.900 - ETA: 4:57 - loss: 0.3733 - acc: 0.901 - ETA: 4:56 - loss: 0.3812 - acc: 0.900 - ETA: 4:55 - loss: 0.3800 - acc: 0.900 - ETA: 4:54 - loss: 0.3814 - acc: 0.900 - ETA: 4:52 - loss: 0.3796 - acc: 0.900 - ETA: 4:51 - loss: 0.3816 - acc: 0.899 - ETA: 4:50 - loss: 0.3780 - acc: 0.900 - ETA: 4:49 - loss: 0.3775 - acc: 0.900 - ETA: 4:48 - loss: 0.3786 - acc: 0.900 - ETA: 4:47 - loss: 0.3825 - acc: 0.897 - ETA: 4:46 - loss: 0.3817 - acc: 0.897 - ETA: 4:45 - loss: 0.3828 - acc: 0.897 - ETA: 4:44 - loss: 0.3786 - acc: 0.899 - ETA: 4:42 - loss: 0.3761 - acc: 0.899 - ETA: 4:41 - loss: 0.3759 - acc: 0.899 - ETA: 4:40 - loss: 0.3744 - acc: 0.899 - ETA: 4:39 - loss: 0.3700 - acc: 0.900 - ETA: 4:38 - loss: 0.3663 - acc: 0.901 - ETA: 4:37 - loss: 0.3665 - acc: 0.901 - ETA: 4:36 - loss: 0.3658 - acc: 0.901 - ETA: 4:35 - loss: 0.3671 - acc: 0.901 - ETA: 4:34 - loss: 0.3667 - acc: 0.901 - ETA: 4:33 - loss: 0.3664 - acc: 0.901 - ETA: 4:32 - loss: 0.3673 - acc: 0.900 - ETA: 4:30 - loss: 0.3751 - acc: 0.899 - ETA: 4:29 - loss: 0.3715 - acc: 0.900 - ETA: 4:28 - loss: 0.3716 - acc: 0.900 - ETA: 4:27 - loss: 0.3694 - acc: 0.901 - ETA: 4:26 - loss: 0.3699 - acc: 0.901 - ETA: 4:25 - loss: 0.3675 - acc: 0.902 - ETA: 4:24 - loss: 0.3639 - acc: 0.903 - ETA: 4:23 - loss: 0.3623 - acc: 0.904 - ETA: 4:22 - loss: 0.3640 - acc: 0.902 - ETA: 4:21 - loss: 0.3633 - acc: 0.902 - ETA: 4:20 - loss: 0.3611 - acc: 0.902 - ETA: 4:19 - loss: 0.3611 - acc: 0.902 - ETA: 4:18 - loss: 0.3590 - acc: 0.902 - ETA: 4:17 - loss: 0.3657 - acc: 0.902 - ETA: 4:16 - loss: 0.3672 - acc: 0.901 - ETA: 4:15 - loss: 0.3671 - acc: 0.901 - ETA: 4:14 - loss: 0.3689 - acc: 0.901 - ETA: 4:13 - loss: 0.3746 - acc: 0.900 - ETA: 4:12 - loss: 0.3754 - acc: 0.900 - ETA: 4:11 - loss: 0.3756 - acc: 0.900 - ETA: 4:10 - loss: 0.3730 - acc: 0.901 - ETA: 4:08 - loss: 0.3715 - acc: 0.901 - ETA: 4:07 - loss: 0.3749 - acc: 0.899 - ETA: 4:06 - loss: 0.3762 - acc: 0.899 - ETA: 4:05 - loss: 0.3762 - acc: 0.898 - ETA: 4:04 - loss: 0.3780 - acc: 0.897 - ETA: 4:03 - loss: 0.3767 - acc: 0.897 - ETA: 4:02 - loss: 0.3753 - acc: 0.897 - ETA: 4:01 - loss: 0.3737 - acc: 0.897 - ETA: 4:00 - loss: 0.3770 - acc: 0.896 - ETA: 3:59 - loss: 0.3754 - acc: 0.897 - ETA: 3:58 - loss: 0.3782 - acc: 0.896 - ETA: 3:57 - loss: 0.3791 - acc: 0.895 - ETA: 3:56 - loss: 0.3771 - acc: 0.896 - ETA: 3:54 - loss: 0.3759 - acc: 0.896 - ETA: 3:53 - loss: 0.3797 - acc: 0.895 - ETA: 3:52 - loss: 0.3793 - acc: 0.895 - ETA: 3:51 - loss: 0.3792 - acc: 0.895 - ETA: 3:50 - loss: 0.3815 - acc: 0.895 - ETA: 3:49 - loss: 0.3808 - acc: 0.895 - ETA: 3:48 - loss: 0.3802 - acc: 0.895 - ETA: 3:47 - loss: 0.3793 - acc: 0.895 - ETA: 3:46 - loss: 0.3791 - acc: 0.895 - ETA: 3:45 - loss: 0.3786 - acc: 0.895 - ETA: 3:44 - loss: 0.3798 - acc: 0.894 - ETA: 3:43 - loss: 0.3793 - acc: 0.894 - ETA: 3:42 - loss: 0.3801 - acc: 0.893 - ETA: 3:41 - loss: 0.3795 - acc: 0.893 - ETA: 3:40 - loss: 0.3780 - acc: 0.893 - ETA: 3:39 - loss: 0.3778 - acc: 0.894 - ETA: 3:37 - loss: 0.3789 - acc: 0.893 - ETA: 3:36 - loss: 0.3818 - acc: 0.893 - ETA: 3:35 - loss: 0.3809 - acc: 0.893 - ETA: 3:34 - loss: 0.3805 - acc: 0.893 - ETA: 3:33 - loss: 0.3812 - acc: 0.893 - ETA: 3:32 - loss: 0.3807 - acc: 0.893 - ETA: 3:31 - loss: 0.3803 - acc: 0.894 - ETA: 3:30 - loss: 0.3835 - acc: 0.893 - ETA: 3:29 - loss: 0.3843 - acc: 0.892 - ETA: 3:28 - loss: 0.3836 - acc: 0.892 - ETA: 3:27 - loss: 0.3855 - acc: 0.892 - ETA: 3:25 - loss: 0.3839 - acc: 0.892 - ETA: 3:24 - loss: 0.3824 - acc: 0.893 - ETA: 3:23 - loss: 0.3818 - acc: 0.893 - ETA: 3:22 - loss: 0.3825 - acc: 0.893 - ETA: 3:21 - loss: 0.3807 - acc: 0.893 - ETA: 3:20 - loss: 0.3812 - acc: 0.893 - ETA: 3:19 - loss: 0.3801 - acc: 0.893 - ETA: 3:18 - loss: 0.3799 - acc: 0.892 - ETA: 3:17 - loss: 0.3794 - acc: 0.892 - ETA: 3:16 - loss: 0.3795 - acc: 0.892 - ETA: 3:15 - loss: 0.3817 - acc: 0.892 - ETA: 3:14 - loss: 0.3810 - acc: 0.892 - ETA: 3:13 - loss: 0.3807 - acc: 0.892 - ETA: 3:11 - loss: 0.3792 - acc: 0.892 - ETA: 3:10 - loss: 0.3777 - acc: 0.892 - ETA: 3:09 - loss: 0.3768 - acc: 0.892 - ETA: 3:08 - loss: 0.3772 - acc: 0.892 - ETA: 3:07 - loss: 0.3769 - acc: 0.892 - ETA: 3:06 - loss: 0.3772 - acc: 0.892 - ETA: 3:05 - loss: 0.3785 - acc: 0.892 - ETA: 3:04 - loss: 0.3797 - acc: 0.892 - ETA: 3:03 - loss: 0.3799 - acc: 0.892 - ETA: 3:02 - loss: 0.3789 - acc: 0.892 - ETA: 3:01 - loss: 0.3805 - acc: 0.891 - ETA: 2:59 - loss: 0.3812 - acc: 0.891 - ETA: 2:58 - loss: 0.3821 - acc: 0.891 - ETA: 2:57 - loss: 0.3850 - acc: 0.890 - ETA: 2:56 - loss: 0.3873 - acc: 0.889 - ETA: 2:55 - loss: 0.3868 - acc: 0.890 - ETA: 2:54 - loss: 0.3877 - acc: 0.890 - ETA: 2:53 - loss: 0.3877 - acc: 0.889 - ETA: 2:52 - loss: 0.3879 - acc: 0.889 - ETA: 2:51 - loss: 0.3863 - acc: 0.889 - ETA: 2:50 - loss: 0.3860 - acc: 0.889 - ETA: 2:48 - loss: 0.3850 - acc: 0.890 - ETA: 2:47 - loss: 0.3842 - acc: 0.890 - ETA: 2:46 - loss: 0.3885 - acc: 0.889 - ETA: 2:45 - loss: 0.3919 - acc: 0.889 - ETA: 2:44 - loss: 0.3948 - acc: 0.888 - ETA: 2:43 - loss: 0.3945 - acc: 0.888 - ETA: 2:42 - loss: 0.3929 - acc: 0.889 - ETA: 2:41 - loss: 0.3930 - acc: 0.889 - ETA: 2:40 - loss: 0.3942 - acc: 0.888 - ETA: 2:39 - loss: 0.3949 - acc: 0.888 - ETA: 2:37 - loss: 0.3957 - acc: 0.887 - ETA: 2:36 - loss: 0.3943 - acc: 0.888 - ETA: 2:35 - loss: 0.3940 - acc: 0.888 - ETA: 2:34 - loss: 0.3930 - acc: 0.888 - ETA: 2:33 - loss: 0.3929 - acc: 0.888 - ETA: 2:32 - loss: 0.3932 - acc: 0.888 - ETA: 2:31 - loss: 0.3950 - acc: 0.888 - ETA: 2:30 - loss: 0.3949 - acc: 0.888 - ETA: 2:29 - loss: 0.3945 - acc: 0.888 - ETA: 2:28 - loss: 0.3934 - acc: 0.888 - ETA: 2:27 - loss: 0.3945 - acc: 0.888 - ETA: 2:25 - loss: 0.3948 - acc: 0.888 - ETA: 2:24 - loss: 0.3944 - acc: 0.888 - ETA: 2:23 - loss: 0.3943 - acc: 0.888 - ETA: 2:22 - loss: 0.3997 - acc: 0.88736680/6680 [==============================] - ETA: 2:21 - loss: 0.3994 - acc: 0.887 - ETA: 2:20 - loss: 0.3999 - acc: 0.887 - ETA: 2:19 - loss: 0.3990 - acc: 0.887 - ETA: 2:18 - loss: 0.3981 - acc: 0.888 - ETA: 2:17 - loss: 0.3975 - acc: 0.888 - ETA: 2:15 - loss: 0.3962 - acc: 0.888 - ETA: 2:14 - loss: 0.3981 - acc: 0.888 - ETA: 2:13 - loss: 0.3996 - acc: 0.888 - ETA: 2:12 - loss: 0.3997 - acc: 0.887 - ETA: 2:11 - loss: 0.4029 - acc: 0.887 - ETA: 2:10 - loss: 0.4038 - acc: 0.887 - ETA: 2:09 - loss: 0.4030 - acc: 0.887 - ETA: 2:08 - loss: 0.4024 - acc: 0.887 - ETA: 2:07 - loss: 0.4008 - acc: 0.888 - ETA: 2:06 - loss: 0.4024 - acc: 0.887 - ETA: 2:05 - loss: 0.4024 - acc: 0.887 - ETA: 2:03 - loss: 0.4032 - acc: 0.887 - ETA: 2:02 - loss: 0.4038 - acc: 0.886 - ETA: 2:01 - loss: 0.4049 - acc: 0.886 - ETA: 2:00 - loss: 0.4093 - acc: 0.885 - ETA: 1:59 - loss: 0.4086 - acc: 0.885 - ETA: 1:58 - loss: 0.4093 - acc: 0.885 - ETA: 1:57 - loss: 0.4108 - acc: 0.885 - ETA: 1:56 - loss: 0.4098 - acc: 0.885 - ETA: 1:55 - loss: 0.4092 - acc: 0.885 - ETA: 1:54 - loss: 0.4080 - acc: 0.886 - ETA: 1:53 - loss: 0.4092 - acc: 0.885 - ETA: 1:51 - loss: 0.4096 - acc: 0.885 - ETA: 1:50 - loss: 0.4094 - acc: 0.885 - ETA: 1:49 - loss: 0.4124 - acc: 0.884 - ETA: 1:48 - loss: 0.4122 - acc: 0.884 - ETA: 1:47 - loss: 0.4137 - acc: 0.884 - ETA: 1:46 - loss: 0.4137 - acc: 0.884 - ETA: 1:45 - loss: 0.4150 - acc: 0.884 - ETA: 1:44 - loss: 0.4147 - acc: 0.884 - ETA: 1:43 - loss: 0.4143 - acc: 0.884 - ETA: 1:42 - loss: 0.4129 - acc: 0.884 - ETA: 1:40 - loss: 0.4134 - acc: 0.884 - ETA: 1:39 - loss: 0.4133 - acc: 0.884 - ETA: 1:38 - loss: 0.4138 - acc: 0.884 - ETA: 1:37 - loss: 0.4160 - acc: 0.884 - ETA: 1:36 - loss: 0.4160 - acc: 0.884 - ETA: 1:35 - loss: 0.4170 - acc: 0.884 - ETA: 1:34 - loss: 0.4174 - acc: 0.884 - ETA: 1:33 - loss: 0.4165 - acc: 0.885 - ETA: 1:32 - loss: 0.4162 - acc: 0.885 - ETA: 1:31 - loss: 0.4184 - acc: 0.884 - ETA: 1:29 - loss: 0.4181 - acc: 0.884 - ETA: 1:28 - loss: 0.4203 - acc: 0.884 - ETA: 1:27 - loss: 0.4208 - acc: 0.884 - ETA: 1:26 - loss: 0.4202 - acc: 0.884 - ETA: 1:25 - loss: 0.4205 - acc: 0.884 - ETA: 1:24 - loss: 0.4224 - acc: 0.883 - ETA: 1:23 - loss: 0.4223 - acc: 0.883 - ETA: 1:22 - loss: 0.4217 - acc: 0.884 - ETA: 1:21 - loss: 0.4219 - acc: 0.884 - ETA: 1:20 - loss: 0.4215 - acc: 0.884 - ETA: 1:18 - loss: 0.4216 - acc: 0.884 - ETA: 1:17 - loss: 0.4207 - acc: 0.884 - ETA: 1:16 - loss: 0.4218 - acc: 0.883 - ETA: 1:15 - loss: 0.4223 - acc: 0.883 - ETA: 1:14 - loss: 0.4215 - acc: 0.883 - ETA: 1:13 - loss: 0.4211 - acc: 0.883 - ETA: 1:12 - loss: 0.4197 - acc: 0.884 - ETA: 1:11 - loss: 0.4186 - acc: 0.884 - ETA: 1:10 - loss: 0.4174 - acc: 0.885 - ETA: 1:09 - loss: 0.4167 - acc: 0.885 - ETA: 1:07 - loss: 0.4160 - acc: 0.885 - ETA: 1:06 - loss: 0.4184 - acc: 0.884 - ETA: 1:05 - loss: 0.4171 - acc: 0.885 - ETA: 1:04 - loss: 0.4177 - acc: 0.885 - ETA: 1:03 - loss: 0.4180 - acc: 0.885 - ETA: 1:02 - loss: 0.4186 - acc: 0.884 - ETA: 1:01 - loss: 0.4179 - acc: 0.884 - ETA: 1:00 - loss: 0.4196 - acc: 0.884 - ETA: 59s - loss: 0.4206 - acc: 0.884 - ETA: 58s - loss: 0.4208 - acc: 0.88 - ETA: 57s - loss: 0.4212 - acc: 0.88 - ETA: 55s - loss: 0.4215 - acc: 0.88 - ETA: 54s - loss: 0.4227 - acc: 0.88 - ETA: 53s - loss: 0.4232 - acc: 0.88 - ETA: 52s - loss: 0.4240 - acc: 0.88 - ETA: 51s - loss: 0.4244 - acc: 0.88 - ETA: 50s - loss: 0.4248 - acc: 0.88 - ETA: 49s - loss: 0.4264 - acc: 0.88 - ETA: 48s - loss: 0.4275 - acc: 0.88 - ETA: 47s - loss: 0.4275 - acc: 0.88 - ETA: 46s - loss: 0.4301 - acc: 0.88 - ETA: 44s - loss: 0.4293 - acc: 0.88 - ETA: 43s - loss: 0.4293 - acc: 0.88 - ETA: 42s - loss: 0.4290 - acc: 0.88 - ETA: 41s - loss: 0.4289 - acc: 0.88 - ETA: 40s - loss: 0.4297 - acc: 0.88 - ETA: 39s - loss: 0.4308 - acc: 0.88 - ETA: 38s - loss: 0.4318 - acc: 0.88 - ETA: 37s - loss: 0.4332 - acc: 0.88 - ETA: 36s - loss: 0.4329 - acc: 0.88 - ETA: 35s - loss: 0.4328 - acc: 0.88 - ETA: 33s - loss: 0.4324 - acc: 0.88 - ETA: 32s - loss: 0.4320 - acc: 0.88 - ETA: 31s - loss: 0.4347 - acc: 0.88 - ETA: 30s - loss: 0.4350 - acc: 0.88 - ETA: 29s - loss: 0.4370 - acc: 0.88 - ETA: 28s - loss: 0.4365 - acc: 0.88 - ETA: 27s - loss: 0.4370 - acc: 0.88 - ETA: 26s - loss: 0.4371 - acc: 0.88 - ETA: 25s - loss: 0.4367 - acc: 0.88 - ETA: 24s - loss: 0.4376 - acc: 0.88 - ETA: 23s - loss: 0.4369 - acc: 0.88 - ETA: 21s - loss: 0.4369 - acc: 0.88 - ETA: 20s - loss: 0.4373 - acc: 0.88 - ETA: 19s - loss: 0.4368 - acc: 0.88 - ETA: 18s - loss: 0.4378 - acc: 0.88 - ETA: 17s - loss: 0.4377 - acc: 0.88 - ETA: 16s - loss: 0.4376 - acc: 0.88 - ETA: 15s - loss: 0.4374 - acc: 0.88 - ETA: 14s - loss: 0.4368 - acc: 0.88 - ETA: 13s - loss: 0.4368 - acc: 0.88 - ETA: 12s - loss: 0.4368 - acc: 0.88 - ETA: 10s - loss: 0.4363 - acc: 0.88 - ETA: 9s - loss: 0.4364 - acc: 0.8808 - ETA: 8s - loss: 0.4365 - acc: 0.880 - ETA: 7s - loss: 0.4371 - acc: 0.880 - ETA: 6s - loss: 0.4361 - acc: 0.880 - ETA: 5s - loss: 0.4360 - acc: 0.880 - ETA: 4s - loss: 0.4353 - acc: 0.881 - ETA: 3s - loss: 0.4354 - acc: 0.881 - ETA: 2s - loss: 0.4348 - acc: 0.881 - ETA: 1s - loss: 0.4354 - acc: 0.881 - 378s 57ms/step - loss: 0.4349 - acc: 0.8817 - val\_loss: 6.9677 - val\_acc: 0.0934

Epoch 00009: val\_loss did not improve from 4.15157
Epoch 10/10
4080/6680 [=================>{\ldots}] - ETA: 6:01 - loss: 0.3952 - acc: 0.850 - ETA: 5:59 - loss: 0.4313 - acc: 0.875 - ETA: 5:58 - loss: 0.3529 - acc: 0.900 - ETA: 5:57 - loss: 0.3253 - acc: 0.925 - ETA: 5:56 - loss: 0.3201 - acc: 0.930 - ETA: 5:55 - loss: 0.2908 - acc: 0.941 - ETA: 5:54 - loss: 0.2649 - acc: 0.942 - ETA: 5:53 - loss: 0.2646 - acc: 0.943 - ETA: 5:52 - loss: 0.2528 - acc: 0.944 - ETA: 5:51 - loss: 0.2403 - acc: 0.950 - ETA: 5:50 - loss: 0.2516 - acc: 0.945 - ETA: 5:49 - loss: 0.2505 - acc: 0.945 - ETA: 5:48 - loss: 0.2608 - acc: 0.946 - ETA: 5:47 - loss: 0.2523 - acc: 0.946 - ETA: 5:46 - loss: 0.2408 - acc: 0.950 - ETA: 5:45 - loss: 0.2390 - acc: 0.950 - ETA: 5:45 - loss: 0.2380 - acc: 0.950 - ETA: 5:44 - loss: 0.2447 - acc: 0.944 - ETA: 5:42 - loss: 0.2564 - acc: 0.942 - ETA: 5:41 - loss: 0.2493 - acc: 0.945 - ETA: 5:40 - loss: 0.2588 - acc: 0.940 - ETA: 5:39 - loss: 0.2509 - acc: 0.943 - ETA: 5:38 - loss: 0.2455 - acc: 0.943 - ETA: 5:37 - loss: 0.2579 - acc: 0.939 - ETA: 5:36 - loss: 0.2648 - acc: 0.938 - ETA: 5:35 - loss: 0.2591 - acc: 0.940 - ETA: 5:34 - loss: 0.2661 - acc: 0.938 - ETA: 5:32 - loss: 0.2679 - acc: 0.937 - ETA: 5:31 - loss: 0.2847 - acc: 0.936 - ETA: 5:30 - loss: 0.2821 - acc: 0.935 - ETA: 5:29 - loss: 0.2782 - acc: 0.935 - ETA: 5:28 - loss: 0.2743 - acc: 0.937 - ETA: 5:27 - loss: 0.2716 - acc: 0.937 - ETA: 5:26 - loss: 0.2732 - acc: 0.938 - ETA: 5:24 - loss: 0.2844 - acc: 0.934 - ETA: 5:23 - loss: 0.2917 - acc: 0.930 - ETA: 5:22 - loss: 0.2857 - acc: 0.932 - ETA: 5:21 - loss: 0.2980 - acc: 0.930 - ETA: 5:20 - loss: 0.3350 - acc: 0.921 - ETA: 5:19 - loss: 0.3346 - acc: 0.921 - ETA: 5:18 - loss: 0.3280 - acc: 0.923 - ETA: 5:17 - loss: 0.3238 - acc: 0.925 - ETA: 5:16 - loss: 0.3227 - acc: 0.925 - ETA: 5:15 - loss: 0.3173 - acc: 0.926 - ETA: 5:14 - loss: 0.3148 - acc: 0.925 - ETA: 5:13 - loss: 0.3164 - acc: 0.923 - ETA: 5:11 - loss: 0.3198 - acc: 0.922 - ETA: 5:10 - loss: 0.3232 - acc: 0.922 - ETA: 5:09 - loss: 0.3251 - acc: 0.921 - ETA: 5:08 - loss: 0.3237 - acc: 0.922 - ETA: 5:07 - loss: 0.3258 - acc: 0.920 - ETA: 5:06 - loss: 0.3413 - acc: 0.918 - ETA: 5:05 - loss: 0.3416 - acc: 0.918 - ETA: 5:04 - loss: 0.3446 - acc: 0.919 - ETA: 5:03 - loss: 0.3451 - acc: 0.919 - ETA: 5:02 - loss: 0.3408 - acc: 0.918 - ETA: 5:01 - loss: 0.3419 - acc: 0.919 - ETA: 5:00 - loss: 0.3451 - acc: 0.917 - ETA: 4:58 - loss: 0.3427 - acc: 0.916 - ETA: 4:57 - loss: 0.3414 - acc: 0.916 - ETA: 4:56 - loss: 0.3412 - acc: 0.917 - ETA: 4:55 - loss: 0.3404 - acc: 0.917 - ETA: 4:54 - loss: 0.3442 - acc: 0.915 - ETA: 4:53 - loss: 0.3463 - acc: 0.914 - ETA: 4:52 - loss: 0.3490 - acc: 0.913 - ETA: 4:51 - loss: 0.3456 - acc: 0.915 - ETA: 4:50 - loss: 0.3423 - acc: 0.915 - ETA: 4:49 - loss: 0.3402 - acc: 0.916 - ETA: 4:48 - loss: 0.3403 - acc: 0.915 - ETA: 4:47 - loss: 0.3419 - acc: 0.915 - ETA: 4:46 - loss: 0.3382 - acc: 0.916 - ETA: 4:45 - loss: 0.3403 - acc: 0.916 - ETA: 4:44 - loss: 0.3399 - acc: 0.916 - ETA: 4:43 - loss: 0.3378 - acc: 0.916 - ETA: 4:41 - loss: 0.3354 - acc: 0.916 - ETA: 4:40 - loss: 0.3394 - acc: 0.915 - ETA: 4:39 - loss: 0.3393 - acc: 0.914 - ETA: 4:38 - loss: 0.3385 - acc: 0.914 - ETA: 4:37 - loss: 0.3379 - acc: 0.914 - ETA: 4:36 - loss: 0.3393 - acc: 0.914 - ETA: 4:35 - loss: 0.3431 - acc: 0.911 - ETA: 4:34 - loss: 0.3483 - acc: 0.909 - ETA: 4:33 - loss: 0.3504 - acc: 0.909 - ETA: 4:32 - loss: 0.3581 - acc: 0.907 - ETA: 4:31 - loss: 0.3550 - acc: 0.908 - ETA: 4:30 - loss: 0.3537 - acc: 0.908 - ETA: 4:28 - loss: 0.3535 - acc: 0.907 - ETA: 4:27 - loss: 0.3514 - acc: 0.908 - ETA: 4:26 - loss: 0.3486 - acc: 0.909 - ETA: 4:25 - loss: 0.3479 - acc: 0.909 - ETA: 4:24 - loss: 0.3506 - acc: 0.909 - ETA: 4:23 - loss: 0.3484 - acc: 0.910 - ETA: 4:22 - loss: 0.3483 - acc: 0.910 - ETA: 4:21 - loss: 0.3464 - acc: 0.910 - ETA: 4:20 - loss: 0.3433 - acc: 0.911 - ETA: 4:19 - loss: 0.3459 - acc: 0.910 - ETA: 4:17 - loss: 0.3509 - acc: 0.908 - ETA: 4:16 - loss: 0.3508 - acc: 0.908 - ETA: 4:15 - loss: 0.3487 - acc: 0.909 - ETA: 4:14 - loss: 0.3499 - acc: 0.909 - ETA: 4:13 - loss: 0.3480 - acc: 0.909 - ETA: 4:12 - loss: 0.3471 - acc: 0.909 - ETA: 4:11 - loss: 0.3457 - acc: 0.910 - ETA: 4:10 - loss: 0.3454 - acc: 0.910 - ETA: 4:09 - loss: 0.3437 - acc: 0.910 - ETA: 4:08 - loss: 0.3441 - acc: 0.909 - ETA: 4:07 - loss: 0.3417 - acc: 0.910 - ETA: 4:06 - loss: 0.3486 - acc: 0.909 - ETA: 4:04 - loss: 0.3518 - acc: 0.907 - ETA: 4:03 - loss: 0.3530 - acc: 0.907 - ETA: 4:02 - loss: 0.3521 - acc: 0.907 - ETA: 4:01 - loss: 0.3531 - acc: 0.907 - ETA: 4:00 - loss: 0.3507 - acc: 0.908 - ETA: 3:59 - loss: 0.3486 - acc: 0.908 - ETA: 3:58 - loss: 0.3524 - acc: 0.908 - ETA: 3:57 - loss: 0.3523 - acc: 0.908 - ETA: 3:56 - loss: 0.3516 - acc: 0.909 - ETA: 3:55 - loss: 0.3507 - acc: 0.909 - ETA: 3:54 - loss: 0.3481 - acc: 0.910 - ETA: 3:52 - loss: 0.3463 - acc: 0.910 - ETA: 3:51 - loss: 0.3460 - acc: 0.911 - ETA: 3:50 - loss: 0.3480 - acc: 0.910 - ETA: 3:49 - loss: 0.3479 - acc: 0.910 - ETA: 3:48 - loss: 0.3491 - acc: 0.910 - ETA: 3:47 - loss: 0.3542 - acc: 0.909 - ETA: 3:46 - loss: 0.3566 - acc: 0.909 - ETA: 3:45 - loss: 0.3568 - acc: 0.909 - ETA: 3:44 - loss: 0.3561 - acc: 0.909 - ETA: 3:43 - loss: 0.3560 - acc: 0.909 - ETA: 3:41 - loss: 0.3552 - acc: 0.909 - ETA: 3:40 - loss: 0.3571 - acc: 0.909 - ETA: 3:39 - loss: 0.3567 - acc: 0.909 - ETA: 3:38 - loss: 0.3603 - acc: 0.908 - ETA: 3:37 - loss: 0.3593 - acc: 0.909 - ETA: 3:36 - loss: 0.3579 - acc: 0.909 - ETA: 3:35 - loss: 0.3559 - acc: 0.910 - ETA: 3:34 - loss: 0.3563 - acc: 0.910 - ETA: 3:33 - loss: 0.3567 - acc: 0.910 - ETA: 3:32 - loss: 0.3560 - acc: 0.910 - ETA: 3:31 - loss: 0.3583 - acc: 0.909 - ETA: 3:29 - loss: 0.3582 - acc: 0.909 - ETA: 3:28 - loss: 0.3635 - acc: 0.908 - ETA: 3:27 - loss: 0.3623 - acc: 0.909 - ETA: 3:26 - loss: 0.3606 - acc: 0.910 - ETA: 3:25 - loss: 0.3591 - acc: 0.910 - ETA: 3:24 - loss: 0.3592 - acc: 0.910 - ETA: 3:23 - loss: 0.3616 - acc: 0.909 - ETA: 3:22 - loss: 0.3622 - acc: 0.909 - ETA: 3:21 - loss: 0.3607 - acc: 0.909 - ETA: 3:20 - loss: 0.3612 - acc: 0.909 - ETA: 3:19 - loss: 0.3599 - acc: 0.908 - ETA: 3:17 - loss: 0.3600 - acc: 0.909 - ETA: 3:16 - loss: 0.3584 - acc: 0.909 - ETA: 3:15 - loss: 0.3564 - acc: 0.910 - ETA: 3:14 - loss: 0.3580 - acc: 0.909 - ETA: 3:13 - loss: 0.3566 - acc: 0.909 - ETA: 3:12 - loss: 0.3548 - acc: 0.909 - ETA: 3:11 - loss: 0.3561 - acc: 0.909 - ETA: 3:10 - loss: 0.3597 - acc: 0.909 - ETA: 3:09 - loss: 0.3581 - acc: 0.909 - ETA: 3:08 - loss: 0.3585 - acc: 0.909 - ETA: 3:07 - loss: 0.3588 - acc: 0.909 - ETA: 3:06 - loss: 0.3588 - acc: 0.908 - ETA: 3:04 - loss: 0.3573 - acc: 0.909 - ETA: 3:03 - loss: 0.3561 - acc: 0.909 - ETA: 3:02 - loss: 0.3546 - acc: 0.910 - ETA: 3:01 - loss: 0.3549 - acc: 0.909 - ETA: 3:00 - loss: 0.3546 - acc: 0.909 - ETA: 2:59 - loss: 0.3541 - acc: 0.909 - ETA: 2:58 - loss: 0.3560 - acc: 0.909 - ETA: 2:57 - loss: 0.3566 - acc: 0.909 - ETA: 2:56 - loss: 0.3557 - acc: 0.909 - ETA: 2:55 - loss: 0.3545 - acc: 0.909 - ETA: 2:54 - loss: 0.3541 - acc: 0.909 - ETA: 2:52 - loss: 0.3535 - acc: 0.909 - ETA: 2:51 - loss: 0.3539 - acc: 0.909 - ETA: 2:50 - loss: 0.3544 - acc: 0.909 - ETA: 2:49 - loss: 0.3573 - acc: 0.909 - ETA: 2:48 - loss: 0.3604 - acc: 0.908 - ETA: 2:47 - loss: 0.3588 - acc: 0.909 - ETA: 2:46 - loss: 0.3576 - acc: 0.909 - ETA: 2:45 - loss: 0.3575 - acc: 0.909 - ETA: 2:44 - loss: 0.3589 - acc: 0.908 - ETA: 2:43 - loss: 0.3636 - acc: 0.907 - ETA: 2:42 - loss: 0.3639 - acc: 0.907 - ETA: 2:40 - loss: 0.3633 - acc: 0.907 - ETA: 2:39 - loss: 0.3638 - acc: 0.907 - ETA: 2:38 - loss: 0.3629 - acc: 0.907 - ETA: 2:37 - loss: 0.3623 - acc: 0.907 - ETA: 2:36 - loss: 0.3623 - acc: 0.907 - ETA: 2:35 - loss: 0.3627 - acc: 0.907 - ETA: 2:34 - loss: 0.3642 - acc: 0.907 - ETA: 2:33 - loss: 0.3649 - acc: 0.906 - ETA: 2:32 - loss: 0.3662 - acc: 0.906 - ETA: 2:31 - loss: 0.3660 - acc: 0.906 - ETA: 2:30 - loss: 0.3681 - acc: 0.906 - ETA: 2:29 - loss: 0.3680 - acc: 0.906 - ETA: 2:27 - loss: 0.3678 - acc: 0.906 - ETA: 2:26 - loss: 0.3703 - acc: 0.905 - ETA: 2:25 - loss: 0.3717 - acc: 0.905 - ETA: 2:24 - loss: 0.3702 - acc: 0.905 - ETA: 2:23 - loss: 0.3695 - acc: 0.905 - ETA: 2:22 - loss: 0.3704 - acc: 0.905 - ETA: 2:21 - loss: 0.3696 - acc: 0.90546680/6680 [==============================] - ETA: 2:20 - loss: 0.3684 - acc: 0.905 - ETA: 2:19 - loss: 0.3678 - acc: 0.905 - ETA: 2:18 - loss: 0.3676 - acc: 0.905 - ETA: 2:17 - loss: 0.3682 - acc: 0.905 - ETA: 2:15 - loss: 0.3700 - acc: 0.905 - ETA: 2:14 - loss: 0.3689 - acc: 0.905 - ETA: 2:13 - loss: 0.3704 - acc: 0.904 - ETA: 2:12 - loss: 0.3706 - acc: 0.904 - ETA: 2:11 - loss: 0.3708 - acc: 0.904 - ETA: 2:10 - loss: 0.3704 - acc: 0.904 - ETA: 2:09 - loss: 0.3700 - acc: 0.904 - ETA: 2:08 - loss: 0.3700 - acc: 0.903 - ETA: 2:07 - loss: 0.3701 - acc: 0.903 - ETA: 2:06 - loss: 0.3692 - acc: 0.903 - ETA: 2:05 - loss: 0.3714 - acc: 0.903 - ETA: 2:03 - loss: 0.3712 - acc: 0.903 - ETA: 2:02 - loss: 0.3708 - acc: 0.903 - ETA: 2:01 - loss: 0.3719 - acc: 0.903 - ETA: 2:00 - loss: 0.3712 - acc: 0.903 - ETA: 1:59 - loss: 0.3696 - acc: 0.904 - ETA: 1:58 - loss: 0.3683 - acc: 0.904 - ETA: 1:57 - loss: 0.3699 - acc: 0.904 - ETA: 1:56 - loss: 0.3719 - acc: 0.903 - ETA: 1:55 - loss: 0.3728 - acc: 0.903 - ETA: 1:54 - loss: 0.3723 - acc: 0.903 - ETA: 1:53 - loss: 0.3723 - acc: 0.903 - ETA: 1:52 - loss: 0.3720 - acc: 0.902 - ETA: 1:50 - loss: 0.3708 - acc: 0.903 - ETA: 1:49 - loss: 0.3703 - acc: 0.903 - ETA: 1:48 - loss: 0.3711 - acc: 0.903 - ETA: 1:47 - loss: 0.3718 - acc: 0.903 - ETA: 1:46 - loss: 0.3715 - acc: 0.903 - ETA: 1:45 - loss: 0.3717 - acc: 0.903 - ETA: 1:44 - loss: 0.3709 - acc: 0.903 - ETA: 1:43 - loss: 0.3704 - acc: 0.903 - ETA: 1:42 - loss: 0.3703 - acc: 0.903 - ETA: 1:41 - loss: 0.3692 - acc: 0.903 - ETA: 1:40 - loss: 0.3695 - acc: 0.903 - ETA: 1:38 - loss: 0.3683 - acc: 0.903 - ETA: 1:37 - loss: 0.3706 - acc: 0.902 - ETA: 1:36 - loss: 0.3703 - acc: 0.903 - ETA: 1:35 - loss: 0.3697 - acc: 0.903 - ETA: 1:34 - loss: 0.3724 - acc: 0.902 - ETA: 1:33 - loss: 0.3715 - acc: 0.903 - ETA: 1:32 - loss: 0.3741 - acc: 0.902 - ETA: 1:31 - loss: 0.3741 - acc: 0.902 - ETA: 1:30 - loss: 0.3739 - acc: 0.902 - ETA: 1:29 - loss: 0.3757 - acc: 0.902 - ETA: 1:28 - loss: 0.3747 - acc: 0.902 - ETA: 1:27 - loss: 0.3749 - acc: 0.902 - ETA: 1:25 - loss: 0.3744 - acc: 0.902 - ETA: 1:24 - loss: 0.3743 - acc: 0.902 - ETA: 1:23 - loss: 0.3749 - acc: 0.902 - ETA: 1:22 - loss: 0.3736 - acc: 0.902 - ETA: 1:21 - loss: 0.3733 - acc: 0.902 - ETA: 1:20 - loss: 0.3732 - acc: 0.903 - ETA: 1:19 - loss: 0.3750 - acc: 0.902 - ETA: 1:18 - loss: 0.3749 - acc: 0.902 - ETA: 1:17 - loss: 0.3752 - acc: 0.902 - ETA: 1:16 - loss: 0.3749 - acc: 0.902 - ETA: 1:15 - loss: 0.3748 - acc: 0.901 - ETA: 1:13 - loss: 0.3750 - acc: 0.901 - ETA: 1:12 - loss: 0.3753 - acc: 0.901 - ETA: 1:11 - loss: 0.3758 - acc: 0.901 - ETA: 1:10 - loss: 0.3755 - acc: 0.901 - ETA: 1:09 - loss: 0.3747 - acc: 0.902 - ETA: 1:08 - loss: 0.3747 - acc: 0.902 - ETA: 1:07 - loss: 0.3740 - acc: 0.902 - ETA: 1:06 - loss: 0.3735 - acc: 0.902 - ETA: 1:05 - loss: 0.3730 - acc: 0.902 - ETA: 1:04 - loss: 0.3739 - acc: 0.902 - ETA: 1:03 - loss: 0.3739 - acc: 0.901 - ETA: 1:01 - loss: 0.3734 - acc: 0.902 - ETA: 1:00 - loss: 0.3730 - acc: 0.902 - ETA: 59s - loss: 0.3719 - acc: 0.902 - ETA: 58s - loss: 0.3707 - acc: 0.90 - ETA: 57s - loss: 0.3727 - acc: 0.90 - ETA: 56s - loss: 0.3749 - acc: 0.90 - ETA: 55s - loss: 0.3746 - acc: 0.90 - ETA: 54s - loss: 0.3754 - acc: 0.90 - ETA: 53s - loss: 0.3767 - acc: 0.90 - ETA: 52s - loss: 0.3776 - acc: 0.90 - ETA: 51s - loss: 0.3769 - acc: 0.90 - ETA: 50s - loss: 0.3775 - acc: 0.90 - ETA: 48s - loss: 0.3780 - acc: 0.90 - ETA: 47s - loss: 0.3784 - acc: 0.90 - ETA: 46s - loss: 0.3782 - acc: 0.90 - ETA: 45s - loss: 0.3782 - acc: 0.90 - ETA: 44s - loss: 0.3799 - acc: 0.89 - ETA: 43s - loss: 0.3806 - acc: 0.89 - ETA: 42s - loss: 0.3810 - acc: 0.89 - ETA: 41s - loss: 0.3804 - acc: 0.89 - ETA: 40s - loss: 0.3795 - acc: 0.89 - ETA: 39s - loss: 0.3783 - acc: 0.90 - ETA: 38s - loss: 0.3777 - acc: 0.90 - ETA: 36s - loss: 0.3768 - acc: 0.90 - ETA: 35s - loss: 0.3766 - acc: 0.90 - ETA: 34s - loss: 0.3770 - acc: 0.90 - ETA: 33s - loss: 0.3772 - acc: 0.90 - ETA: 32s - loss: 0.3762 - acc: 0.90 - ETA: 31s - loss: 0.3753 - acc: 0.90 - ETA: 30s - loss: 0.3746 - acc: 0.90 - ETA: 29s - loss: 0.3744 - acc: 0.90 - ETA: 28s - loss: 0.3739 - acc: 0.90 - ETA: 27s - loss: 0.3730 - acc: 0.90 - ETA: 26s - loss: 0.3729 - acc: 0.90 - ETA: 25s - loss: 0.3734 - acc: 0.90 - ETA: 23s - loss: 0.3738 - acc: 0.90 - ETA: 22s - loss: 0.3733 - acc: 0.90 - ETA: 21s - loss: 0.3745 - acc: 0.90 - ETA: 20s - loss: 0.3748 - acc: 0.90 - ETA: 19s - loss: 0.3744 - acc: 0.90 - ETA: 18s - loss: 0.3754 - acc: 0.90 - ETA: 17s - loss: 0.3754 - acc: 0.90 - ETA: 16s - loss: 0.3749 - acc: 0.90 - ETA: 15s - loss: 0.3740 - acc: 0.90 - ETA: 14s - loss: 0.3742 - acc: 0.90 - ETA: 13s - loss: 0.3740 - acc: 0.90 - ETA: 11s - loss: 0.3733 - acc: 0.90 - ETA: 10s - loss: 0.3732 - acc: 0.90 - ETA: 9s - loss: 0.3748 - acc: 0.9008 - ETA: 8s - loss: 0.3748 - acc: 0.900 - ETA: 7s - loss: 0.3764 - acc: 0.900 - ETA: 6s - loss: 0.3774 - acc: 0.899 - ETA: 5s - loss: 0.3770 - acc: 0.899 - ETA: 4s - loss: 0.3770 - acc: 0.899 - ETA: 3s - loss: 0.3763 - acc: 0.899 - ETA: 2s - loss: 0.3758 - acc: 0.899 - ETA: 1s - loss: 0.3762 - acc: 0.899 - 375s 56ms/step - loss: 0.3776 - acc: 0.8991 - val\_loss: 6.7306 - val\_acc: 0.0946

Epoch 00010: val\_loss did not improve from 4.15157

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:} <keras.callbacks.History at 0x28305552438>
\end{Verbatim}
            
    \subsubsection{Load the Model with the Best Validation
Loss}\label{load-the-model-with-the-best-validation-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.from\PYZus{}scratch.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Test the Model}\label{test-the-model}

Try out your model on the test dataset of dog images. Ensure that your
test accuracy is greater than 1\%.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{} get index of predicted dog breed for each image in test set}
         \PY{n}{dog\PYZus{}breed\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{tensor} \PY{o+ow}{in} \PY{n}{test\PYZus{}tensors}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{dog\PYZus{}breed\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dog\PYZus{}breed\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 9.2105\%

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 4: Use a CNN to Classify Dog Breeds

To reduce training time without sacrificing accuracy, we show you how to
train a CNN using transfer learning. In the following step, you will get
a chance to use transfer learning to train your own CNN.

\subsubsection{Obtain Bottleneck
Features}\label{obtain-bottleneck-features}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DogVGG16Data.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{valid\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{test\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \subsubsection{Model Architecture}\label{model-architecture}

The model uses the the pre-trained VGG-16 model as a fixed feature
extractor, where the last convolutional output of VGG-16 is fed as input
to our model. We only add a global average pooling layer and a fully
connected layer, where the latter contains one node for each dog
category and is equipped with a softmax.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{VGG16\PYZus{}model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{train\PYZus{}VGG16}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_1 ( (None, 512)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_5 (Dense)              (None, 133)               68229     
=================================================================
Total params: 68,229
Trainable params: 68,229
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{Compile the Model}\label{compile-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmsprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Train the Model}\label{train-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.VGG16.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}VGG16}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}VGG16}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/20
6680/6680 [==============================] - ETA: 4:33 - loss: 14.1251 - acc: 0.0000e+ - ETA: 31s - loss: 14.9353 - acc: 0.0056     - ETA: 18s - loss: 14.6966 - acc: 0.021 - ETA: 12s - loss: 14.5903 - acc: 0.029 - ETA: 9s - loss: 14.4844 - acc: 0.031 - ETA: 7s - loss: 14.4711 - acc: 0.03 - ETA: 6s - loss: 14.4436 - acc: 0.03 - ETA: 5s - loss: 14.3730 - acc: 0.03 - ETA: 5s - loss: 14.1591 - acc: 0.03 - ETA: 4s - loss: 14.0060 - acc: 0.03 - ETA: 4s - loss: 13.8158 - acc: 0.04 - ETA: 3s - loss: 13.7397 - acc: 0.04 - ETA: 3s - loss: 13.6151 - acc: 0.05 - ETA: 3s - loss: 13.5311 - acc: 0.05 - ETA: 2s - loss: 13.4554 - acc: 0.05 - ETA: 2s - loss: 13.3730 - acc: 0.06 - ETA: 2s - loss: 13.3196 - acc: 0.06 - ETA: 2s - loss: 13.2599 - acc: 0.06 - ETA: 2s - loss: 13.1812 - acc: 0.06 - ETA: 2s - loss: 13.0949 - acc: 0.07 - ETA: 2s - loss: 13.0401 - acc: 0.07 - ETA: 1s - loss: 12.9693 - acc: 0.07 - ETA: 1s - loss: 12.9321 - acc: 0.08 - ETA: 1s - loss: 12.8485 - acc: 0.08 - ETA: 1s - loss: 12.8190 - acc: 0.08 - ETA: 1s - loss: 12.7671 - acc: 0.08 - ETA: 1s - loss: 12.7218 - acc: 0.08 - ETA: 1s - loss: 12.6728 - acc: 0.09 - ETA: 1s - loss: 12.6475 - acc: 0.09 - ETA: 1s - loss: 12.6025 - acc: 0.09 - ETA: 1s - loss: 12.5566 - acc: 0.09 - ETA: 0s - loss: 12.5058 - acc: 0.09 - ETA: 0s - loss: 12.4465 - acc: 0.10 - ETA: 0s - loss: 12.4145 - acc: 0.10 - ETA: 0s - loss: 12.4087 - acc: 0.10 - ETA: 0s - loss: 12.3620 - acc: 0.10 - ETA: 0s - loss: 12.3489 - acc: 0.10 - ETA: 0s - loss: 12.3284 - acc: 0.10 - ETA: 0s - loss: 12.3047 - acc: 0.11 - ETA: 0s - loss: 12.2758 - acc: 0.11 - ETA: 0s - loss: 12.2527 - acc: 0.11 - ETA: 0s - loss: 12.2389 - acc: 0.11 - ETA: 0s - loss: 12.1971 - acc: 0.11 - ETA: 0s - loss: 12.1315 - acc: 0.12 - ETA: 0s - loss: 12.0806 - acc: 0.12 - 3s 503us/step - loss: 12.0672 - acc: 0.1251 - val\_loss: 10.5289 - val\_acc: 0.2216

Epoch 00001: val\_loss improved from inf to 10.52887, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 2/20
6680/6680 [==============================] - ETA: 3s - loss: 10.2714 - acc: 0.30 - ETA: 2s - loss: 9.7330 - acc: 0.2438 - ETA: 2s - loss: 9.3964 - acc: 0.286 - ETA: 2s - loss: 9.6847 - acc: 0.273 - ETA: 2s - loss: 10.1133 - acc: 0.25 - ETA: 2s - loss: 9.9920 - acc: 0.2718 - ETA: 2s - loss: 10.0346 - acc: 0.26 - ETA: 1s - loss: 10.0055 - acc: 0.26 - ETA: 1s - loss: 10.0489 - acc: 0.26 - ETA: 1s - loss: 10.1561 - acc: 0.26 - ETA: 1s - loss: 10.1151 - acc: 0.25 - ETA: 1s - loss: 10.0351 - acc: 0.26 - ETA: 1s - loss: 10.0463 - acc: 0.26 - ETA: 1s - loss: 10.0475 - acc: 0.26 - ETA: 1s - loss: 10.0119 - acc: 0.26 - ETA: 1s - loss: 9.9737 - acc: 0.2703 - ETA: 1s - loss: 10.0379 - acc: 0.26 - ETA: 1s - loss: 10.0533 - acc: 0.26 - ETA: 1s - loss: 10.0426 - acc: 0.26 - ETA: 1s - loss: 10.0648 - acc: 0.26 - ETA: 1s - loss: 10.0825 - acc: 0.27 - ETA: 1s - loss: 10.0706 - acc: 0.27 - ETA: 1s - loss: 9.9956 - acc: 0.2753 - ETA: 1s - loss: 10.0044 - acc: 0.27 - ETA: 1s - loss: 10.0016 - acc: 0.27 - ETA: 1s - loss: 9.9806 - acc: 0.2795 - ETA: 0s - loss: 9.9591 - acc: 0.281 - ETA: 0s - loss: 9.9689 - acc: 0.281 - ETA: 0s - loss: 9.9486 - acc: 0.282 - ETA: 0s - loss: 9.9166 - acc: 0.284 - ETA: 0s - loss: 9.9450 - acc: 0.284 - ETA: 0s - loss: 9.9147 - acc: 0.286 - ETA: 0s - loss: 9.9119 - acc: 0.287 - ETA: 0s - loss: 9.9468 - acc: 0.285 - ETA: 0s - loss: 9.9555 - acc: 0.286 - ETA: 0s - loss: 9.9200 - acc: 0.288 - ETA: 0s - loss: 9.8883 - acc: 0.290 - ETA: 0s - loss: 9.9101 - acc: 0.289 - ETA: 0s - loss: 9.8803 - acc: 0.290 - ETA: 0s - loss: 9.8787 - acc: 0.291 - ETA: 0s - loss: 9.9143 - acc: 0.289 - ETA: 0s - loss: 9.8933 - acc: 0.291 - ETA: 0s - loss: 9.8651 - acc: 0.293 - ETA: 0s - loss: 9.8561 - acc: 0.293 - ETA: 0s - loss: 9.8416 - acc: 0.294 - ETA: 0s - loss: 9.8220 - acc: 0.295 - 3s 391us/step - loss: 9.8253 - acc: 0.2949 - val\_loss: 9.6646 - val\_acc: 0.3030

Epoch 00002: val\_loss improved from 10.52887 to 9.66456, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 3/20
6680/6680 [==============================] - ETA: 3s - loss: 10.1045 - acc: 0.30 - ETA: 2s - loss: 8.8872 - acc: 0.3667 - ETA: 2s - loss: 9.2384 - acc: 0.353 - ETA: 2s - loss: 9.4027 - acc: 0.347 - ETA: 2s - loss: 9.2575 - acc: 0.350 - ETA: 2s - loss: 9.2622 - acc: 0.352 - ETA: 2s - loss: 9.2303 - acc: 0.356 - ETA: 2s - loss: 9.2496 - acc: 0.351 - ETA: 2s - loss: 9.4260 - acc: 0.342 - ETA: 1s - loss: 9.3705 - acc: 0.347 - ETA: 1s - loss: 9.3680 - acc: 0.347 - ETA: 1s - loss: 9.3279 - acc: 0.350 - ETA: 1s - loss: 9.2856 - acc: 0.351 - ETA: 1s - loss: 9.2768 - acc: 0.352 - ETA: 1s - loss: 9.2451 - acc: 0.355 - ETA: 1s - loss: 9.2745 - acc: 0.352 - ETA: 1s - loss: 9.2590 - acc: 0.353 - ETA: 1s - loss: 9.2945 - acc: 0.351 - ETA: 1s - loss: 9.3373 - acc: 0.347 - ETA: 1s - loss: 9.3484 - acc: 0.347 - ETA: 1s - loss: 9.3265 - acc: 0.349 - ETA: 1s - loss: 9.2832 - acc: 0.350 - ETA: 1s - loss: 9.2598 - acc: 0.353 - ETA: 1s - loss: 9.2663 - acc: 0.353 - ETA: 1s - loss: 9.2645 - acc: 0.356 - ETA: 1s - loss: 9.2617 - acc: 0.357 - ETA: 1s - loss: 9.2365 - acc: 0.358 - ETA: 1s - loss: 9.2215 - acc: 0.358 - ETA: 1s - loss: 9.2033 - acc: 0.360 - ETA: 0s - loss: 9.1789 - acc: 0.362 - ETA: 0s - loss: 9.1723 - acc: 0.362 - ETA: 0s - loss: 9.2175 - acc: 0.359 - ETA: 0s - loss: 9.2062 - acc: 0.360 - ETA: 0s - loss: 9.1973 - acc: 0.360 - ETA: 0s - loss: 9.1917 - acc: 0.360 - ETA: 0s - loss: 9.1881 - acc: 0.360 - ETA: 0s - loss: 9.1900 - acc: 0.359 - ETA: 0s - loss: 9.2086 - acc: 0.358 - ETA: 0s - loss: 9.1941 - acc: 0.359 - ETA: 0s - loss: 9.1999 - acc: 0.359 - ETA: 0s - loss: 9.1842 - acc: 0.359 - ETA: 0s - loss: 9.2068 - acc: 0.358 - ETA: 0s - loss: 9.2045 - acc: 0.358 - ETA: 0s - loss: 9.2056 - acc: 0.358 - ETA: 0s - loss: 9.2280 - acc: 0.357 - ETA: 0s - loss: 9.2238 - acc: 0.358 - ETA: 0s - loss: 9.2140 - acc: 0.359 - ETA: 0s - loss: 9.2274 - acc: 0.358 - ETA: 0s - loss: 9.2358 - acc: 0.358 - 3s 409us/step - loss: 9.2242 - acc: 0.3597 - val\_loss: 9.4389 - val\_acc: 0.3293

Epoch 00003: val\_loss improved from 9.66456 to 9.43887, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 4/20
6680/6680 [==============================] - ETA: 2s - loss: 9.0881 - acc: 0.400 - ETA: 2s - loss: 8.3478 - acc: 0.433 - ETA: 2s - loss: 8.7136 - acc: 0.429 - ETA: 2s - loss: 8.5889 - acc: 0.440 - ETA: 2s - loss: 8.7987 - acc: 0.423 - ETA: 1s - loss: 8.9032 - acc: 0.415 - ETA: 1s - loss: 8.8779 - acc: 0.415 - ETA: 1s - loss: 8.9306 - acc: 0.412 - ETA: 1s - loss: 8.9891 - acc: 0.407 - ETA: 1s - loss: 9.1598 - acc: 0.395 - ETA: 1s - loss: 9.1378 - acc: 0.394 - ETA: 1s - loss: 9.0671 - acc: 0.397 - ETA: 1s - loss: 9.0664 - acc: 0.395 - ETA: 1s - loss: 9.0683 - acc: 0.395 - ETA: 1s - loss: 9.0149 - acc: 0.397 - ETA: 1s - loss: 9.0491 - acc: 0.395 - ETA: 1s - loss: 9.0189 - acc: 0.396 - ETA: 1s - loss: 8.9859 - acc: 0.399 - ETA: 1s - loss: 8.9658 - acc: 0.398 - ETA: 1s - loss: 8.9322 - acc: 0.400 - ETA: 1s - loss: 8.9782 - acc: 0.397 - ETA: 1s - loss: 8.9595 - acc: 0.397 - ETA: 1s - loss: 8.9663 - acc: 0.397 - ETA: 1s - loss: 8.9655 - acc: 0.397 - ETA: 1s - loss: 8.9313 - acc: 0.399 - ETA: 0s - loss: 8.9225 - acc: 0.399 - ETA: 0s - loss: 8.9316 - acc: 0.398 - ETA: 0s - loss: 8.9146 - acc: 0.399 - ETA: 0s - loss: 8.9452 - acc: 0.398 - ETA: 0s - loss: 8.9246 - acc: 0.399 - ETA: 0s - loss: 8.9001 - acc: 0.400 - ETA: 0s - loss: 8.9173 - acc: 0.400 - ETA: 0s - loss: 8.9314 - acc: 0.398 - ETA: 0s - loss: 8.9160 - acc: 0.398 - ETA: 0s - loss: 8.8902 - acc: 0.399 - ETA: 0s - loss: 8.8930 - acc: 0.399 - ETA: 0s - loss: 8.8825 - acc: 0.400 - ETA: 0s - loss: 8.8862 - acc: 0.399 - ETA: 0s - loss: 8.9060 - acc: 0.398 - ETA: 0s - loss: 8.8788 - acc: 0.398 - ETA: 0s - loss: 8.8691 - acc: 0.399 - ETA: 0s - loss: 8.8733 - acc: 0.398 - ETA: 0s - loss: 8.8990 - acc: 0.396 - ETA: 0s - loss: 8.8906 - acc: 0.396 - 2s 365us/step - loss: 8.9028 - acc: 0.3954 - val\_loss: 9.1803 - val\_acc: 0.3509

Epoch 00004: val\_loss improved from 9.43887 to 9.18029, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 5/20
6680/6680 [==============================] - ETA: 1s - loss: 11.3443 - acc: 0.30 - ETA: 2s - loss: 8.8880 - acc: 0.4167 - ETA: 2s - loss: 8.7571 - acc: 0.431 - ETA: 2s - loss: 8.5636 - acc: 0.434 - ETA: 2s - loss: 8.5923 - acc: 0.427 - ETA: 1s - loss: 8.7272 - acc: 0.416 - ETA: 1s - loss: 8.6170 - acc: 0.421 - ETA: 1s - loss: 8.5302 - acc: 0.422 - ETA: 1s - loss: 8.5325 - acc: 0.421 - ETA: 1s - loss: 8.5162 - acc: 0.425 - ETA: 1s - loss: 8.5159 - acc: 0.426 - ETA: 1s - loss: 8.4965 - acc: 0.428 - ETA: 1s - loss: 8.4513 - acc: 0.432 - ETA: 1s - loss: 8.4595 - acc: 0.431 - ETA: 1s - loss: 8.4962 - acc: 0.428 - ETA: 1s - loss: 8.4971 - acc: 0.428 - ETA: 1s - loss: 8.5167 - acc: 0.427 - ETA: 1s - loss: 8.5176 - acc: 0.428 - ETA: 1s - loss: 8.5167 - acc: 0.428 - ETA: 0s - loss: 8.5001 - acc: 0.429 - ETA: 0s - loss: 8.4599 - acc: 0.430 - ETA: 0s - loss: 8.4610 - acc: 0.429 - ETA: 0s - loss: 8.5167 - acc: 0.427 - ETA: 0s - loss: 8.5450 - acc: 0.426 - ETA: 0s - loss: 8.5511 - acc: 0.427 - ETA: 0s - loss: 8.5973 - acc: 0.424 - ETA: 0s - loss: 8.6104 - acc: 0.423 - ETA: 0s - loss: 8.5948 - acc: 0.424 - ETA: 0s - loss: 8.5759 - acc: 0.426 - ETA: 0s - loss: 8.6007 - acc: 0.424 - ETA: 0s - loss: 8.6319 - acc: 0.423 - ETA: 0s - loss: 8.6535 - acc: 0.421 - ETA: 0s - loss: 8.6486 - acc: 0.422 - ETA: 0s - loss: 8.6704 - acc: 0.420 - ETA: 0s - loss: 8.6768 - acc: 0.420 - ETA: 0s - loss: 8.6751 - acc: 0.420 - ETA: 0s - loss: 8.6871 - acc: 0.420 - 2s 311us/step - loss: 8.6607 - acc: 0.4222 - val\_loss: 9.0046 - val\_acc: 0.3689

Epoch 00005: val\_loss improved from 9.18029 to 9.00463, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 6/20
6680/6680 [==============================] - ETA: 3s - loss: 8.8806 - acc: 0.450 - ETA: 2s - loss: 9.4688 - acc: 0.377 - ETA: 2s - loss: 9.0918 - acc: 0.408 - ETA: 2s - loss: 8.9150 - acc: 0.426 - ETA: 1s - loss: 8.9309 - acc: 0.422 - ETA: 1s - loss: 8.7958 - acc: 0.430 - ETA: 1s - loss: 8.5789 - acc: 0.445 - ETA: 1s - loss: 8.6221 - acc: 0.443 - ETA: 1s - loss: 8.5497 - acc: 0.446 - ETA: 1s - loss: 8.5706 - acc: 0.445 - ETA: 1s - loss: 8.6610 - acc: 0.440 - ETA: 1s - loss: 8.6580 - acc: 0.440 - ETA: 1s - loss: 8.6212 - acc: 0.441 - ETA: 1s - loss: 8.6606 - acc: 0.438 - ETA: 1s - loss: 8.6204 - acc: 0.440 - ETA: 1s - loss: 8.6363 - acc: 0.438 - ETA: 1s - loss: 8.6044 - acc: 0.439 - ETA: 1s - loss: 8.5981 - acc: 0.438 - ETA: 1s - loss: 8.6080 - acc: 0.437 - ETA: 0s - loss: 8.5909 - acc: 0.437 - ETA: 0s - loss: 8.6185 - acc: 0.435 - ETA: 0s - loss: 8.6020 - acc: 0.436 - ETA: 0s - loss: 8.6351 - acc: 0.434 - ETA: 0s - loss: 8.6212 - acc: 0.434 - ETA: 0s - loss: 8.6456 - acc: 0.432 - ETA: 0s - loss: 8.6719 - acc: 0.431 - ETA: 0s - loss: 8.6933 - acc: 0.430 - ETA: 0s - loss: 8.7380 - acc: 0.428 - ETA: 0s - loss: 8.7147 - acc: 0.429 - ETA: 0s - loss: 8.7398 - acc: 0.428 - ETA: 0s - loss: 8.7525 - acc: 0.426 - ETA: 0s - loss: 8.7222 - acc: 0.428 - ETA: 0s - loss: 8.6793 - acc: 0.431 - ETA: 0s - loss: 8.6453 - acc: 0.433 - ETA: 0s - loss: 8.6334 - acc: 0.433 - ETA: 0s - loss: 8.6012 - acc: 0.436 - ETA: 0s - loss: 8.5894 - acc: 0.436 - ETA: 0s - loss: 8.5787 - acc: 0.437 - 2s 310us/step - loss: 8.5757 - acc: 0.4379 - val\_loss: 8.9263 - val\_acc: 0.3784

Epoch 00006: val\_loss improved from 9.00463 to 8.92626, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 7/20
6680/6680 [==============================] - ETA: 2s - loss: 8.9296 - acc: 0.450 - ETA: 2s - loss: 7.3628 - acc: 0.530 - ETA: 2s - loss: 7.8756 - acc: 0.491 - ETA: 1s - loss: 7.7774 - acc: 0.494 - ETA: 1s - loss: 7.9659 - acc: 0.480 - ETA: 1s - loss: 7.8772 - acc: 0.486 - ETA: 1s - loss: 7.9856 - acc: 0.480 - ETA: 1s - loss: 8.1942 - acc: 0.466 - ETA: 1s - loss: 8.2574 - acc: 0.462 - ETA: 1s - loss: 8.3179 - acc: 0.460 - ETA: 1s - loss: 8.2705 - acc: 0.462 - ETA: 1s - loss: 8.3677 - acc: 0.455 - ETA: 1s - loss: 8.4004 - acc: 0.453 - ETA: 1s - loss: 8.4147 - acc: 0.453 - ETA: 1s - loss: 8.4146 - acc: 0.453 - ETA: 1s - loss: 8.3860 - acc: 0.455 - ETA: 1s - loss: 8.3737 - acc: 0.456 - ETA: 0s - loss: 8.4013 - acc: 0.454 - ETA: 0s - loss: 8.3963 - acc: 0.454 - ETA: 0s - loss: 8.3510 - acc: 0.456 - ETA: 0s - loss: 8.3727 - acc: 0.454 - ETA: 0s - loss: 8.4254 - acc: 0.450 - ETA: 0s - loss: 8.4536 - acc: 0.448 - ETA: 0s - loss: 8.4516 - acc: 0.448 - ETA: 0s - loss: 8.4935 - acc: 0.445 - ETA: 0s - loss: 8.5240 - acc: 0.443 - ETA: 0s - loss: 8.4974 - acc: 0.445 - ETA: 0s - loss: 8.4848 - acc: 0.445 - ETA: 0s - loss: 8.4720 - acc: 0.446 - ETA: 0s - loss: 8.4809 - acc: 0.445 - ETA: 0s - loss: 8.4644 - acc: 0.446 - ETA: 0s - loss: 8.4453 - acc: 0.447 - ETA: 0s - loss: 8.4572 - acc: 0.446 - ETA: 0s - loss: 8.4479 - acc: 0.447 - ETA: 0s - loss: 8.4356 - acc: 0.447 - 2s 296us/step - loss: 8.4282 - acc: 0.4472 - val\_loss: 8.8091 - val\_acc: 0.3677

Epoch 00007: val\_loss improved from 8.92626 to 8.80910, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 8/20
6680/6680 [==============================] - ETA: 8s - loss: 7.2854 - acc: 0.550 - ETA: 2s - loss: 8.0575 - acc: 0.475 - ETA: 2s - loss: 7.8150 - acc: 0.481 - ETA: 1s - loss: 8.0299 - acc: 0.462 - ETA: 1s - loss: 7.9905 - acc: 0.467 - ETA: 1s - loss: 8.2200 - acc: 0.454 - ETA: 1s - loss: 8.0542 - acc: 0.464 - ETA: 1s - loss: 8.1171 - acc: 0.459 - ETA: 1s - loss: 8.1035 - acc: 0.462 - ETA: 1s - loss: 8.1057 - acc: 0.464 - ETA: 1s - loss: 8.1481 - acc: 0.459 - ETA: 1s - loss: 8.0969 - acc: 0.463 - ETA: 1s - loss: 8.0990 - acc: 0.461 - ETA: 1s - loss: 8.0831 - acc: 0.463 - ETA: 1s - loss: 8.0037 - acc: 0.467 - ETA: 1s - loss: 7.9254 - acc: 0.472 - ETA: 1s - loss: 7.9350 - acc: 0.470 - ETA: 0s - loss: 7.9580 - acc: 0.469 - ETA: 0s - loss: 8.0791 - acc: 0.462 - ETA: 0s - loss: 8.0281 - acc: 0.464 - ETA: 0s - loss: 8.0795 - acc: 0.461 - ETA: 0s - loss: 8.1119 - acc: 0.459 - ETA: 0s - loss: 8.1497 - acc: 0.457 - ETA: 0s - loss: 8.1444 - acc: 0.458 - ETA: 0s - loss: 8.1624 - acc: 0.457 - ETA: 0s - loss: 8.1791 - acc: 0.455 - ETA: 0s - loss: 8.1483 - acc: 0.457 - ETA: 0s - loss: 8.1523 - acc: 0.457 - ETA: 0s - loss: 8.1608 - acc: 0.456 - ETA: 0s - loss: 8.1534 - acc: 0.457 - ETA: 0s - loss: 8.1345 - acc: 0.458 - ETA: 0s - loss: 8.0985 - acc: 0.460 - ETA: 0s - loss: 8.0648 - acc: 0.462 - ETA: 0s - loss: 8.0366 - acc: 0.464 - ETA: 0s - loss: 8.0304 - acc: 0.464 - ETA: 0s - loss: 8.0579 - acc: 0.463 - 2s 299us/step - loss: 8.0490 - acc: 0.4641 - val\_loss: 8.5064 - val\_acc: 0.3868

Epoch 00008: val\_loss improved from 8.80910 to 8.50636, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 9/20
6680/6680 [==============================] - ETA: 2s - loss: 8.0652 - acc: 0.500 - ETA: 1s - loss: 8.4515 - acc: 0.454 - ETA: 1s - loss: 8.0838 - acc: 0.484 - ETA: 1s - loss: 7.9306 - acc: 0.489 - ETA: 1s - loss: 7.8750 - acc: 0.488 - ETA: 1s - loss: 7.8320 - acc: 0.491 - ETA: 1s - loss: 7.7400 - acc: 0.497 - ETA: 1s - loss: 7.7496 - acc: 0.496 - ETA: 1s - loss: 7.7046 - acc: 0.497 - ETA: 1s - loss: 7.6928 - acc: 0.499 - ETA: 1s - loss: 7.7055 - acc: 0.499 - ETA: 1s - loss: 7.6886 - acc: 0.499 - ETA: 1s - loss: 7.7506 - acc: 0.494 - ETA: 1s - loss: 7.7658 - acc: 0.491 - ETA: 1s - loss: 7.6833 - acc: 0.495 - ETA: 1s - loss: 7.7381 - acc: 0.491 - ETA: 0s - loss: 7.8253 - acc: 0.485 - ETA: 0s - loss: 7.7805 - acc: 0.488 - ETA: 0s - loss: 7.8163 - acc: 0.486 - ETA: 0s - loss: 7.8116 - acc: 0.487 - ETA: 0s - loss: 7.8132 - acc: 0.486 - ETA: 0s - loss: 7.8430 - acc: 0.485 - ETA: 0s - loss: 7.7914 - acc: 0.487 - ETA: 0s - loss: 7.7759 - acc: 0.488 - ETA: 0s - loss: 7.7617 - acc: 0.488 - ETA: 0s - loss: 7.7484 - acc: 0.489 - ETA: 0s - loss: 7.7781 - acc: 0.487 - ETA: 0s - loss: 7.7750 - acc: 0.487 - ETA: 0s - loss: 7.7653 - acc: 0.487 - ETA: 0s - loss: 7.7924 - acc: 0.485 - ETA: 0s - loss: 7.7971 - acc: 0.485 - ETA: 0s - loss: 7.7807 - acc: 0.486 - ETA: 0s - loss: 7.7665 - acc: 0.487 - ETA: 0s - loss: 7.7743 - acc: 0.487 - ETA: 0s - loss: 7.7663 - acc: 0.488 - ETA: 0s - loss: 7.7714 - acc: 0.487 - 2s 302us/step - loss: 7.7555 - acc: 0.4888 - val\_loss: 8.2822 - val\_acc: 0.3976

Epoch 00009: val\_loss improved from 8.50636 to 8.28224, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 10/20
6680/6680 [==============================] - ETA: 2s - loss: 7.2536 - acc: 0.550 - ETA: 1s - loss: 7.2145 - acc: 0.540 - ETA: 1s - loss: 7.8098 - acc: 0.507 - ETA: 1s - loss: 7.5908 - acc: 0.514 - ETA: 1s - loss: 7.4335 - acc: 0.526 - ETA: 1s - loss: 7.7894 - acc: 0.503 - ETA: 1s - loss: 7.5253 - acc: 0.513 - ETA: 1s - loss: 7.3811 - acc: 0.521 - ETA: 1s - loss: 7.4737 - acc: 0.516 - ETA: 1s - loss: 7.4499 - acc: 0.516 - ETA: 1s - loss: 7.4318 - acc: 0.517 - ETA: 1s - loss: 7.3664 - acc: 0.521 - ETA: 1s - loss: 7.3897 - acc: 0.520 - ETA: 1s - loss: 7.4053 - acc: 0.518 - ETA: 0s - loss: 7.4364 - acc: 0.516 - ETA: 0s - loss: 7.4839 - acc: 0.513 - ETA: 0s - loss: 7.5074 - acc: 0.511 - ETA: 0s - loss: 7.5310 - acc: 0.510 - ETA: 0s - loss: 7.5677 - acc: 0.508 - ETA: 0s - loss: 7.5794 - acc: 0.506 - ETA: 0s - loss: 7.5842 - acc: 0.506 - ETA: 0s - loss: 7.5535 - acc: 0.507 - ETA: 0s - loss: 7.5627 - acc: 0.506 - ETA: 0s - loss: 7.5858 - acc: 0.503 - ETA: 0s - loss: 7.6092 - acc: 0.501 - ETA: 0s - loss: 7.6194 - acc: 0.500 - ETA: 0s - loss: 7.6267 - acc: 0.500 - ETA: 0s - loss: 7.6153 - acc: 0.500 - ETA: 0s - loss: 7.6204 - acc: 0.499 - ETA: 0s - loss: 7.6177 - acc: 0.500 - ETA: 0s - loss: 7.6263 - acc: 0.499 - ETA: 0s - loss: 7.6138 - acc: 0.500 - ETA: 0s - loss: 7.6154 - acc: 0.500 - 2s 285us/step - loss: 7.5938 - acc: 0.5010 - val\_loss: 8.1237 - val\_acc: 0.3976

Epoch 00010: val\_loss improved from 8.28224 to 8.12367, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 11/20
6680/6680 [==============================] - ETA: 2s - loss: 7.5175 - acc: 0.450 - ETA: 1s - loss: 7.0180 - acc: 0.540 - ETA: 1s - loss: 6.9458 - acc: 0.554 - ETA: 1s - loss: 6.8253 - acc: 0.554 - ETA: 1s - loss: 7.0483 - acc: 0.540 - ETA: 1s - loss: 7.3504 - acc: 0.521 - ETA: 1s - loss: 7.3870 - acc: 0.516 - ETA: 1s - loss: 7.3362 - acc: 0.519 - ETA: 1s - loss: 7.3329 - acc: 0.520 - ETA: 1s - loss: 7.3282 - acc: 0.521 - ETA: 1s - loss: 7.3024 - acc: 0.523 - ETA: 1s - loss: 7.3810 - acc: 0.518 - ETA: 1s - loss: 7.3555 - acc: 0.520 - ETA: 1s - loss: 7.3272 - acc: 0.521 - ETA: 1s - loss: 7.2783 - acc: 0.524 - ETA: 1s - loss: 7.3256 - acc: 0.520 - ETA: 0s - loss: 7.2673 - acc: 0.525 - ETA: 0s - loss: 7.2838 - acc: 0.523 - ETA: 0s - loss: 7.2324 - acc: 0.526 - ETA: 0s - loss: 7.2019 - acc: 0.528 - ETA: 0s - loss: 7.2304 - acc: 0.526 - ETA: 0s - loss: 7.2486 - acc: 0.524 - ETA: 0s - loss: 7.2346 - acc: 0.525 - ETA: 0s - loss: 7.2417 - acc: 0.525 - ETA: 0s - loss: 7.2648 - acc: 0.522 - ETA: 0s - loss: 7.3099 - acc: 0.519 - ETA: 0s - loss: 7.2955 - acc: 0.520 - ETA: 0s - loss: 7.3040 - acc: 0.519 - ETA: 0s - loss: 7.3280 - acc: 0.517 - ETA: 0s - loss: 7.3414 - acc: 0.516 - ETA: 0s - loss: 7.3644 - acc: 0.515 - ETA: 0s - loss: 7.3656 - acc: 0.514 - ETA: 0s - loss: 7.3696 - acc: 0.514 - ETA: 0s - loss: 7.3757 - acc: 0.514 - 2s 288us/step - loss: 7.3855 - acc: 0.5133 - val\_loss: 8.0429 - val\_acc: 0.3940

Epoch 00011: val\_loss improved from 8.12367 to 8.04295, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 12/20
6680/6680 [==============================] - ETA: 2s - loss: 5.6414 - acc: 0.650 - ETA: 1s - loss: 8.1023 - acc: 0.473 - ETA: 1s - loss: 7.8084 - acc: 0.486 - ETA: 1s - loss: 7.6503 - acc: 0.500 - ETA: 1s - loss: 7.6117 - acc: 0.498 - ETA: 1s - loss: 7.5537 - acc: 0.504 - ETA: 1s - loss: 7.4543 - acc: 0.510 - ETA: 1s - loss: 7.5246 - acc: 0.507 - ETA: 1s - loss: 7.5614 - acc: 0.503 - ETA: 1s - loss: 7.6054 - acc: 0.502 - ETA: 1s - loss: 7.6701 - acc: 0.498 - ETA: 1s - loss: 7.5593 - acc: 0.505 - ETA: 1s - loss: 7.5519 - acc: 0.506 - ETA: 1s - loss: 7.5333 - acc: 0.508 - ETA: 1s - loss: 7.4177 - acc: 0.516 - ETA: 1s - loss: 7.4395 - acc: 0.515 - ETA: 0s - loss: 7.4067 - acc: 0.517 - ETA: 0s - loss: 7.3957 - acc: 0.516 - ETA: 0s - loss: 7.3624 - acc: 0.519 - ETA: 0s - loss: 7.3172 - acc: 0.522 - ETA: 0s - loss: 7.3716 - acc: 0.518 - ETA: 0s - loss: 7.3390 - acc: 0.520 - ETA: 0s - loss: 7.3488 - acc: 0.519 - ETA: 0s - loss: 7.3473 - acc: 0.518 - ETA: 0s - loss: 7.3347 - acc: 0.519 - ETA: 0s - loss: 7.2823 - acc: 0.522 - ETA: 0s - loss: 7.2251 - acc: 0.525 - ETA: 0s - loss: 7.2070 - acc: 0.527 - ETA: 0s - loss: 7.1832 - acc: 0.528 - ETA: 0s - loss: 7.2087 - acc: 0.527 - ETA: 0s - loss: 7.1905 - acc: 0.528 - ETA: 0s - loss: 7.1596 - acc: 0.529 - ETA: 0s - loss: 7.1390 - acc: 0.531 - ETA: 0s - loss: 7.1319 - acc: 0.531 - ETA: 0s - loss: 7.1326 - acc: 0.530 - 2s 294us/step - loss: 7.1402 - acc: 0.5301 - val\_loss: 7.8647 - val\_acc: 0.4156

Epoch 00012: val\_loss improved from 8.04295 to 7.86466, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 13/20
6680/6680 [==============================] - ETA: 2s - loss: 5.6483 - acc: 0.650 - ETA: 1s - loss: 6.4178 - acc: 0.581 - ETA: 1s - loss: 7.1810 - acc: 0.540 - ETA: 1s - loss: 6.8417 - acc: 0.559 - ETA: 1s - loss: 6.9963 - acc: 0.551 - ETA: 1s - loss: 6.9632 - acc: 0.551 - ETA: 1s - loss: 7.0921 - acc: 0.544 - ETA: 1s - loss: 7.1663 - acc: 0.537 - ETA: 1s - loss: 7.1435 - acc: 0.538 - ETA: 1s - loss: 7.0005 - acc: 0.548 - ETA: 1s - loss: 6.9108 - acc: 0.554 - ETA: 1s - loss: 6.8977 - acc: 0.556 - ETA: 1s - loss: 6.8719 - acc: 0.557 - ETA: 1s - loss: 6.9241 - acc: 0.554 - ETA: 1s - loss: 6.8909 - acc: 0.555 - ETA: 1s - loss: 6.9802 - acc: 0.549 - ETA: 0s - loss: 6.9206 - acc: 0.553 - ETA: 0s - loss: 6.8983 - acc: 0.555 - ETA: 0s - loss: 6.9153 - acc: 0.554 - ETA: 0s - loss: 6.9597 - acc: 0.552 - ETA: 0s - loss: 6.9796 - acc: 0.550 - ETA: 0s - loss: 6.9772 - acc: 0.550 - ETA: 0s - loss: 6.9549 - acc: 0.550 - ETA: 0s - loss: 6.9532 - acc: 0.550 - ETA: 0s - loss: 6.9169 - acc: 0.553 - ETA: 0s - loss: 6.9338 - acc: 0.552 - ETA: 0s - loss: 6.9359 - acc: 0.552 - ETA: 0s - loss: 6.9497 - acc: 0.551 - ETA: 0s - loss: 6.9958 - acc: 0.549 - ETA: 0s - loss: 7.0122 - acc: 0.548 - ETA: 0s - loss: 7.0109 - acc: 0.547 - ETA: 0s - loss: 6.9967 - acc: 0.548 - ETA: 0s - loss: 6.9651 - acc: 0.550 - ETA: 0s - loss: 6.9642 - acc: 0.550 - ETA: 0s - loss: 6.9815 - acc: 0.548 - 2s 292us/step - loss: 6.9603 - acc: 0.5497 - val\_loss: 7.7340 - val\_acc: 0.4359

Epoch 00013: val\_loss improved from 7.86466 to 7.73403, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 14/20
6680/6680 [==============================] - ETA: 2s - loss: 5.6652 - acc: 0.650 - ETA: 1s - loss: 7.5556 - acc: 0.522 - ETA: 1s - loss: 7.5784 - acc: 0.519 - ETA: 1s - loss: 7.3217 - acc: 0.533 - ETA: 1s - loss: 7.2257 - acc: 0.541 - ETA: 1s - loss: 7.2711 - acc: 0.538 - ETA: 1s - loss: 7.3256 - acc: 0.535 - ETA: 1s - loss: 7.2258 - acc: 0.541 - ETA: 1s - loss: 7.2684 - acc: 0.538 - ETA: 1s - loss: 7.2026 - acc: 0.542 - ETA: 1s - loss: 7.1266 - acc: 0.545 - ETA: 1s - loss: 7.1423 - acc: 0.544 - ETA: 1s - loss: 7.0669 - acc: 0.548 - ETA: 1s - loss: 7.0349 - acc: 0.550 - ETA: 1s - loss: 6.9977 - acc: 0.551 - ETA: 1s - loss: 7.0218 - acc: 0.550 - ETA: 1s - loss: 6.9924 - acc: 0.553 - ETA: 0s - loss: 7.0614 - acc: 0.548 - ETA: 0s - loss: 7.0345 - acc: 0.549 - ETA: 0s - loss: 7.0239 - acc: 0.550 - ETA: 0s - loss: 7.0105 - acc: 0.551 - ETA: 0s - loss: 7.0189 - acc: 0.550 - ETA: 0s - loss: 7.0018 - acc: 0.551 - ETA: 0s - loss: 7.0395 - acc: 0.549 - ETA: 0s - loss: 7.0346 - acc: 0.549 - ETA: 0s - loss: 7.0225 - acc: 0.550 - ETA: 0s - loss: 6.9693 - acc: 0.554 - ETA: 0s - loss: 6.9766 - acc: 0.553 - ETA: 0s - loss: 6.9733 - acc: 0.554 - ETA: 0s - loss: 6.9161 - acc: 0.557 - ETA: 0s - loss: 6.9027 - acc: 0.558 - ETA: 0s - loss: 6.9322 - acc: 0.556 - ETA: 0s - loss: 6.9359 - acc: 0.556 - ETA: 0s - loss: 6.9253 - acc: 0.557 - ETA: 0s - loss: 6.9174 - acc: 0.557 - 2s 294us/step - loss: 6.9084 - acc: 0.5585 - val\_loss: 7.6419 - val\_acc: 0.4383

Epoch 00014: val\_loss improved from 7.73403 to 7.64190, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 15/20
6680/6680 [==============================] - ETA: 2s - loss: 3.2274 - acc: 0.800 - ETA: 1s - loss: 7.1515 - acc: 0.550 - ETA: 1s - loss: 7.1705 - acc: 0.545 - ETA: 1s - loss: 7.0953 - acc: 0.548 - ETA: 1s - loss: 7.2223 - acc: 0.540 - ETA: 1s - loss: 7.2443 - acc: 0.539 - ETA: 1s - loss: 7.2374 - acc: 0.540 - ETA: 1s - loss: 7.1238 - acc: 0.547 - ETA: 1s - loss: 7.0716 - acc: 0.548 - ETA: 1s - loss: 7.0349 - acc: 0.549 - ETA: 1s - loss: 6.9863 - acc: 0.553 - ETA: 1s - loss: 6.9127 - acc: 0.558 - ETA: 1s - loss: 6.9715 - acc: 0.553 - ETA: 1s - loss: 6.9119 - acc: 0.557 - ETA: 1s - loss: 6.9113 - acc: 0.556 - ETA: 1s - loss: 6.8533 - acc: 0.560 - ETA: 0s - loss: 6.8318 - acc: 0.562 - ETA: 0s - loss: 6.8254 - acc: 0.562 - ETA: 0s - loss: 6.8332 - acc: 0.562 - ETA: 0s - loss: 6.8025 - acc: 0.564 - ETA: 0s - loss: 6.7978 - acc: 0.564 - ETA: 0s - loss: 6.7952 - acc: 0.564 - ETA: 0s - loss: 6.7919 - acc: 0.564 - ETA: 0s - loss: 6.8013 - acc: 0.564 - ETA: 0s - loss: 6.7859 - acc: 0.564 - ETA: 0s - loss: 6.7774 - acc: 0.565 - ETA: 0s - loss: 6.7695 - acc: 0.566 - ETA: 0s - loss: 6.7732 - acc: 0.566 - ETA: 0s - loss: 6.7574 - acc: 0.567 - ETA: 0s - loss: 6.7765 - acc: 0.565 - ETA: 0s - loss: 6.8003 - acc: 0.564 - ETA: 0s - loss: 6.8060 - acc: 0.563 - ETA: 0s - loss: 6.8040 - acc: 0.563 - ETA: 0s - loss: 6.8036 - acc: 0.564 - ETA: 0s - loss: 6.8238 - acc: 0.562 - 2s 289us/step - loss: 6.8251 - acc: 0.5627 - val\_loss: 7.6110 - val\_acc: 0.4479

Epoch 00015: val\_loss improved from 7.64190 to 7.61098, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 16/20
6680/6680 [==============================] - ETA: 2s - loss: 6.4501 - acc: 0.600 - ETA: 1s - loss: 7.1054 - acc: 0.540 - ETA: 1s - loss: 7.0193 - acc: 0.552 - ETA: 1s - loss: 6.5761 - acc: 0.581 - ETA: 1s - loss: 6.5655 - acc: 0.584 - ETA: 1s - loss: 6.7556 - acc: 0.574 - ETA: 1s - loss: 6.6083 - acc: 0.584 - ETA: 1s - loss: 6.5361 - acc: 0.587 - ETA: 1s - loss: 6.6089 - acc: 0.584 - ETA: 1s - loss: 6.6625 - acc: 0.580 - ETA: 1s - loss: 6.6586 - acc: 0.578 - ETA: 1s - loss: 6.7060 - acc: 0.576 - ETA: 1s - loss: 6.6847 - acc: 0.578 - ETA: 1s - loss: 6.6987 - acc: 0.577 - ETA: 1s - loss: 6.7061 - acc: 0.576 - ETA: 0s - loss: 6.7763 - acc: 0.573 - ETA: 0s - loss: 6.7688 - acc: 0.572 - ETA: 0s - loss: 6.7598 - acc: 0.574 - ETA: 0s - loss: 6.7559 - acc: 0.574 - ETA: 0s - loss: 6.7750 - acc: 0.573 - ETA: 0s - loss: 6.7835 - acc: 0.573 - ETA: 0s - loss: 6.7921 - acc: 0.572 - ETA: 0s - loss: 6.8230 - acc: 0.569 - ETA: 0s - loss: 6.8507 - acc: 0.568 - ETA: 0s - loss: 6.8074 - acc: 0.570 - ETA: 0s - loss: 6.7752 - acc: 0.572 - ETA: 0s - loss: 6.7974 - acc: 0.571 - ETA: 0s - loss: 6.8045 - acc: 0.570 - ETA: 0s - loss: 6.7627 - acc: 0.573 - ETA: 0s - loss: 6.7513 - acc: 0.574 - ETA: 0s - loss: 6.7741 - acc: 0.572 - ETA: 0s - loss: 6.7562 - acc: 0.573 - ETA: 0s - loss: 6.7669 - acc: 0.572 - ETA: 0s - loss: 6.7647 - acc: 0.572 - ETA: 0s - loss: 6.7758 - acc: 0.572 - 2s 292us/step - loss: 6.7898 - acc: 0.5713 - val\_loss: 7.5086 - val\_acc: 0.4623

Epoch 00016: val\_loss improved from 7.61098 to 7.50857, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 17/20
6680/6680 [==============================] - ETA: 2s - loss: 7.2532 - acc: 0.550 - ETA: 1s - loss: 6.3341 - acc: 0.603 - ETA: 1s - loss: 6.3068 - acc: 0.602 - ETA: 1s - loss: 6.6319 - acc: 0.582 - ETA: 1s - loss: 6.6411 - acc: 0.581 - ETA: 1s - loss: 6.6364 - acc: 0.582 - ETA: 1s - loss: 6.6382 - acc: 0.578 - ETA: 1s - loss: 6.6164 - acc: 0.578 - ETA: 1s - loss: 6.6268 - acc: 0.578 - ETA: 1s - loss: 6.6475 - acc: 0.577 - ETA: 1s - loss: 6.6150 - acc: 0.580 - ETA: 1s - loss: 6.6427 - acc: 0.577 - ETA: 1s - loss: 6.6415 - acc: 0.577 - ETA: 1s - loss: 6.5904 - acc: 0.580 - ETA: 1s - loss: 6.5690 - acc: 0.582 - ETA: 1s - loss: 6.5766 - acc: 0.582 - ETA: 0s - loss: 6.5987 - acc: 0.580 - ETA: 0s - loss: 6.6213 - acc: 0.579 - ETA: 0s - loss: 6.6261 - acc: 0.578 - ETA: 0s - loss: 6.6315 - acc: 0.577 - ETA: 0s - loss: 6.6732 - acc: 0.575 - ETA: 0s - loss: 6.6638 - acc: 0.576 - ETA: 0s - loss: 6.6527 - acc: 0.576 - ETA: 0s - loss: 6.6221 - acc: 0.578 - ETA: 0s - loss: 6.6071 - acc: 0.578 - ETA: 0s - loss: 6.6170 - acc: 0.577 - ETA: 0s - loss: 6.6137 - acc: 0.578 - ETA: 0s - loss: 6.6111 - acc: 0.578 - ETA: 0s - loss: 6.6211 - acc: 0.577 - ETA: 0s - loss: 6.6181 - acc: 0.577 - ETA: 0s - loss: 6.6197 - acc: 0.577 - ETA: 0s - loss: 6.6605 - acc: 0.574 - ETA: 0s - loss: 6.6844 - acc: 0.573 - ETA: 0s - loss: 6.6884 - acc: 0.573 - ETA: 0s - loss: 6.7159 - acc: 0.571 - 2s 291us/step - loss: 6.7168 - acc: 0.5716 - val\_loss: 7.4588 - val\_acc: 0.4491

Epoch 00017: val\_loss improved from 7.50857 to 7.45885, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 18/20
6680/6680 [==============================] - ETA: 2s - loss: 4.0340 - acc: 0.750 - ETA: 1s - loss: 6.1075 - acc: 0.609 - ETA: 1s - loss: 6.3033 - acc: 0.602 - ETA: 1s - loss: 6.1323 - acc: 0.615 - ETA: 1s - loss: 6.4498 - acc: 0.592 - ETA: 1s - loss: 6.3182 - acc: 0.601 - ETA: 1s - loss: 6.5554 - acc: 0.585 - ETA: 1s - loss: 6.6966 - acc: 0.575 - ETA: 1s - loss: 6.8081 - acc: 0.569 - ETA: 1s - loss: 6.8275 - acc: 0.569 - ETA: 1s - loss: 6.8138 - acc: 0.570 - ETA: 1s - loss: 6.7073 - acc: 0.576 - ETA: 1s - loss: 6.6781 - acc: 0.578 - ETA: 1s - loss: 6.6205 - acc: 0.582 - ETA: 1s - loss: 6.6395 - acc: 0.581 - ETA: 1s - loss: 6.6550 - acc: 0.578 - ETA: 1s - loss: 6.6915 - acc: 0.577 - ETA: 0s - loss: 6.6378 - acc: 0.580 - ETA: 0s - loss: 6.6137 - acc: 0.582 - ETA: 0s - loss: 6.6196 - acc: 0.581 - ETA: 0s - loss: 6.6742 - acc: 0.578 - ETA: 0s - loss: 6.6348 - acc: 0.580 - ETA: 0s - loss: 6.6651 - acc: 0.579 - ETA: 0s - loss: 6.6450 - acc: 0.580 - ETA: 0s - loss: 6.6443 - acc: 0.580 - ETA: 0s - loss: 6.6591 - acc: 0.579 - ETA: 0s - loss: 6.6604 - acc: 0.580 - ETA: 0s - loss: 6.6603 - acc: 0.580 - ETA: 0s - loss: 6.6287 - acc: 0.582 - ETA: 0s - loss: 6.6272 - acc: 0.581 - ETA: 0s - loss: 6.6013 - acc: 0.583 - ETA: 0s - loss: 6.6240 - acc: 0.582 - ETA: 0s - loss: 6.6375 - acc: 0.581 - ETA: 0s - loss: 6.6660 - acc: 0.579 - ETA: 0s - loss: 6.6702 - acc: 0.579 - ETA: 0s - loss: 6.6662 - acc: 0.579 - 2s 299us/step - loss: 6.6616 - acc: 0.5799 - val\_loss: 7.4545 - val\_acc: 0.4599

Epoch 00018: val\_loss improved from 7.45885 to 7.45445, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 19/20
6680/6680 [==============================] - ETA: 2s - loss: 7.2635 - acc: 0.550 - ETA: 1s - loss: 6.6735 - acc: 0.581 - ETA: 1s - loss: 6.7925 - acc: 0.575 - ETA: 1s - loss: 6.7666 - acc: 0.575 - ETA: 1s - loss: 6.6975 - acc: 0.578 - ETA: 1s - loss: 6.7013 - acc: 0.579 - ETA: 1s - loss: 6.7040 - acc: 0.579 - ETA: 1s - loss: 6.6418 - acc: 0.584 - ETA: 1s - loss: 6.5243 - acc: 0.591 - ETA: 1s - loss: 6.5301 - acc: 0.590 - ETA: 1s - loss: 6.4802 - acc: 0.594 - ETA: 1s - loss: 6.4157 - acc: 0.598 - ETA: 1s - loss: 6.4107 - acc: 0.598 - ETA: 1s - loss: 6.4474 - acc: 0.594 - ETA: 1s - loss: 6.4782 - acc: 0.593 - ETA: 1s - loss: 6.5271 - acc: 0.590 - ETA: 1s - loss: 6.5551 - acc: 0.588 - ETA: 0s - loss: 6.6050 - acc: 0.585 - ETA: 0s - loss: 6.6100 - acc: 0.585 - ETA: 0s - loss: 6.6191 - acc: 0.584 - ETA: 0s - loss: 6.6132 - acc: 0.584 - ETA: 0s - loss: 6.6498 - acc: 0.582 - ETA: 0s - loss: 6.6786 - acc: 0.580 - ETA: 0s - loss: 6.6958 - acc: 0.578 - ETA: 0s - loss: 6.6875 - acc: 0.578 - ETA: 0s - loss: 6.7055 - acc: 0.577 - ETA: 0s - loss: 6.6981 - acc: 0.577 - ETA: 0s - loss: 6.7000 - acc: 0.577 - ETA: 0s - loss: 6.7128 - acc: 0.577 - ETA: 0s - loss: 6.6714 - acc: 0.580 - ETA: 0s - loss: 6.6696 - acc: 0.580 - ETA: 0s - loss: 6.6679 - acc: 0.580 - ETA: 0s - loss: 6.6719 - acc: 0.579 - ETA: 0s - loss: 6.6381 - acc: 0.582 - ETA: 0s - loss: 6.6229 - acc: 0.583 - 2s 294us/step - loss: 6.6332 - acc: 0.5825 - val\_loss: 7.4504 - val\_acc: 0.4719

Epoch 00019: val\_loss improved from 7.45445 to 7.45037, saving model to saved\_models/weights.best.VGG16.hdf5
Epoch 20/20
6680/6680 [==============================] - ETA: 1s - loss: 7.2536 - acc: 0.550 - ETA: 1s - loss: 6.0110 - acc: 0.627 - ETA: 1s - loss: 5.9965 - acc: 0.622 - ETA: 1s - loss: 6.0967 - acc: 0.616 - ETA: 1s - loss: 6.3305 - acc: 0.601 - ETA: 1s - loss: 6.5329 - acc: 0.589 - ETA: 1s - loss: 6.6518 - acc: 0.583 - ETA: 1s - loss: 6.7461 - acc: 0.577 - ETA: 1s - loss: 6.7891 - acc: 0.575 - ETA: 1s - loss: 6.7964 - acc: 0.573 - ETA: 1s - loss: 6.7929 - acc: 0.573 - ETA: 1s - loss: 6.7999 - acc: 0.571 - ETA: 1s - loss: 6.7452 - acc: 0.575 - ETA: 1s - loss: 6.7540 - acc: 0.574 - ETA: 1s - loss: 6.7279 - acc: 0.576 - ETA: 0s - loss: 6.7279 - acc: 0.576 - ETA: 0s - loss: 6.6538 - acc: 0.580 - ETA: 0s - loss: 6.6402 - acc: 0.580 - ETA: 0s - loss: 6.6214 - acc: 0.581 - ETA: 0s - loss: 6.6516 - acc: 0.580 - ETA: 0s - loss: 6.6751 - acc: 0.579 - ETA: 0s - loss: 6.6366 - acc: 0.581 - ETA: 0s - loss: 6.6102 - acc: 0.583 - ETA: 0s - loss: 6.6437 - acc: 0.581 - ETA: 0s - loss: 6.6388 - acc: 0.581 - ETA: 0s - loss: 6.6335 - acc: 0.581 - ETA: 0s - loss: 6.6397 - acc: 0.581 - ETA: 0s - loss: 6.6410 - acc: 0.581 - ETA: 0s - loss: 6.6335 - acc: 0.581 - ETA: 0s - loss: 6.6304 - acc: 0.581 - ETA: 0s - loss: 6.5969 - acc: 0.583 - ETA: 0s - loss: 6.5786 - acc: 0.584 - ETA: 0s - loss: 6.5937 - acc: 0.583 - ETA: 0s - loss: 6.6052 - acc: 0.582 - 2s 289us/step - loss: 6.6140 - acc: 0.5820 - val\_loss: 7.5078 - val\_acc: 0.4455

Epoch 00020: val\_loss did not improve from 7.45037

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}28}]:} <keras.callbacks.History at 0x28310442518>
\end{Verbatim}
            
    \subsubsection{Load the Model with the Best Validation
Loss}\label{load-the-model-with-the-best-validation-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.VGG16.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Test the Model}\label{test-the-model}

Now, we can use the CNN to test how well it identifies breed within our
test dataset of dog images. We print the test accuracy below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{} get index of predicted dog breed for each image in test set}
         \PY{n}{VGG16\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{feature}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{test\PYZus{}VGG16}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{VGG16\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{VGG16\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 47.6077\%

    \end{Verbatim}

    \subsubsection{Predict Dog Breed with the
Model}\label{predict-dog-breed-with-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{o}{*}
         
         \PY{k}{def} \PY{n+nf}{VGG16\PYZus{}predict\PYZus{}breed}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} extract bottleneck features}
             \PY{n}{bottleneck\PYZus{}feature} \PY{o}{=} \PY{n}{extract\PYZus{}VGG16}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} obtain predicted vector}
             \PY{n}{predicted\PYZus{}vector} \PY{o}{=} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{bottleneck\PYZus{}feature}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} return dog breed that is predicted by the model}
             \PY{k}{return} \PY{n}{dog\PYZus{}names}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{predicted\PYZus{}vector}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 5: Create a CNN to Classify Dog Breeds (using Transfer
Learning)

You will now use transfer learning to create a CNN that can identify dog
breed from images. Your CNN must attain at least 60\% accuracy on the
test set.

In Step 4, we used transfer learning to create a CNN using VGG-16
bottleneck features. In this section, you must use the bottleneck
features from a different pre-trained model. To make things easier for
you, we have pre-computed the features for all of the networks that are
currently available in Keras: -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz}{VGG-19}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz}{ResNet-50}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz}{Inception}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz}{Xception}
bottleneck features

The files are encoded as such:

\begin{verbatim}
Dog{network}Data.npz
\end{verbatim}

where \texttt{\{network\}}, in the above filename, can be one of
\texttt{VGG19}, \texttt{Resnet50}, \texttt{InceptionV3}, or
\texttt{Xception}. Pick one of the above architectures, download the
corresponding bottleneck features, and store the downloaded file in the
\texttt{bottleneck\_features/} folder in the repository.

\subsubsection{(IMPLEMENTATION) Obtain Bottleneck
Features}\label{implementation-obtain-bottleneck-features}

In the code block below, extract the bottleneck features corresponding
to the train, test, and validation sets by running the following:

\begin{verbatim}
bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')
train_{network} = bottleneck_features['train']
valid_{network} = bottleneck_features['valid']
test_{network} = bottleneck_features['test']
\end{verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Obtain bottleneck features from another pre\PYZhy{}trained CNN.}
        \PY{c+c1}{\PYZsh{}From Resnet}
        \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bottleneck\PYZus{}features/DogResnet50Data.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{train\PYZus{}ResNet50} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{valid\PYZus{}ResNet50} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{test\PYZus{}ResNet50} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Model
Architecture}\label{implementation-model-architecture}

Create a CNN to classify dog breed. At the end of your code cell block,
summarize the layers of your model by executing the line:

\begin{verbatim}
    <your model's name>.summary()
\end{verbatim}

\textbf{Question 5:} Outline the steps you took to get to your final CNN
architecture and your reasoning at each step. Describe why you think the
architecture is suitable for the current problem.

\textbf{Answer:}

I used the ResNet-50 bottleneck features to create my CNN architecture
through transfer learning. I used ResNet-50 bottleneck features compared
to other bottleneck features because of file size. The ResNet-50 size is
small compared to the rest but it yields high accuracy prediction so the
tradeoff was worth it. The model achieved above 60 \% just trained on 20
epochs. I added the Globalpooling prior the final layer for optimal
result per recommendation on the lecture. This architecture is suitable
for the current problem because it predicted a Test accuracy: 83.0144\%
which is above 60\%.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Define your architecture.}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Conv2D}\PY{p}{,} \PY{n}{MaxPooling2D}\PY{p}{,} \PY{n}{GlobalAveragePooling2D}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Flatten}\PY{p}{,} \PY{n}{Dense}\PY{p}{,} \PY{n}{Activation}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers}\PY{n+nn}{.}\PY{n+nn}{normalization} \PY{k}{import} \PY{n}{BatchNormalization}
         
         \PY{n}{ResNet\PYZus{}model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{ResNet\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{train\PYZus{}ResNet50}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{ResNet\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{ResNet\PYZus{}model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_2 ( (None, 2048)              0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_6 (Dense)              (None, 133)               272517    
=================================================================
Total params: 272,517
Trainable params: 272,517
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \subsubsection{(IMPLEMENTATION) Compile the
Model}\label{implementation-compile-the-model}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Compile the model.}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{optimizers} \PY{k}{import} \PY{n}{Adam}\PY{p}{,} \PY{n}{Adamax}
         \PY{n}{ResNet\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{Adamax}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.002}\PY{p}{)}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Train the
Model}\label{implementation-train-the-model}

Train your model in the code cell below. Use model checkpointing to save
the model that attains the best validation loss.

You are welcome to
\href{https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html}{augment
the training data}, but this is not a requirement.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Train the model.}
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{ModelCheckpoint}  
         
         \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best\PYZus{}adamax.ResNet50.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         
         
         \PY{n}{ResNet\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}ResNet50}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}ResNet50}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/20
6680/6680 [==============================] - ETA: 5s - loss: 0.1068 - acc: 1.000 - ETA: 3s - loss: 0.1384 - acc: 0.985 - ETA: 3s - loss: 0.1296 - acc: 0.992 - ETA: 3s - loss: 0.1277 - acc: 0.991 - ETA: 3s - loss: 0.1239 - acc: 0.993 - ETA: 3s - loss: 0.1266 - acc: 0.994 - ETA: 3s - loss: 0.1275 - acc: 0.993 - ETA: 3s - loss: 0.1257 - acc: 0.993 - ETA: 3s - loss: 0.1285 - acc: 0.991 - ETA: 3s - loss: 0.1300 - acc: 0.991 - ETA: 3s - loss: 0.1325 - acc: 0.991 - ETA: 3s - loss: 0.1336 - acc: 0.990 - ETA: 3s - loss: 0.1345 - acc: 0.990 - ETA: 3s - loss: 0.1347 - acc: 0.990 - ETA: 2s - loss: 0.1343 - acc: 0.991 - ETA: 2s - loss: 0.1354 - acc: 0.991 - ETA: 2s - loss: 0.1356 - acc: 0.991 - ETA: 2s - loss: 0.1357 - acc: 0.990 - ETA: 2s - loss: 0.1361 - acc: 0.990 - ETA: 2s - loss: 0.1374 - acc: 0.989 - ETA: 2s - loss: 0.1374 - acc: 0.989 - ETA: 2s - loss: 0.1363 - acc: 0.989 - ETA: 2s - loss: 0.1383 - acc: 0.989 - ETA: 2s - loss: 0.1393 - acc: 0.988 - ETA: 2s - loss: 0.1380 - acc: 0.988 - ETA: 2s - loss: 0.1384 - acc: 0.988 - ETA: 2s - loss: 0.1383 - acc: 0.988 - ETA: 2s - loss: 0.1385 - acc: 0.988 - ETA: 2s - loss: 0.1391 - acc: 0.988 - ETA: 2s - loss: 0.1386 - acc: 0.988 - ETA: 2s - loss: 0.1389 - acc: 0.988 - ETA: 2s - loss: 0.1396 - acc: 0.988 - ETA: 2s - loss: 0.1383 - acc: 0.988 - ETA: 1s - loss: 0.1388 - acc: 0.988 - ETA: 1s - loss: 0.1376 - acc: 0.988 - ETA: 1s - loss: 0.1371 - acc: 0.989 - ETA: 1s - loss: 0.1372 - acc: 0.989 - ETA: 1s - loss: 0.1369 - acc: 0.989 - ETA: 1s - loss: 0.1377 - acc: 0.988 - ETA: 1s - loss: 0.1383 - acc: 0.988 - ETA: 1s - loss: 0.1386 - acc: 0.987 - ETA: 1s - loss: 0.1384 - acc: 0.988 - ETA: 1s - loss: 0.1392 - acc: 0.987 - ETA: 1s - loss: 0.1391 - acc: 0.987 - ETA: 1s - loss: 0.1392 - acc: 0.987 - ETA: 1s - loss: 0.1388 - acc: 0.987 - ETA: 1s - loss: 0.1382 - acc: 0.987 - ETA: 1s - loss: 0.1375 - acc: 0.987 - ETA: 1s - loss: 0.1371 - acc: 0.987 - ETA: 1s - loss: 0.1373 - acc: 0.987 - ETA: 1s - loss: 0.1370 - acc: 0.987 - ETA: 0s - loss: 0.1368 - acc: 0.987 - ETA: 0s - loss: 0.1371 - acc: 0.987 - ETA: 0s - loss: 0.1368 - acc: 0.987 - ETA: 0s - loss: 0.1376 - acc: 0.986 - ETA: 0s - loss: 0.1380 - acc: 0.986 - ETA: 0s - loss: 0.1382 - acc: 0.986 - ETA: 0s - loss: 0.1380 - acc: 0.986 - ETA: 0s - loss: 0.1382 - acc: 0.986 - ETA: 0s - loss: 0.1384 - acc: 0.986 - ETA: 0s - loss: 0.1376 - acc: 0.986 - ETA: 0s - loss: 0.1374 - acc: 0.987 - ETA: 0s - loss: 0.1373 - acc: 0.986 - ETA: 0s - loss: 0.1368 - acc: 0.986 - ETA: 0s - loss: 0.1358 - acc: 0.986 - ETA: 0s - loss: 0.1362 - acc: 0.986 - ETA: 0s - loss: 0.1361 - acc: 0.986 - ETA: 0s - loss: 0.1363 - acc: 0.986 - 4s 580us/step - loss: 0.1364 - acc: 0.9867 - val\_loss: 0.5652 - val\_acc: 0.8287

Epoch 00001: val\_loss improved from inf to 0.56524, saving model to saved\_models/weights.best\_adamax.ResNet50.hdf5
Epoch 2/20
6680/6680 [==============================] - ETA: 2s - loss: 0.0904 - acc: 1.000 - ETA: 3s - loss: 0.0854 - acc: 1.000 - ETA: 3s - loss: 0.0832 - acc: 1.000 - ETA: 3s - loss: 0.0835 - acc: 0.996 - ETA: 3s - loss: 0.0887 - acc: 0.995 - ETA: 3s - loss: 0.0895 - acc: 0.994 - ETA: 3s - loss: 0.0902 - acc: 0.993 - ETA: 3s - loss: 0.0896 - acc: 0.994 - ETA: 3s - loss: 0.0914 - acc: 0.992 - ETA: 3s - loss: 0.0903 - acc: 0.993 - ETA: 3s - loss: 0.0925 - acc: 0.992 - ETA: 3s - loss: 0.0925 - acc: 0.992 - ETA: 3s - loss: 0.0912 - acc: 0.993 - ETA: 3s - loss: 0.0920 - acc: 0.992 - ETA: 3s - loss: 0.0902 - acc: 0.992 - ETA: 2s - loss: 0.0897 - acc: 0.993 - ETA: 2s - loss: 0.0914 - acc: 0.992 - ETA: 2s - loss: 0.0933 - acc: 0.992 - ETA: 2s - loss: 0.0928 - acc: 0.993 - ETA: 2s - loss: 0.0927 - acc: 0.993 - ETA: 2s - loss: 0.0938 - acc: 0.993 - ETA: 2s - loss: 0.0936 - acc: 0.993 - ETA: 2s - loss: 0.0936 - acc: 0.993 - ETA: 2s - loss: 0.0933 - acc: 0.993 - ETA: 2s - loss: 0.0929 - acc: 0.994 - ETA: 2s - loss: 0.0921 - acc: 0.994 - ETA: 2s - loss: 0.0923 - acc: 0.994 - ETA: 2s - loss: 0.0930 - acc: 0.993 - ETA: 2s - loss: 0.0927 - acc: 0.993 - ETA: 2s - loss: 0.0917 - acc: 0.993 - ETA: 2s - loss: 0.0920 - acc: 0.992 - ETA: 2s - loss: 0.0917 - acc: 0.993 - ETA: 2s - loss: 0.0911 - acc: 0.993 - ETA: 2s - loss: 0.0905 - acc: 0.993 - ETA: 1s - loss: 0.0904 - acc: 0.993 - ETA: 1s - loss: 0.0904 - acc: 0.993 - ETA: 1s - loss: 0.0908 - acc: 0.993 - ETA: 1s - loss: 0.0901 - acc: 0.993 - ETA: 1s - loss: 0.0903 - acc: 0.994 - ETA: 1s - loss: 0.0903 - acc: 0.994 - ETA: 1s - loss: 0.0906 - acc: 0.993 - ETA: 1s - loss: 0.0907 - acc: 0.993 - ETA: 1s - loss: 0.0907 - acc: 0.993 - ETA: 1s - loss: 0.0903 - acc: 0.994 - ETA: 1s - loss: 0.0905 - acc: 0.994 - ETA: 1s - loss: 0.0902 - acc: 0.994 - ETA: 1s - loss: 0.0901 - acc: 0.994 - ETA: 1s - loss: 0.0900 - acc: 0.994 - ETA: 1s - loss: 0.0901 - acc: 0.994 - ETA: 1s - loss: 0.0900 - acc: 0.994 - ETA: 1s - loss: 0.0900 - acc: 0.994 - ETA: 1s - loss: 0.0899 - acc: 0.994 - ETA: 1s - loss: 0.0896 - acc: 0.994 - ETA: 0s - loss: 0.0901 - acc: 0.994 - ETA: 0s - loss: 0.0910 - acc: 0.993 - ETA: 0s - loss: 0.0911 - acc: 0.993 - ETA: 0s - loss: 0.0913 - acc: 0.993 - ETA: 0s - loss: 0.0910 - acc: 0.993 - ETA: 0s - loss: 0.0908 - acc: 0.993 - ETA: 0s - loss: 0.0908 - acc: 0.993 - ETA: 0s - loss: 0.0908 - acc: 0.994 - ETA: 0s - loss: 0.0910 - acc: 0.994 - ETA: 0s - loss: 0.0908 - acc: 0.994 - ETA: 0s - loss: 0.0905 - acc: 0.994 - ETA: 0s - loss: 0.0907 - acc: 0.994 - ETA: 0s - loss: 0.0912 - acc: 0.994 - ETA: 0s - loss: 0.0908 - acc: 0.994 - ETA: 0s - loss: 0.0908 - acc: 0.994 - ETA: 0s - loss: 0.0910 - acc: 0.994 - ETA: 0s - loss: 0.0911 - acc: 0.994 - ETA: 0s - loss: 0.0909 - acc: 0.994 - ETA: 0s - loss: 0.0910 - acc: 0.994 - 4s 620us/step - loss: 0.0910 - acc: 0.9945 - val\_loss: 0.5488 - val\_acc: 0.8371

Epoch 00002: val\_loss improved from 0.56524 to 0.54877, saving model to saved\_models/weights.best\_adamax.ResNet50.hdf5
Epoch 3/20
6680/6680 [==============================] - ETA: 3s - loss: 0.0554 - acc: 1.000 - ETA: 3s - loss: 0.0586 - acc: 1.000 - ETA: 3s - loss: 0.0687 - acc: 1.000 - ETA: 4s - loss: 0.0672 - acc: 1.000 - ETA: 4s - loss: 0.0678 - acc: 1.000 - ETA: 4s - loss: 0.0699 - acc: 1.000 - ETA: 4s - loss: 0.0716 - acc: 0.996 - ETA: 4s - loss: 0.0717 - acc: 0.995 - ETA: 3s - loss: 0.0698 - acc: 0.995 - ETA: 3s - loss: 0.0681 - acc: 0.996 - ETA: 3s - loss: 0.0687 - acc: 0.996 - ETA: 3s - loss: 0.0676 - acc: 0.997 - ETA: 3s - loss: 0.0690 - acc: 0.996 - ETA: 3s - loss: 0.0691 - acc: 0.996 - ETA: 3s - loss: 0.0681 - acc: 0.996 - ETA: 3s - loss: 0.0684 - acc: 0.997 - ETA: 3s - loss: 0.0678 - acc: 0.997 - ETA: 3s - loss: 0.0679 - acc: 0.997 - ETA: 3s - loss: 0.0670 - acc: 0.997 - ETA: 3s - loss: 0.0673 - acc: 0.997 - ETA: 2s - loss: 0.0675 - acc: 0.997 - ETA: 2s - loss: 0.0669 - acc: 0.997 - ETA: 2s - loss: 0.0678 - acc: 0.997 - ETA: 2s - loss: 0.0674 - acc: 0.997 - ETA: 2s - loss: 0.0667 - acc: 0.997 - ETA: 2s - loss: 0.0666 - acc: 0.997 - ETA: 2s - loss: 0.0667 - acc: 0.997 - ETA: 2s - loss: 0.0669 - acc: 0.997 - ETA: 2s - loss: 0.0670 - acc: 0.997 - ETA: 2s - loss: 0.0670 - acc: 0.997 - ETA: 2s - loss: 0.0667 - acc: 0.997 - ETA: 2s - loss: 0.0668 - acc: 0.997 - ETA: 2s - loss: 0.0671 - acc: 0.998 - ETA: 2s - loss: 0.0669 - acc: 0.998 - ETA: 2s - loss: 0.0679 - acc: 0.997 - ETA: 2s - loss: 0.0679 - acc: 0.997 - ETA: 1s - loss: 0.0677 - acc: 0.997 - ETA: 1s - loss: 0.0678 - acc: 0.997 - ETA: 1s - loss: 0.0680 - acc: 0.997 - ETA: 1s - loss: 0.0682 - acc: 0.997 - ETA: 1s - loss: 0.0685 - acc: 0.997 - ETA: 1s - loss: 0.0688 - acc: 0.997 - ETA: 1s - loss: 0.0688 - acc: 0.997 - ETA: 1s - loss: 0.0693 - acc: 0.997 - ETA: 1s - loss: 0.0692 - acc: 0.997 - ETA: 1s - loss: 0.0693 - acc: 0.997 - ETA: 1s - loss: 0.0689 - acc: 0.997 - ETA: 1s - loss: 0.0693 - acc: 0.997 - ETA: 1s - loss: 0.0691 - acc: 0.997 - ETA: 1s - loss: 0.0689 - acc: 0.997 - ETA: 1s - loss: 0.0692 - acc: 0.997 - ETA: 1s - loss: 0.0687 - acc: 0.997 - ETA: 1s - loss: 0.0692 - acc: 0.997 - ETA: 0s - loss: 0.0688 - acc: 0.997 - ETA: 0s - loss: 0.0690 - acc: 0.997 - ETA: 0s - loss: 0.0689 - acc: 0.997 - ETA: 0s - loss: 0.0687 - acc: 0.997 - ETA: 0s - loss: 0.0687 - acc: 0.997 - ETA: 0s - loss: 0.0687 - acc: 0.997 - ETA: 0s - loss: 0.0691 - acc: 0.997 - ETA: 0s - loss: 0.0691 - acc: 0.997 - ETA: 0s - loss: 0.0690 - acc: 0.997 - ETA: 0s - loss: 0.0688 - acc: 0.997 - ETA: 0s - loss: 0.0691 - acc: 0.997 - ETA: 0s - loss: 0.0693 - acc: 0.996 - ETA: 0s - loss: 0.0691 - acc: 0.996 - ETA: 0s - loss: 0.0692 - acc: 0.997 - ETA: 0s - loss: 0.0691 - acc: 0.997 - ETA: 0s - loss: 0.0690 - acc: 0.997 - ETA: 0s - loss: 0.0693 - acc: 0.997 - 4s 598us/step - loss: 0.0694 - acc: 0.9970 - val\_loss: 0.5438 - val\_acc: 0.8407

Epoch 00003: val\_loss improved from 0.54877 to 0.54381, saving model to saved\_models/weights.best\_adamax.ResNet50.hdf5
Epoch 4/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0733 - acc: 1.000 - ETA: 3s - loss: 0.0567 - acc: 1.000 - ETA: 3s - loss: 0.0489 - acc: 1.000 - ETA: 3s - loss: 0.0516 - acc: 1.000 - ETA: 3s - loss: 0.0499 - acc: 1.000 - ETA: 3s - loss: 0.0493 - acc: 1.000 - ETA: 2s - loss: 0.0501 - acc: 1.000 - ETA: 2s - loss: 0.0504 - acc: 1.000 - ETA: 2s - loss: 0.0495 - acc: 1.000 - ETA: 2s - loss: 0.0491 - acc: 1.000 - ETA: 2s - loss: 0.0483 - acc: 1.000 - ETA: 2s - loss: 0.0483 - acc: 0.999 - ETA: 2s - loss: 0.0482 - acc: 0.999 - ETA: 2s - loss: 0.0487 - acc: 0.999 - ETA: 2s - loss: 0.0499 - acc: 0.998 - ETA: 2s - loss: 0.0498 - acc: 0.998 - ETA: 2s - loss: 0.0495 - acc: 0.998 - ETA: 2s - loss: 0.0495 - acc: 0.998 - ETA: 2s - loss: 0.0496 - acc: 0.998 - ETA: 2s - loss: 0.0500 - acc: 0.998 - ETA: 2s - loss: 0.0499 - acc: 0.998 - ETA: 2s - loss: 0.0498 - acc: 0.998 - ETA: 2s - loss: 0.0496 - acc: 0.998 - ETA: 2s - loss: 0.0495 - acc: 0.998 - ETA: 2s - loss: 0.0497 - acc: 0.998 - ETA: 2s - loss: 0.0509 - acc: 0.997 - ETA: 2s - loss: 0.0507 - acc: 0.997 - ETA: 2s - loss: 0.0511 - acc: 0.997 - ETA: 2s - loss: 0.0507 - acc: 0.997 - ETA: 1s - loss: 0.0507 - acc: 0.997 - ETA: 1s - loss: 0.0506 - acc: 0.997 - ETA: 1s - loss: 0.0504 - acc: 0.997 - ETA: 1s - loss: 0.0509 - acc: 0.997 - ETA: 1s - loss: 0.0510 - acc: 0.997 - ETA: 1s - loss: 0.0512 - acc: 0.997 - ETA: 1s - loss: 0.0515 - acc: 0.997 - ETA: 1s - loss: 0.0515 - acc: 0.997 - ETA: 1s - loss: 0.0513 - acc: 0.997 - ETA: 1s - loss: 0.0517 - acc: 0.997 - ETA: 1s - loss: 0.0518 - acc: 0.997 - ETA: 1s - loss: 0.0517 - acc: 0.997 - ETA: 1s - loss: 0.0517 - acc: 0.997 - ETA: 1s - loss: 0.0519 - acc: 0.997 - ETA: 1s - loss: 0.0521 - acc: 0.997 - ETA: 1s - loss: 0.0522 - acc: 0.998 - ETA: 1s - loss: 0.0523 - acc: 0.997 - ETA: 1s - loss: 0.0526 - acc: 0.997 - ETA: 1s - loss: 0.0525 - acc: 0.997 - ETA: 1s - loss: 0.0524 - acc: 0.997 - ETA: 1s - loss: 0.0528 - acc: 0.997 - ETA: 0s - loss: 0.0525 - acc: 0.997 - ETA: 0s - loss: 0.0526 - acc: 0.997 - ETA: 0s - loss: 0.0526 - acc: 0.997 - ETA: 0s - loss: 0.0527 - acc: 0.997 - ETA: 0s - loss: 0.0524 - acc: 0.997 - ETA: 0s - loss: 0.0526 - acc: 0.997 - ETA: 0s - loss: 0.0528 - acc: 0.998 - ETA: 0s - loss: 0.0530 - acc: 0.998 - ETA: 0s - loss: 0.0533 - acc: 0.997 - ETA: 0s - loss: 0.0535 - acc: 0.997 - ETA: 0s - loss: 0.0535 - acc: 0.998 - ETA: 0s - loss: 0.0536 - acc: 0.997 - ETA: 0s - loss: 0.0539 - acc: 0.997 - ETA: 0s - loss: 0.0542 - acc: 0.997 - ETA: 0s - loss: 0.0542 - acc: 0.997 - ETA: 0s - loss: 0.0542 - acc: 0.997 - ETA: 0s - loss: 0.0545 - acc: 0.997 - ETA: 0s - loss: 0.0547 - acc: 0.997 - 4s 573us/step - loss: 0.0547 - acc: 0.9973 - val\_loss: 0.5280 - val\_acc: 0.8371

Epoch 00004: val\_loss improved from 0.54381 to 0.52800, saving model to saved\_models/weights.best\_adamax.ResNet50.hdf5
Epoch 5/20
6680/6680 [==============================] - ETA: 2s - loss: 0.0307 - acc: 1.000 - ETA: 3s - loss: 0.0457 - acc: 0.991 - ETA: 3s - loss: 0.0481 - acc: 0.990 - ETA: 3s - loss: 0.0461 - acc: 0.994 - ETA: 3s - loss: 0.0440 - acc: 0.995 - ETA: 3s - loss: 0.0443 - acc: 0.996 - ETA: 3s - loss: 0.0440 - acc: 0.996 - ETA: 3s - loss: 0.0444 - acc: 0.997 - ETA: 3s - loss: 0.0458 - acc: 0.996 - ETA: 3s - loss: 0.0442 - acc: 0.996 - ETA: 3s - loss: 0.0426 - acc: 0.997 - ETA: 3s - loss: 0.0423 - acc: 0.997 - ETA: 3s - loss: 0.0427 - acc: 0.997 - ETA: 3s - loss: 0.0419 - acc: 0.997 - ETA: 2s - loss: 0.0427 - acc: 0.997 - ETA: 2s - loss: 0.0422 - acc: 0.997 - ETA: 2s - loss: 0.0419 - acc: 0.997 - ETA: 2s - loss: 0.0418 - acc: 0.997 - ETA: 2s - loss: 0.0421 - acc: 0.997 - ETA: 2s - loss: 0.0422 - acc: 0.997 - ETA: 2s - loss: 0.0416 - acc: 0.998 - ETA: 2s - loss: 0.0412 - acc: 0.998 - ETA: 2s - loss: 0.0408 - acc: 0.998 - ETA: 2s - loss: 0.0403 - acc: 0.998 - ETA: 2s - loss: 0.0404 - acc: 0.997 - ETA: 2s - loss: 0.0404 - acc: 0.998 - ETA: 2s - loss: 0.0406 - acc: 0.998 - ETA: 2s - loss: 0.0406 - acc: 0.998 - ETA: 2s - loss: 0.0403 - acc: 0.998 - ETA: 2s - loss: 0.0405 - acc: 0.998 - ETA: 2s - loss: 0.0405 - acc: 0.998 - ETA: 2s - loss: 0.0407 - acc: 0.998 - ETA: 1s - loss: 0.0406 - acc: 0.998 - ETA: 1s - loss: 0.0406 - acc: 0.998 - ETA: 1s - loss: 0.0409 - acc: 0.998 - ETA: 1s - loss: 0.0411 - acc: 0.998 - ETA: 1s - loss: 0.0412 - acc: 0.998 - ETA: 1s - loss: 0.0416 - acc: 0.997 - ETA: 1s - loss: 0.0414 - acc: 0.997 - ETA: 1s - loss: 0.0414 - acc: 0.997 - ETA: 1s - loss: 0.0416 - acc: 0.997 - ETA: 1s - loss: 0.0419 - acc: 0.997 - ETA: 1s - loss: 0.0422 - acc: 0.997 - ETA: 1s - loss: 0.0424 - acc: 0.997 - ETA: 1s - loss: 0.0426 - acc: 0.997 - ETA: 1s - loss: 0.0428 - acc: 0.997 - ETA: 1s - loss: 0.0429 - acc: 0.997 - ETA: 1s - loss: 0.0427 - acc: 0.997 - ETA: 1s - loss: 0.0425 - acc: 0.997 - ETA: 1s - loss: 0.0428 - acc: 0.997 - ETA: 1s - loss: 0.0426 - acc: 0.997 - ETA: 0s - loss: 0.0426 - acc: 0.997 - ETA: 0s - loss: 0.0425 - acc: 0.997 - ETA: 0s - loss: 0.0422 - acc: 0.997 - ETA: 0s - loss: 0.0422 - acc: 0.997 - ETA: 0s - loss: 0.0425 - acc: 0.997 - ETA: 0s - loss: 0.0425 - acc: 0.997 - ETA: 0s - loss: 0.0424 - acc: 0.997 - ETA: 0s - loss: 0.0424 - acc: 0.997 - ETA: 0s - loss: 0.0427 - acc: 0.997 - ETA: 0s - loss: 0.0427 - acc: 0.997 - ETA: 0s - loss: 0.0428 - acc: 0.997 - ETA: 0s - loss: 0.0428 - acc: 0.997 - ETA: 0s - loss: 0.0427 - acc: 0.997 - ETA: 0s - loss: 0.0426 - acc: 0.997 - ETA: 0s - loss: 0.0426 - acc: 0.997 - ETA: 0s - loss: 0.0425 - acc: 0.997 - ETA: 0s - loss: 0.0426 - acc: 0.997 - ETA: 0s - loss: 0.0427 - acc: 0.997 - 4s 583us/step - loss: 0.0427 - acc: 0.9978 - val\_loss: 0.5409 - val\_acc: 0.8323

Epoch 00005: val\_loss did not improve from 0.52800
Epoch 6/20
6680/6680 [==============================] - ETA: 5s - loss: 0.0465 - acc: 1.000 - ETA: 3s - loss: 0.0291 - acc: 1.000 - ETA: 3s - loss: 0.0315 - acc: 1.000 - ETA: 3s - loss: 0.0329 - acc: 1.000 - ETA: 3s - loss: 0.0302 - acc: 1.000 - ETA: 3s - loss: 0.0320 - acc: 1.000 - ETA: 3s - loss: 0.0319 - acc: 1.000 - ETA: 3s - loss: 0.0316 - acc: 1.000 - ETA: 3s - loss: 0.0309 - acc: 1.000 - ETA: 2s - loss: 0.0306 - acc: 1.000 - ETA: 2s - loss: 0.0310 - acc: 1.000 - ETA: 2s - loss: 0.0305 - acc: 1.000 - ETA: 2s - loss: 0.0300 - acc: 1.000 - ETA: 2s - loss: 0.0300 - acc: 1.000 - ETA: 2s - loss: 0.0311 - acc: 1.000 - ETA: 2s - loss: 0.0315 - acc: 1.000 - ETA: 2s - loss: 0.0313 - acc: 1.000 - ETA: 2s - loss: 0.0314 - acc: 1.000 - ETA: 2s - loss: 0.0316 - acc: 1.000 - ETA: 2s - loss: 0.0323 - acc: 1.000 - ETA: 2s - loss: 0.0323 - acc: 0.999 - ETA: 2s - loss: 0.0321 - acc: 0.999 - ETA: 2s - loss: 0.0320 - acc: 0.999 - ETA: 2s - loss: 0.0319 - acc: 0.999 - ETA: 2s - loss: 0.0315 - acc: 0.999 - ETA: 2s - loss: 0.0316 - acc: 0.999 - ETA: 1s - loss: 0.0317 - acc: 0.999 - ETA: 1s - loss: 0.0315 - acc: 0.999 - ETA: 1s - loss: 0.0314 - acc: 0.999 - ETA: 1s - loss: 0.0313 - acc: 0.999 - ETA: 1s - loss: 0.0313 - acc: 0.999 - ETA: 1s - loss: 0.0313 - acc: 0.999 - ETA: 1s - loss: 0.0314 - acc: 0.999 - ETA: 1s - loss: 0.0314 - acc: 0.999 - ETA: 1s - loss: 0.0318 - acc: 0.999 - ETA: 1s - loss: 0.0319 - acc: 0.999 - ETA: 1s - loss: 0.0324 - acc: 0.999 - ETA: 1s - loss: 0.0327 - acc: 0.999 - ETA: 1s - loss: 0.0327 - acc: 0.999 - ETA: 1s - loss: 0.0326 - acc: 0.999 - ETA: 1s - loss: 0.0324 - acc: 0.999 - ETA: 1s - loss: 0.0328 - acc: 0.999 - ETA: 1s - loss: 0.0328 - acc: 0.999 - ETA: 1s - loss: 0.0328 - acc: 0.999 - ETA: 1s - loss: 0.0327 - acc: 0.999 - ETA: 0s - loss: 0.0328 - acc: 0.999 - ETA: 0s - loss: 0.0327 - acc: 0.999 - ETA: 0s - loss: 0.0327 - acc: 0.999 - ETA: 0s - loss: 0.0327 - acc: 0.999 - ETA: 0s - loss: 0.0326 - acc: 0.999 - ETA: 0s - loss: 0.0325 - acc: 0.999 - ETA: 0s - loss: 0.0325 - acc: 0.999 - ETA: 0s - loss: 0.0324 - acc: 0.999 - ETA: 0s - loss: 0.0324 - acc: 0.999 - ETA: 0s - loss: 0.0331 - acc: 0.998 - ETA: 0s - loss: 0.0330 - acc: 0.998 - ETA: 0s - loss: 0.0335 - acc: 0.998 - ETA: 0s - loss: 0.0335 - acc: 0.998 - ETA: 0s - loss: 0.0335 - acc: 0.998 - ETA: 0s - loss: 0.0336 - acc: 0.998 - ETA: 0s - loss: 0.0337 - acc: 0.998 - ETA: 0s - loss: 0.0337 - acc: 0.998 - ETA: 0s - loss: 0.0338 - acc: 0.998 - ETA: 0s - loss: 0.0338 - acc: 0.998 - 4s 543us/step - loss: 0.0338 - acc: 0.9987 - val\_loss: 0.5330 - val\_acc: 0.8347

Epoch 00006: val\_loss did not improve from 0.52800
Epoch 7/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0143 - acc: 1.000 - ETA: 3s - loss: 0.0191 - acc: 1.000 - ETA: 3s - loss: 0.0248 - acc: 1.000 - ETA: 3s - loss: 0.0251 - acc: 1.000 - ETA: 3s - loss: 0.0251 - acc: 1.000 - ETA: 3s - loss: 0.0252 - acc: 1.000 - ETA: 3s - loss: 0.0247 - acc: 1.000 - ETA: 3s - loss: 0.0252 - acc: 1.000 - ETA: 3s - loss: 0.0253 - acc: 1.000 - ETA: 3s - loss: 0.0250 - acc: 1.000 - ETA: 3s - loss: 0.0253 - acc: 1.000 - ETA: 3s - loss: 0.0261 - acc: 1.000 - ETA: 3s - loss: 0.0260 - acc: 1.000 - ETA: 3s - loss: 0.0257 - acc: 1.000 - ETA: 3s - loss: 0.0254 - acc: 1.000 - ETA: 3s - loss: 0.0253 - acc: 1.000 - ETA: 3s - loss: 0.0252 - acc: 1.000 - ETA: 3s - loss: 0.0247 - acc: 1.000 - ETA: 2s - loss: 0.0244 - acc: 1.000 - ETA: 2s - loss: 0.0245 - acc: 1.000 - ETA: 2s - loss: 0.0249 - acc: 1.000 - ETA: 2s - loss: 0.0249 - acc: 1.000 - ETA: 2s - loss: 0.0251 - acc: 1.000 - ETA: 2s - loss: 0.0251 - acc: 1.000 - ETA: 2s - loss: 0.0252 - acc: 1.000 - ETA: 2s - loss: 0.0251 - acc: 1.000 - ETA: 2s - loss: 0.0255 - acc: 0.999 - ETA: 2s - loss: 0.0253 - acc: 0.999 - ETA: 2s - loss: 0.0256 - acc: 0.999 - ETA: 2s - loss: 0.0255 - acc: 0.999 - ETA: 2s - loss: 0.0255 - acc: 0.999 - ETA: 2s - loss: 0.0253 - acc: 0.999 - ETA: 2s - loss: 0.0254 - acc: 0.999 - ETA: 2s - loss: 0.0254 - acc: 0.999 - ETA: 1s - loss: 0.0255 - acc: 0.999 - ETA: 1s - loss: 0.0257 - acc: 0.999 - ETA: 1s - loss: 0.0256 - acc: 0.999 - ETA: 1s - loss: 0.0256 - acc: 0.999 - ETA: 1s - loss: 0.0262 - acc: 0.999 - ETA: 1s - loss: 0.0263 - acc: 0.999 - ETA: 1s - loss: 0.0267 - acc: 0.999 - ETA: 1s - loss: 0.0266 - acc: 0.999 - ETA: 1s - loss: 0.0265 - acc: 0.999 - ETA: 1s - loss: 0.0262 - acc: 0.999 - ETA: 1s - loss: 0.0262 - acc: 0.999 - ETA: 1s - loss: 0.0261 - acc: 0.999 - ETA: 1s - loss: 0.0259 - acc: 0.999 - ETA: 1s - loss: 0.0259 - acc: 0.999 - ETA: 1s - loss: 0.0260 - acc: 0.998 - ETA: 1s - loss: 0.0261 - acc: 0.998 - ETA: 1s - loss: 0.0265 - acc: 0.998 - ETA: 0s - loss: 0.0264 - acc: 0.998 - ETA: 0s - loss: 0.0264 - acc: 0.998 - ETA: 0s - loss: 0.0268 - acc: 0.998 - ETA: 0s - loss: 0.0268 - acc: 0.998 - ETA: 0s - loss: 0.0268 - acc: 0.998 - ETA: 0s - loss: 0.0271 - acc: 0.998 - ETA: 0s - loss: 0.0271 - acc: 0.998 - ETA: 0s - loss: 0.0271 - acc: 0.998 - ETA: 0s - loss: 0.0270 - acc: 0.998 - ETA: 0s - loss: 0.0270 - acc: 0.998 - ETA: 0s - loss: 0.0269 - acc: 0.998 - ETA: 0s - loss: 0.0268 - acc: 0.998 - ETA: 0s - loss: 0.0268 - acc: 0.998 - ETA: 0s - loss: 0.0268 - acc: 0.998 - ETA: 0s - loss: 0.0268 - acc: 0.998 - ETA: 0s - loss: 0.0268 - acc: 0.998 - ETA: 0s - loss: 0.0271 - acc: 0.998 - ETA: 0s - loss: 0.0275 - acc: 0.998 - 4s 581us/step - loss: 0.0275 - acc: 0.9985 - val\_loss: 0.5405 - val\_acc: 0.8335

Epoch 00007: val\_loss did not improve from 0.52800
Epoch 8/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0265 - acc: 1.000 - ETA: 4s - loss: 0.0194 - acc: 1.000 - ETA: 3s - loss: 0.0182 - acc: 1.000 - ETA: 3s - loss: 0.0208 - acc: 1.000 - ETA: 3s - loss: 0.0191 - acc: 1.000 - ETA: 3s - loss: 0.0197 - acc: 1.000 - ETA: 3s - loss: 0.0210 - acc: 1.000 - ETA: 3s - loss: 0.0206 - acc: 1.000 - ETA: 3s - loss: 0.0200 - acc: 1.000 - ETA: 3s - loss: 0.0206 - acc: 1.000 - ETA: 3s - loss: 0.0209 - acc: 1.000 - ETA: 3s - loss: 0.0205 - acc: 1.000 - ETA: 3s - loss: 0.0201 - acc: 1.000 - ETA: 3s - loss: 0.0217 - acc: 0.999 - ETA: 2s - loss: 0.0214 - acc: 0.999 - ETA: 2s - loss: 0.0215 - acc: 0.999 - ETA: 2s - loss: 0.0212 - acc: 0.999 - ETA: 2s - loss: 0.0210 - acc: 0.999 - ETA: 2s - loss: 0.0209 - acc: 0.999 - ETA: 2s - loss: 0.0214 - acc: 0.999 - ETA: 2s - loss: 0.0213 - acc: 0.999 - ETA: 2s - loss: 0.0214 - acc: 0.998 - ETA: 2s - loss: 0.0217 - acc: 0.998 - ETA: 2s - loss: 0.0216 - acc: 0.998 - ETA: 2s - loss: 0.0212 - acc: 0.998 - ETA: 2s - loss: 0.0214 - acc: 0.998 - ETA: 2s - loss: 0.0213 - acc: 0.998 - ETA: 2s - loss: 0.0216 - acc: 0.998 - ETA: 2s - loss: 0.0215 - acc: 0.999 - ETA: 2s - loss: 0.0215 - acc: 0.999 - ETA: 1s - loss: 0.0214 - acc: 0.999 - ETA: 1s - loss: 0.0213 - acc: 0.999 - ETA: 1s - loss: 0.0214 - acc: 0.999 - ETA: 1s - loss: 0.0218 - acc: 0.998 - ETA: 1s - loss: 0.0218 - acc: 0.998 - ETA: 1s - loss: 0.0223 - acc: 0.998 - ETA: 1s - loss: 0.0222 - acc: 0.998 - ETA: 1s - loss: 0.0222 - acc: 0.998 - ETA: 1s - loss: 0.0222 - acc: 0.998 - ETA: 1s - loss: 0.0222 - acc: 0.998 - ETA: 1s - loss: 0.0222 - acc: 0.998 - ETA: 1s - loss: 0.0221 - acc: 0.998 - ETA: 1s - loss: 0.0219 - acc: 0.998 - ETA: 1s - loss: 0.0220 - acc: 0.998 - ETA: 1s - loss: 0.0219 - acc: 0.998 - ETA: 1s - loss: 0.0219 - acc: 0.998 - ETA: 1s - loss: 0.0222 - acc: 0.998 - ETA: 1s - loss: 0.0222 - acc: 0.998 - ETA: 1s - loss: 0.0222 - acc: 0.998 - ETA: 0s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0222 - acc: 0.998 - ETA: 0s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0222 - acc: 0.998 - ETA: 0s - loss: 0.0222 - acc: 0.998 - ETA: 0s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0223 - acc: 0.998 - ETA: 0s - loss: 0.0223 - acc: 0.998 - ETA: 0s - loss: 0.0223 - acc: 0.998 - ETA: 0s - loss: 0.0223 - acc: 0.998 - ETA: 0s - loss: 0.0226 - acc: 0.998 - ETA: 0s - loss: 0.0227 - acc: 0.998 - ETA: 0s - loss: 0.0227 - acc: 0.998 - ETA: 0s - loss: 0.0226 - acc: 0.998 - ETA: 0s - loss: 0.0226 - acc: 0.998 - 4s 564us/step - loss: 0.0226 - acc: 0.9985 - val\_loss: 0.5478 - val\_acc: 0.8395

Epoch 00008: val\_loss did not improve from 0.52800
Epoch 9/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0151 - acc: 1.000 - ETA: 3s - loss: 0.0132 - acc: 1.000 - ETA: 3s - loss: 0.0171 - acc: 1.000 - ETA: 3s - loss: 0.0168 - acc: 1.000 - ETA: 3s - loss: 0.0173 - acc: 1.000 - ETA: 3s - loss: 0.0166 - acc: 1.000 - ETA: 3s - loss: 0.0161 - acc: 1.000 - ETA: 3s - loss: 0.0171 - acc: 0.998 - ETA: 3s - loss: 0.0165 - acc: 0.998 - ETA: 3s - loss: 0.0165 - acc: 0.998 - ETA: 3s - loss: 0.0161 - acc: 0.999 - ETA: 3s - loss: 0.0167 - acc: 0.999 - ETA: 2s - loss: 0.0167 - acc: 0.999 - ETA: 2s - loss: 0.0169 - acc: 0.999 - ETA: 2s - loss: 0.0171 - acc: 0.999 - ETA: 2s - loss: 0.0170 - acc: 0.999 - ETA: 2s - loss: 0.0169 - acc: 0.999 - ETA: 2s - loss: 0.0170 - acc: 0.999 - ETA: 2s - loss: 0.0170 - acc: 0.999 - ETA: 2s - loss: 0.0168 - acc: 0.999 - ETA: 2s - loss: 0.0170 - acc: 0.999 - ETA: 2s - loss: 0.0170 - acc: 0.999 - ETA: 2s - loss: 0.0169 - acc: 0.999 - ETA: 2s - loss: 0.0167 - acc: 0.999 - ETA: 2s - loss: 0.0166 - acc: 0.999 - ETA: 2s - loss: 0.0164 - acc: 0.999 - ETA: 2s - loss: 0.0164 - acc: 0.999 - ETA: 2s - loss: 0.0164 - acc: 0.999 - ETA: 2s - loss: 0.0163 - acc: 0.999 - ETA: 1s - loss: 0.0162 - acc: 0.999 - ETA: 1s - loss: 0.0162 - acc: 0.999 - ETA: 1s - loss: 0.0161 - acc: 0.999 - ETA: 1s - loss: 0.0161 - acc: 0.999 - ETA: 1s - loss: 0.0160 - acc: 0.999 - ETA: 1s - loss: 0.0160 - acc: 0.999 - ETA: 1s - loss: 0.0160 - acc: 0.999 - ETA: 1s - loss: 0.0160 - acc: 0.999 - ETA: 1s - loss: 0.0163 - acc: 0.999 - ETA: 1s - loss: 0.0163 - acc: 0.999 - ETA: 1s - loss: 0.0166 - acc: 0.999 - ETA: 1s - loss: 0.0165 - acc: 0.999 - ETA: 1s - loss: 0.0166 - acc: 0.999 - ETA: 1s - loss: 0.0167 - acc: 0.999 - ETA: 1s - loss: 0.0167 - acc: 0.999 - ETA: 1s - loss: 0.0169 - acc: 0.999 - ETA: 1s - loss: 0.0168 - acc: 0.999 - ETA: 1s - loss: 0.0170 - acc: 0.999 - ETA: 1s - loss: 0.0170 - acc: 0.999 - ETA: 0s - loss: 0.0170 - acc: 0.999 - ETA: 0s - loss: 0.0170 - acc: 0.999 - ETA: 0s - loss: 0.0171 - acc: 0.999 - ETA: 0s - loss: 0.0171 - acc: 0.999 - ETA: 0s - loss: 0.0171 - acc: 0.999 - ETA: 0s - loss: 0.0173 - acc: 0.999 - ETA: 0s - loss: 0.0174 - acc: 0.999 - ETA: 0s - loss: 0.0174 - acc: 0.999 - ETA: 0s - loss: 0.0178 - acc: 0.999 - ETA: 0s - loss: 0.0178 - acc: 0.999 - ETA: 0s - loss: 0.0178 - acc: 0.999 - ETA: 0s - loss: 0.0178 - acc: 0.999 - ETA: 0s - loss: 0.0178 - acc: 0.999 - ETA: 0s - loss: 0.0178 - acc: 0.999 - ETA: 0s - loss: 0.0178 - acc: 0.999 - ETA: 0s - loss: 0.0178 - acc: 0.999 - ETA: 0s - loss: 0.0180 - acc: 0.998 - 4s 547us/step - loss: 0.0183 - acc: 0.9988 - val\_loss: 0.5300 - val\_acc: 0.8515

Epoch 00009: val\_loss did not improve from 0.52800
Epoch 10/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0106 - acc: 1.000 - ETA: 3s - loss: 0.0133 - acc: 1.000 - ETA: 3s - loss: 0.0133 - acc: 1.000 - ETA: 3s - loss: 0.0131 - acc: 1.000 - ETA: 3s - loss: 0.0121 - acc: 1.000 - ETA: 3s - loss: 0.0120 - acc: 1.000 - ETA: 3s - loss: 0.0121 - acc: 1.000 - ETA: 3s - loss: 0.0125 - acc: 1.000 - ETA: 3s - loss: 0.0127 - acc: 1.000 - ETA: 3s - loss: 0.0128 - acc: 1.000 - ETA: 3s - loss: 0.0127 - acc: 1.000 - ETA: 3s - loss: 0.0128 - acc: 1.000 - ETA: 2s - loss: 0.0131 - acc: 1.000 - ETA: 2s - loss: 0.0132 - acc: 1.000 - ETA: 2s - loss: 0.0135 - acc: 1.000 - ETA: 2s - loss: 0.0135 - acc: 1.000 - ETA: 2s - loss: 0.0133 - acc: 1.000 - ETA: 2s - loss: 0.0134 - acc: 1.000 - ETA: 2s - loss: 0.0134 - acc: 1.000 - ETA: 2s - loss: 0.0135 - acc: 1.000 - ETA: 2s - loss: 0.0135 - acc: 1.000 - ETA: 2s - loss: 0.0137 - acc: 1.000 - ETA: 2s - loss: 0.0141 - acc: 0.999 - ETA: 2s - loss: 0.0142 - acc: 0.999 - ETA: 2s - loss: 0.0148 - acc: 0.999 - ETA: 2s - loss: 0.0147 - acc: 0.999 - ETA: 2s - loss: 0.0147 - acc: 0.999 - ETA: 2s - loss: 0.0146 - acc: 0.999 - ETA: 2s - loss: 0.0145 - acc: 0.999 - ETA: 1s - loss: 0.0144 - acc: 0.999 - ETA: 1s - loss: 0.0143 - acc: 0.999 - ETA: 1s - loss: 0.0142 - acc: 0.999 - ETA: 1s - loss: 0.0142 - acc: 0.999 - ETA: 1s - loss: 0.0142 - acc: 0.999 - ETA: 1s - loss: 0.0143 - acc: 0.999 - ETA: 1s - loss: 0.0142 - acc: 0.999 - ETA: 1s - loss: 0.0141 - acc: 0.999 - ETA: 1s - loss: 0.0140 - acc: 0.999 - ETA: 1s - loss: 0.0144 - acc: 0.999 - ETA: 1s - loss: 0.0144 - acc: 0.999 - ETA: 1s - loss: 0.0146 - acc: 0.999 - ETA: 1s - loss: 0.0146 - acc: 0.999 - ETA: 1s - loss: 0.0145 - acc: 0.999 - ETA: 1s - loss: 0.0145 - acc: 0.999 - ETA: 1s - loss: 0.0145 - acc: 0.999 - ETA: 1s - loss: 0.0145 - acc: 0.999 - ETA: 1s - loss: 0.0144 - acc: 0.999 - ETA: 0s - loss: 0.0146 - acc: 0.999 - ETA: 0s - loss: 0.0146 - acc: 0.999 - ETA: 0s - loss: 0.0146 - acc: 0.999 - ETA: 0s - loss: 0.0146 - acc: 0.999 - ETA: 0s - loss: 0.0145 - acc: 0.999 - ETA: 0s - loss: 0.0144 - acc: 0.999 - ETA: 0s - loss: 0.0145 - acc: 0.999 - ETA: 0s - loss: 0.0145 - acc: 0.999 - ETA: 0s - loss: 0.0148 - acc: 0.998 - ETA: 0s - loss: 0.0147 - acc: 0.999 - ETA: 0s - loss: 0.0147 - acc: 0.999 - ETA: 0s - loss: 0.0147 - acc: 0.999 - ETA: 0s - loss: 0.0147 - acc: 0.999 - ETA: 0s - loss: 0.0147 - acc: 0.999 - ETA: 0s - loss: 0.0147 - acc: 0.999 - ETA: 0s - loss: 0.0150 - acc: 0.998 - ETA: 0s - loss: 0.0150 - acc: 0.998 - ETA: 0s - loss: 0.0152 - acc: 0.998 - 4s 544us/step - loss: 0.0152 - acc: 0.9988 - val\_loss: 0.5516 - val\_acc: 0.8443

Epoch 00010: val\_loss did not improve from 0.52800
Epoch 11/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0112 - acc: 1.000 - ETA: 3s - loss: 0.0100 - acc: 1.000 - ETA: 3s - loss: 0.0099 - acc: 1.000 - ETA: 3s - loss: 0.0159 - acc: 0.996 - ETA: 3s - loss: 0.0143 - acc: 0.997 - ETA: 3s - loss: 0.0134 - acc: 0.998 - ETA: 3s - loss: 0.0125 - acc: 0.998 - ETA: 3s - loss: 0.0119 - acc: 0.998 - ETA: 3s - loss: 0.0113 - acc: 0.998 - ETA: 3s - loss: 0.0112 - acc: 0.998 - ETA: 3s - loss: 0.0108 - acc: 0.999 - ETA: 3s - loss: 0.0109 - acc: 0.999 - ETA: 2s - loss: 0.0109 - acc: 0.999 - ETA: 2s - loss: 0.0109 - acc: 0.999 - ETA: 2s - loss: 0.0109 - acc: 0.999 - ETA: 2s - loss: 0.0109 - acc: 0.999 - ETA: 2s - loss: 0.0107 - acc: 0.999 - ETA: 2s - loss: 0.0106 - acc: 0.999 - ETA: 2s - loss: 0.0107 - acc: 0.999 - ETA: 2s - loss: 0.0110 - acc: 0.999 - ETA: 2s - loss: 0.0109 - acc: 0.999 - ETA: 2s - loss: 0.0110 - acc: 0.999 - ETA: 2s - loss: 0.0109 - acc: 0.999 - ETA: 2s - loss: 0.0109 - acc: 0.999 - ETA: 2s - loss: 0.0113 - acc: 0.999 - ETA: 2s - loss: 0.0113 - acc: 0.999 - ETA: 2s - loss: 0.0122 - acc: 0.998 - ETA: 2s - loss: 0.0128 - acc: 0.998 - ETA: 1s - loss: 0.0134 - acc: 0.997 - ETA: 1s - loss: 0.0133 - acc: 0.998 - ETA: 1s - loss: 0.0132 - acc: 0.998 - ETA: 1s - loss: 0.0136 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.997 - ETA: 1s - loss: 0.0135 - acc: 0.997 - ETA: 1s - loss: 0.0138 - acc: 0.997 - ETA: 1s - loss: 0.0137 - acc: 0.997 - ETA: 1s - loss: 0.0137 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.997 - ETA: 1s - loss: 0.0137 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.998 - ETA: 1s - loss: 0.0135 - acc: 0.998 - ETA: 1s - loss: 0.0134 - acc: 0.998 - ETA: 1s - loss: 0.0134 - acc: 0.998 - ETA: 0s - loss: 0.0136 - acc: 0.997 - ETA: 0s - loss: 0.0134 - acc: 0.998 - ETA: 0s - loss: 0.0133 - acc: 0.998 - ETA: 0s - loss: 0.0133 - acc: 0.998 - ETA: 0s - loss: 0.0132 - acc: 0.998 - ETA: 0s - loss: 0.0132 - acc: 0.998 - ETA: 0s - loss: 0.0131 - acc: 0.998 - ETA: 0s - loss: 0.0131 - acc: 0.998 - ETA: 0s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0132 - acc: 0.998 - ETA: 0s - loss: 0.0131 - acc: 0.998 - ETA: 0s - loss: 0.0131 - acc: 0.998 - ETA: 0s - loss: 0.0131 - acc: 0.998 - 4s 541us/step - loss: 0.0131 - acc: 0.9984 - val\_loss: 0.5452 - val\_acc: 0.8539

Epoch 00011: val\_loss did not improve from 0.52800
Epoch 12/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0131 - acc: 1.000 - ETA: 4s - loss: 0.0154 - acc: 0.991 - ETA: 3s - loss: 0.0113 - acc: 0.995 - ETA: 3s - loss: 0.0107 - acc: 0.996 - ETA: 3s - loss: 0.0113 - acc: 0.997 - ETA: 3s - loss: 0.0107 - acc: 0.998 - ETA: 3s - loss: 0.0101 - acc: 0.998 - ETA: 3s - loss: 0.0098 - acc: 0.998 - ETA: 3s - loss: 0.0097 - acc: 0.998 - ETA: 3s - loss: 0.0097 - acc: 0.998 - ETA: 3s - loss: 0.0123 - acc: 0.998 - ETA: 3s - loss: 0.0118 - acc: 0.998 - ETA: 2s - loss: 0.0115 - acc: 0.998 - ETA: 2s - loss: 0.0121 - acc: 0.997 - ETA: 2s - loss: 0.0117 - acc: 0.997 - ETA: 2s - loss: 0.0116 - acc: 0.998 - ETA: 2s - loss: 0.0115 - acc: 0.998 - ETA: 2s - loss: 0.0114 - acc: 0.998 - ETA: 2s - loss: 0.0115 - acc: 0.998 - ETA: 2s - loss: 0.0114 - acc: 0.998 - ETA: 2s - loss: 0.0112 - acc: 0.998 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 2s - loss: 0.0110 - acc: 0.998 - ETA: 2s - loss: 0.0110 - acc: 0.998 - ETA: 2s - loss: 0.0108 - acc: 0.998 - ETA: 2s - loss: 0.0107 - acc: 0.998 - ETA: 2s - loss: 0.0106 - acc: 0.998 - ETA: 2s - loss: 0.0110 - acc: 0.998 - ETA: 2s - loss: 0.0110 - acc: 0.998 - ETA: 1s - loss: 0.0110 - acc: 0.998 - ETA: 1s - loss: 0.0112 - acc: 0.998 - ETA: 1s - loss: 0.0113 - acc: 0.998 - ETA: 1s - loss: 0.0113 - acc: 0.998 - ETA: 1s - loss: 0.0112 - acc: 0.998 - ETA: 1s - loss: 0.0111 - acc: 0.998 - ETA: 1s - loss: 0.0110 - acc: 0.998 - ETA: 1s - loss: 0.0115 - acc: 0.998 - ETA: 1s - loss: 0.0115 - acc: 0.998 - ETA: 1s - loss: 0.0117 - acc: 0.998 - ETA: 1s - loss: 0.0116 - acc: 0.998 - ETA: 1s - loss: 0.0116 - acc: 0.998 - ETA: 1s - loss: 0.0115 - acc: 0.998 - ETA: 1s - loss: 0.0119 - acc: 0.998 - ETA: 1s - loss: 0.0118 - acc: 0.998 - ETA: 1s - loss: 0.0117 - acc: 0.998 - ETA: 1s - loss: 0.0117 - acc: 0.998 - ETA: 0s - loss: 0.0117 - acc: 0.998 - ETA: 0s - loss: 0.0116 - acc: 0.998 - ETA: 0s - loss: 0.0115 - acc: 0.998 - ETA: 0s - loss: 0.0114 - acc: 0.998 - ETA: 0s - loss: 0.0113 - acc: 0.998 - ETA: 0s - loss: 0.0113 - acc: 0.998 - ETA: 0s - loss: 0.0112 - acc: 0.998 - ETA: 0s - loss: 0.0114 - acc: 0.998 - ETA: 0s - loss: 0.0114 - acc: 0.998 - ETA: 0s - loss: 0.0114 - acc: 0.998 - ETA: 0s - loss: 0.0114 - acc: 0.998 - ETA: 0s - loss: 0.0114 - acc: 0.998 - ETA: 0s - loss: 0.0113 - acc: 0.998 - ETA: 0s - loss: 0.0113 - acc: 0.998 - ETA: 0s - loss: 0.0115 - acc: 0.998 - ETA: 0s - loss: 0.0115 - acc: 0.998 - ETA: 0s - loss: 0.0115 - acc: 0.998 - 4s 535us/step - loss: 0.0114 - acc: 0.9985 - val\_loss: 0.5440 - val\_acc: 0.8419

Epoch 00012: val\_loss did not improve from 0.52800
Epoch 13/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0353 - acc: 1.000 - ETA: 3s - loss: 0.0112 - acc: 1.000 - ETA: 3s - loss: 0.0103 - acc: 1.000 - ETA: 3s - loss: 0.0101 - acc: 1.000 - ETA: 3s - loss: 0.0116 - acc: 0.997 - ETA: 3s - loss: 0.0108 - acc: 0.998 - ETA: 3s - loss: 0.0098 - acc: 0.998 - ETA: 3s - loss: 0.0092 - acc: 0.998 - ETA: 3s - loss: 0.0087 - acc: 0.998 - ETA: 3s - loss: 0.0091 - acc: 0.999 - ETA: 3s - loss: 0.0089 - acc: 0.999 - ETA: 2s - loss: 0.0091 - acc: 0.999 - ETA: 2s - loss: 0.0090 - acc: 0.999 - ETA: 2s - loss: 0.0088 - acc: 0.999 - ETA: 2s - loss: 0.0091 - acc: 0.999 - ETA: 2s - loss: 0.0090 - acc: 0.999 - ETA: 2s - loss: 0.0089 - acc: 0.999 - ETA: 2s - loss: 0.0088 - acc: 0.999 - ETA: 2s - loss: 0.0087 - acc: 0.999 - ETA: 2s - loss: 0.0091 - acc: 0.999 - ETA: 2s - loss: 0.0092 - acc: 0.998 - ETA: 2s - loss: 0.0091 - acc: 0.998 - ETA: 2s - loss: 0.0090 - acc: 0.998 - ETA: 2s - loss: 0.0089 - acc: 0.998 - ETA: 2s - loss: 0.0088 - acc: 0.998 - ETA: 2s - loss: 0.0092 - acc: 0.998 - ETA: 2s - loss: 0.0091 - acc: 0.998 - ETA: 2s - loss: 0.0094 - acc: 0.998 - ETA: 2s - loss: 0.0094 - acc: 0.998 - ETA: 1s - loss: 0.0093 - acc: 0.998 - ETA: 1s - loss: 0.0092 - acc: 0.998 - ETA: 1s - loss: 0.0091 - acc: 0.998 - ETA: 1s - loss: 0.0092 - acc: 0.998 - ETA: 1s - loss: 0.0093 - acc: 0.998 - ETA: 1s - loss: 0.0092 - acc: 0.998 - ETA: 1s - loss: 0.0096 - acc: 0.998 - ETA: 1s - loss: 0.0096 - acc: 0.998 - ETA: 1s - loss: 0.0095 - acc: 0.998 - ETA: 1s - loss: 0.0094 - acc: 0.998 - ETA: 1s - loss: 0.0094 - acc: 0.998 - ETA: 1s - loss: 0.0096 - acc: 0.998 - ETA: 1s - loss: 0.0096 - acc: 0.998 - ETA: 1s - loss: 0.0096 - acc: 0.998 - ETA: 1s - loss: 0.0095 - acc: 0.998 - ETA: 1s - loss: 0.0095 - acc: 0.998 - ETA: 1s - loss: 0.0094 - acc: 0.998 - ETA: 1s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0097 - acc: 0.998 - ETA: 0s - loss: 0.0096 - acc: 0.998 - ETA: 0s - loss: 0.0095 - acc: 0.998 - ETA: 0s - loss: 0.0095 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0094 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0092 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - ETA: 0s - loss: 0.0093 - acc: 0.998 - 4s 555us/step - loss: 0.0093 - acc: 0.9985 - val\_loss: 0.5707 - val\_acc: 0.8431

Epoch 00013: val\_loss did not improve from 0.52800
Epoch 14/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0082 - acc: 1.000 - ETA: 3s - loss: 0.0068 - acc: 1.000 - ETA: 3s - loss: 0.0064 - acc: 1.000 - ETA: 3s - loss: 0.0073 - acc: 1.000 - ETA: 3s - loss: 0.0068 - acc: 1.000 - ETA: 3s - loss: 0.0074 - acc: 1.000 - ETA: 3s - loss: 0.0092 - acc: 0.998 - ETA: 3s - loss: 0.0087 - acc: 0.998 - ETA: 3s - loss: 0.0085 - acc: 0.998 - ETA: 3s - loss: 0.0083 - acc: 0.999 - ETA: 3s - loss: 0.0079 - acc: 0.999 - ETA: 2s - loss: 0.0079 - acc: 0.999 - ETA: 2s - loss: 0.0077 - acc: 0.999 - ETA: 2s - loss: 0.0077 - acc: 0.999 - ETA: 2s - loss: 0.0085 - acc: 0.998 - ETA: 2s - loss: 0.0083 - acc: 0.998 - ETA: 2s - loss: 0.0082 - acc: 0.998 - ETA: 2s - loss: 0.0080 - acc: 0.998 - ETA: 2s - loss: 0.0078 - acc: 0.998 - ETA: 2s - loss: 0.0077 - acc: 0.999 - ETA: 2s - loss: 0.0083 - acc: 0.998 - ETA: 2s - loss: 0.0081 - acc: 0.998 - ETA: 2s - loss: 0.0079 - acc: 0.998 - ETA: 2s - loss: 0.0078 - acc: 0.998 - ETA: 2s - loss: 0.0078 - acc: 0.998 - ETA: 2s - loss: 0.0076 - acc: 0.998 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 2s - loss: 0.0075 - acc: 0.998 - ETA: 2s - loss: 0.0074 - acc: 0.999 - ETA: 1s - loss: 0.0073 - acc: 0.999 - ETA: 1s - loss: 0.0073 - acc: 0.999 - ETA: 1s - loss: 0.0073 - acc: 0.999 - ETA: 1s - loss: 0.0072 - acc: 0.999 - ETA: 1s - loss: 0.0072 - acc: 0.999 - ETA: 1s - loss: 0.0071 - acc: 0.999 - ETA: 1s - loss: 0.0070 - acc: 0.999 - ETA: 1s - loss: 0.0070 - acc: 0.999 - ETA: 1s - loss: 0.0069 - acc: 0.999 - ETA: 1s - loss: 0.0069 - acc: 0.999 - ETA: 1s - loss: 0.0069 - acc: 0.999 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 1s - loss: 0.0069 - acc: 0.999 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 0s - loss: 0.0072 - acc: 0.999 - ETA: 0s - loss: 0.0072 - acc: 0.999 - ETA: 0s - loss: 0.0071 - acc: 0.999 - ETA: 0s - loss: 0.0072 - acc: 0.999 - ETA: 0s - loss: 0.0076 - acc: 0.999 - ETA: 0s - loss: 0.0078 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.998 - ETA: 0s - loss: 0.0078 - acc: 0.999 - ETA: 0s - loss: 0.0078 - acc: 0.999 - ETA: 0s - loss: 0.0078 - acc: 0.999 - ETA: 0s - loss: 0.0081 - acc: 0.998 - ETA: 0s - loss: 0.0084 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0083 - acc: 0.998 - ETA: 0s - loss: 0.0082 - acc: 0.998 - 4s 545us/step - loss: 0.0082 - acc: 0.9988 - val\_loss: 0.5560 - val\_acc: 0.8395

Epoch 00014: val\_loss did not improve from 0.52800
Epoch 15/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0051 - acc: 1.000 - ETA: 3s - loss: 0.0057 - acc: 1.000 - ETA: 4s - loss: 0.0052 - acc: 1.000 - ETA: 4s - loss: 0.0063 - acc: 1.000 - ETA: 4s - loss: 0.0059 - acc: 1.000 - ETA: 3s - loss: 0.0057 - acc: 1.000 - ETA: 3s - loss: 0.0060 - acc: 1.000 - ETA: 3s - loss: 0.0058 - acc: 1.000 - ETA: 3s - loss: 0.0057 - acc: 1.000 - ETA: 3s - loss: 0.0053 - acc: 1.000 - ETA: 3s - loss: 0.0052 - acc: 1.000 - ETA: 3s - loss: 0.0051 - acc: 1.000 - ETA: 3s - loss: 0.0052 - acc: 1.000 - ETA: 3s - loss: 0.0052 - acc: 1.000 - ETA: 3s - loss: 0.0051 - acc: 1.000 - ETA: 2s - loss: 0.0051 - acc: 1.000 - ETA: 2s - loss: 0.0050 - acc: 1.000 - ETA: 2s - loss: 0.0054 - acc: 1.000 - ETA: 2s - loss: 0.0064 - acc: 0.999 - ETA: 2s - loss: 0.0063 - acc: 0.999 - ETA: 2s - loss: 0.0063 - acc: 0.999 - ETA: 2s - loss: 0.0062 - acc: 0.999 - ETA: 2s - loss: 0.0064 - acc: 0.999 - ETA: 2s - loss: 0.0071 - acc: 0.999 - ETA: 2s - loss: 0.0070 - acc: 0.999 - ETA: 2s - loss: 0.0069 - acc: 0.999 - ETA: 2s - loss: 0.0069 - acc: 0.999 - ETA: 2s - loss: 0.0068 - acc: 0.999 - ETA: 2s - loss: 0.0069 - acc: 0.999 - ETA: 2s - loss: 0.0068 - acc: 0.999 - ETA: 2s - loss: 0.0073 - acc: 0.999 - ETA: 1s - loss: 0.0073 - acc: 0.999 - ETA: 1s - loss: 0.0072 - acc: 0.999 - ETA: 1s - loss: 0.0072 - acc: 0.999 - ETA: 1s - loss: 0.0071 - acc: 0.999 - ETA: 1s - loss: 0.0071 - acc: 0.999 - ETA: 1s - loss: 0.0070 - acc: 0.999 - ETA: 1s - loss: 0.0069 - acc: 0.999 - ETA: 1s - loss: 0.0069 - acc: 0.999 - ETA: 1s - loss: 0.0070 - acc: 0.999 - ETA: 1s - loss: 0.0070 - acc: 0.999 - ETA: 1s - loss: 0.0069 - acc: 0.999 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 1s - loss: 0.0068 - acc: 0.999 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0073 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.999 - ETA: 0s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0073 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0072 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0071 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0070 - acc: 0.998 - ETA: 0s - loss: 0.0069 - acc: 0.999 - ETA: 0s - loss: 0.0069 - acc: 0.999 - ETA: 0s - loss: 0.0070 - acc: 0.999 - ETA: 0s - loss: 0.0070 - acc: 0.999 - ETA: 0s - loss: 0.0070 - acc: 0.999 - ETA: 0s - loss: 0.0069 - acc: 0.999 - ETA: 0s - loss: 0.0069 - acc: 0.999 - ETA: 0s - loss: 0.0069 - acc: 0.999 - ETA: 0s - loss: 0.0073 - acc: 0.998 - 4s 551us/step - loss: 0.0073 - acc: 0.9990 - val\_loss: 0.5698 - val\_acc: 0.8419

Epoch 00015: val\_loss did not improve from 0.52800
Epoch 16/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0138 - acc: 0.991 - ETA: 3s - loss: 0.0093 - acc: 0.995 - ETA: 3s - loss: 0.0116 - acc: 0.993 - ETA: 3s - loss: 0.0102 - acc: 0.995 - ETA: 3s - loss: 0.0087 - acc: 0.996 - ETA: 3s - loss: 0.0079 - acc: 0.996 - ETA: 3s - loss: 0.0080 - acc: 0.997 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0089 - acc: 0.996 - ETA: 3s - loss: 0.0086 - acc: 0.997 - ETA: 3s - loss: 0.0086 - acc: 0.997 - ETA: 2s - loss: 0.0082 - acc: 0.997 - ETA: 2s - loss: 0.0079 - acc: 0.997 - ETA: 2s - loss: 0.0077 - acc: 0.997 - ETA: 2s - loss: 0.0074 - acc: 0.998 - ETA: 2s - loss: 0.0071 - acc: 0.998 - ETA: 2s - loss: 0.0070 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0068 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0065 - acc: 0.998 - ETA: 2s - loss: 0.0071 - acc: 0.998 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0068 - acc: 0.998 - ETA: 2s - loss: 0.0067 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.998 - ETA: 2s - loss: 0.0065 - acc: 0.998 - ETA: 2s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0072 - acc: 0.997 - ETA: 1s - loss: 0.0071 - acc: 0.997 - ETA: 1s - loss: 0.0070 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0069 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0066 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0064 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0063 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - ETA: 0s - loss: 0.0062 - acc: 0.998 - 4s 542us/step - loss: 0.0064 - acc: 0.9985 - val\_loss: 0.5820 - val\_acc: 0.8467

Epoch 00016: val\_loss did not improve from 0.52800
Epoch 17/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0025 - acc: 1.000 - ETA: 3s - loss: 0.0028 - acc: 1.000 - ETA: 3s - loss: 0.0025 - acc: 1.000 - ETA: 3s - loss: 0.0026 - acc: 1.000 - ETA: 3s - loss: 0.0032 - acc: 1.000 - ETA: 3s - loss: 0.0032 - acc: 1.000 - ETA: 3s - loss: 0.0031 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0030 - acc: 1.000 - ETA: 3s - loss: 0.0031 - acc: 1.000 - ETA: 3s - loss: 0.0045 - acc: 0.999 - ETA: 2s - loss: 0.0045 - acc: 0.999 - ETA: 2s - loss: 0.0044 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0043 - acc: 0.999 - ETA: 2s - loss: 0.0042 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0040 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 2s - loss: 0.0039 - acc: 0.999 - ETA: 2s - loss: 0.0047 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.999 - ETA: 2s - loss: 0.0049 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0047 - acc: 0.999 - ETA: 1s - loss: 0.0047 - acc: 0.999 - ETA: 1s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0046 - acc: 0.999 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.999 - ETA: 1s - loss: 0.0050 - acc: 0.999 - ETA: 1s - loss: 0.0050 - acc: 0.999 - ETA: 1s - loss: 0.0049 - acc: 0.999 - ETA: 1s - loss: 0.0049 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 1s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0047 - acc: 0.999 - ETA: 0s - loss: 0.0047 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0047 - acc: 0.999 - ETA: 0s - loss: 0.0047 - acc: 0.999 - ETA: 0s - loss: 0.0047 - acc: 0.999 - ETA: 0s - loss: 0.0047 - acc: 0.999 - ETA: 0s - loss: 0.0047 - acc: 0.999 - ETA: 0s - loss: 0.0050 - acc: 0.999 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - 4s 559us/step - loss: 0.0057 - acc: 0.9987 - val\_loss: 0.6016 - val\_acc: 0.8431

Epoch 00017: val\_loss did not improve from 0.52800
Epoch 18/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0079 - acc: 1.000 - ETA: 3s - loss: 0.0074 - acc: 1.000 - ETA: 3s - loss: 0.0059 - acc: 1.000 - ETA: 3s - loss: 0.0050 - acc: 1.000 - ETA: 3s - loss: 0.0044 - acc: 1.000 - ETA: 3s - loss: 0.0043 - acc: 1.000 - ETA: 3s - loss: 0.0040 - acc: 1.000 - ETA: 3s - loss: 0.0043 - acc: 1.000 - ETA: 3s - loss: 0.0040 - acc: 1.000 - ETA: 3s - loss: 0.0039 - acc: 1.000 - ETA: 3s - loss: 0.0037 - acc: 1.000 - ETA: 3s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0033 - acc: 1.000 - ETA: 2s - loss: 0.0036 - acc: 1.000 - ETA: 2s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0035 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0034 - acc: 1.000 - ETA: 2s - loss: 0.0033 - acc: 1.000 - ETA: 2s - loss: 0.0032 - acc: 1.000 - ETA: 2s - loss: 0.0032 - acc: 1.000 - ETA: 2s - loss: 0.0032 - acc: 1.000 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0040 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0044 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0043 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.999 - ETA: 1s - loss: 0.0041 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0045 - acc: 0.999 - ETA: 1s - loss: 0.0050 - acc: 0.998 - ETA: 1s - loss: 0.0050 - acc: 0.999 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - 4s 555us/step - loss: 0.0052 - acc: 0.9988 - val\_loss: 0.5782 - val\_acc: 0.8527

Epoch 00018: val\_loss did not improve from 0.52800
Epoch 19/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0015 - acc: 1.000 - ETA: 3s - loss: 0.0019 - acc: 1.000 - ETA: 3s - loss: 0.0019 - acc: 1.000 - ETA: 3s - loss: 0.0021 - acc: 1.000 - ETA: 3s - loss: 0.0020 - acc: 1.000 - ETA: 3s - loss: 0.0021 - acc: 1.000 - ETA: 3s - loss: 0.0021 - acc: 1.000 - ETA: 3s - loss: 0.0040 - acc: 0.998 - ETA: 3s - loss: 0.0037 - acc: 0.998 - ETA: 3s - loss: 0.0036 - acc: 0.999 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 2s - loss: 0.0041 - acc: 0.998 - ETA: 2s - loss: 0.0040 - acc: 0.998 - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 2s - loss: 0.0047 - acc: 0.998 - ETA: 2s - loss: 0.0052 - acc: 0.997 - ETA: 2s - loss: 0.0050 - acc: 0.997 - ETA: 2s - loss: 0.0049 - acc: 0.997 - ETA: 2s - loss: 0.0047 - acc: 0.998 - ETA: 2s - loss: 0.0046 - acc: 0.998 - ETA: 2s - loss: 0.0045 - acc: 0.998 - ETA: 2s - loss: 0.0044 - acc: 0.998 - ETA: 2s - loss: 0.0043 - acc: 0.998 - ETA: 2s - loss: 0.0042 - acc: 0.998 - ETA: 2s - loss: 0.0041 - acc: 0.998 - ETA: 2s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0039 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0036 - acc: 0.999 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0042 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 1s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.999 - ETA: 0s - loss: 0.0042 - acc: 0.999 - ETA: 0s - loss: 0.0042 - acc: 0.999 - ETA: 0s - loss: 0.0042 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0041 - acc: 0.999 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - 4s 548us/step - loss: 0.0047 - acc: 0.9987 - val\_loss: 0.5911 - val\_acc: 0.8455

Epoch 00019: val\_loss did not improve from 0.52800
Epoch 20/20
6680/6680 [==============================] - ETA: 4s - loss: 0.0039 - acc: 1.000 - ETA: 3s - loss: 0.0029 - acc: 1.000 - ETA: 3s - loss: 0.0064 - acc: 0.995 - ETA: 3s - loss: 0.0046 - acc: 0.997 - ETA: 3s - loss: 0.0040 - acc: 0.997 - ETA: 3s - loss: 0.0040 - acc: 0.998 - ETA: 3s - loss: 0.0036 - acc: 0.998 - ETA: 3s - loss: 0.0034 - acc: 0.998 - ETA: 3s - loss: 0.0035 - acc: 0.998 - ETA: 3s - loss: 0.0033 - acc: 0.999 - ETA: 3s - loss: 0.0031 - acc: 0.999 - ETA: 2s - loss: 0.0030 - acc: 0.999 - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0036 - acc: 0.998 - ETA: 2s - loss: 0.0036 - acc: 0.998 - ETA: 2s - loss: 0.0037 - acc: 0.998 - ETA: 2s - loss: 0.0037 - acc: 0.998 - ETA: 2s - loss: 0.0036 - acc: 0.998 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.999 - ETA: 2s - loss: 0.0034 - acc: 0.999 - ETA: 2s - loss: 0.0033 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0032 - acc: 0.999 - ETA: 2s - loss: 0.0035 - acc: 0.998 - ETA: 1s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0032 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0033 - acc: 0.999 - ETA: 1s - loss: 0.0035 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0034 - acc: 0.999 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0038 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0037 - acc: 0.998 - ETA: 1s - loss: 0.0040 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0042 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0041 - acc: 0.998 - ETA: 0s - loss: 0.0043 - acc: 0.998 - ETA: 0s - loss: 0.0047 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0045 - acc: 0.998 - ETA: 0s - loss: 0.0046 - acc: 0.998 - 4s 550us/step - loss: 0.0045 - acc: 0.9985 - val\_loss: 0.6097 - val\_acc: 0.8479

Epoch 00020: val\_loss did not improve from 0.52800

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:} <keras.callbacks.History at 0x283102b1fd0>
\end{Verbatim}
            
    \subsubsection{(IMPLEMENTATION) Load the Model with the Best Validation
Loss}\label{implementation-load-the-model-with-the-best-validation-loss}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Load the model weights with the best validation loss.}
         \PY{n}{ResNet\PYZus{}model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best\PYZus{}adamax.ResNet50.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \subsubsection{(IMPLEMENTATION) Test the
Model}\label{implementation-test-the-model}

Try out your model on the test dataset of dog images. Ensure that your
test accuracy is greater than 60\%.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Calculate classification accuracy on the test dataset.}
         \PY{n}{ResNet\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{ResNet\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{feature}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{test\PYZus{}ResNet50}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{ResNet\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{ResNet\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 83.0144\%

    \end{Verbatim}

    \subsubsection{(IMPLEMENTATION) Predict Dog Breed with the
Model}\label{implementation-predict-dog-breed-with-the-model}

Write a function that takes an image path as input and returns the dog
breed (\texttt{Affenpinscher}, \texttt{Afghan\_hound}, etc) that is
predicted by your model.

Similar to the analogous function in Step 5, your function should have
three steps: 1. Extract the bottleneck features corresponding to the
chosen CNN model. 2. Supply the bottleneck features as input to the
model to return the predicted vector. Note that the argmax of this
prediction vector gives the index of the predicted dog breed. 3. Use the
\texttt{dog\_names} array defined in Step 0 of this notebook to return
the corresponding breed.

The functions to extract the bottleneck features can be found in
\texttt{extract\_bottleneck\_features.py}, and they have been imported
in an earlier code cell. To obtain the bottleneck features corresponding
to your chosen CNN architecture, you need to use the function

\begin{verbatim}
extract_{network}
\end{verbatim}

where \texttt{\{network\}}, in the above filename, should be one of
\texttt{VGG19}, \texttt{Resnet50}, \texttt{InceptionV3}, or
\texttt{Xception}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Write a function that takes a path to an image as input}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} and returns the dog breed that is predicted by the model.}
         
         \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{o}{*}
         
         \PY{k}{def} \PY{n+nf}{Predict\PYZus{}breed\PYZus{}ResNet50}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}This fuction obtain bottleneck\PYZus{}features frpm Resnet 50}
         \PY{l+s+sd}{        and Use them to obtain predicted vector}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{n}{bottleneck\PYZus{}feature} \PY{o}{=} \PY{n}{extract\PYZus{}Resnet50}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
             \PY{n}{predicted\PYZus{}vector} \PY{o}{=} \PY{n}{ResNet\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{bottleneck\PYZus{}feature}\PY{p}{)}
             \PY{k}{return} \PY{n}{dog\PYZus{}names}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{predicted\PYZus{}vector}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 6: Write your Algorithm

Write an algorithm that accepts a file path to an image and first
determines whether the image contains a human, dog, or neither. Then, -
if a \textbf{dog} is detected in the image, return the predicted breed.
- if a \textbf{human} is detected in the image, return the resembling
dog breed. - if \textbf{neither} is detected in the image, provide
output that indicates an error.

You are welcome to write your own functions for detecting humans and
dogs in images, but feel free to use the \texttt{face\_detector} and
\texttt{dog\_detector} functions developed above. You are
\textbf{required} to use your CNN from Step 5 to predict dog breed.

Some sample output for our algorithm is provided below, but feel free to
design your own user experience!

\begin{figure}
\centering
\includegraphics{images/sample_human_output.png}
\caption{Sample Human Output}
\end{figure}

\subsubsection{(IMPLEMENTATION) Write your
Algorithm}\label{implementation-write-your-algorithm}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Write your algorithm.}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{as} \PY{n+nn}{mpimg}
         
         \PY{k}{def} \PY{n+nf}{detector\PYZus{}algorithm}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{n}{frame} \PY{o}{=} \PY{n}{mpimg}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{frame}\PY{p}{)}
             \PY{k}{if} \PY{n}{face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)} \PY{o}{==} \PY{k+kc}{True}\PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Human image detected and it resembles the following dog }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{Predict\PYZus{}breed\PYZus{}ResNet50}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}\PY{p}{;}
             \PY{k}{elif} \PY{n}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)} \PY{o}{==} \PY{k+kc}{True}\PY{p}{:} 
                 \PY{n}{plt}\PY{o}{.}\PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dog images detected and it}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{s probably }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{Predict\PYZus{}breed\PYZus{}ResNet50}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}\PY{p}{;}
             \PY{k}{else}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No human or dog images Detected!}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             \PY{k}{return}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 7: Test Your Algorithm

In this section, you will take your new algorithm for a spin! What kind
of dog does the algorithm think that \textbf{you} look like? If you have
a dog, does it predict your dog's breed accurately? If you have a cat,
does it mistakenly think that your cat is a dog?

\subsubsection{(IMPLEMENTATION) Test Your Algorithm on Sample
Images!}\label{implementation-test-your-algorithm-on-sample-images}

Test your algorithm at least six images on your computer. Feel free to
use any images you like. Use at least two human and two dog images.

\textbf{Question 6:} Is the output better than you expected :) ? Or
worse :( ? Provide at least three possible points of improvement for
your algorithm.

\textbf{Answer:}

The predictions are actually far better than I expected. The algorithm
is capable of detecting human face and dog face at 83.0144\% accuracy.
It can differentiate a wolf from a dog which I thought was mindblowing.

Points of improvement for the algorithm:

One, use of image augmentation in the training data. Two, train the
algorithm in a larger dataset which contains diverse images of dog
breeds. Three, fine tune the algorithm to detect the difference between
dog breeds that look closely resemble each other by training it on
images containing breeds that resemble each other.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}113}]:} \PY{k+kn}{import} \PY{n+nn}{os}
          \PY{k}{for} \PY{n}{files} \PY{o+ow}{in} \PY{n}{os}\PY{o}{.}\PY{n}{listdir}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{new images}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
              
              \PY{n+nb}{print}\PY{p}{(}\PY{n}{files}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
affenpinscher.jpg
Afghan Hound.jpg
american bulldog.jpg
Chow chow.jpg
Collie.jpg
coyote.jpg
Dachshund.jpg
Diana.jpg
DonaldTrump.jpg
English Cocker Spaniel.jpg
GeorgeWashington.jpg
Great dane.jpg
hairyhuman.jpg
huskies.jpg
jackal.jpg
Komondor.jpg
Marie Curie.jpg
Marilyn Monroe.jpg
obama.jpg
oprahwinfrey.jpg
Raccoon dog.jpg
ThomasJefferson.jpg
Wangari Maathai.jpg
wolf.jpg
XiJinPing.jpg

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}137}]:} \PY{k+kn}{import} \PY{n+nn}{os}
          \PY{k}{for} \PY{n}{files} \PY{o+ow}{in} \PY{n}{os}\PY{o}{.}\PY{n}{listdir}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{new images}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
              \PY{n}{new\PYZus{}path} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{new images/}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n}{files}
              \PY{n}{final\PYZus{}imges} \PY{o}{=} \PY{n}{detector\PYZus{}algorithm}\PY{p}{(}\PY{n}{new\PYZus{}path}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
No human or dog images Detected!

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_7.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_8.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_9.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_10.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_11.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_12.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
No human or dog images Detected!

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_14.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_15.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
No human or dog images Detected!

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_17.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_18.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_19.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_20.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_21.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_22.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
No human or dog images Detected!

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_24.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_25.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_26.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
No human or dog images Detected!

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_28.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_29.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
